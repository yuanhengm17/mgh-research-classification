{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "30e998e0-f6cd-4d3d-82f0-08637032af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from firebase_admin import credentials, firestore\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Firebase Initialization\n",
    "cred = credentials.Certificate(\"adminkey.json\")\n",
    "#firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename='training_log.txt', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "INCLUDE_ONLY = ['Stephen', 'Lillian', 'Ren', 'Yuanheng', 'Ethan Shao']\n",
    "ACTIVITIES = ['sit','walk','upstair']\n",
    "CHUNK_SIZE = 2.375  # in seconds (can be a decimal)\n",
    "SAMPLING_RATE = 100  # Hz\n",
    "NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f933bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "OVERLAP = 0.5  # Fix for previous NameError\n",
    "\n",
    "# Define feature extraction function\n",
    "def extract_features(chunk):\n",
    "    \"\"\"Extract features from a chunked acceleration segment with selected statistics.\"\"\"\n",
    "    feature_vector = []\n",
    "    \n",
    "    for axis in ['x', 'y', 'z']:\n",
    "        data_series = pd.Series(chunk[axis])\n",
    "        # Apply smoothing\n",
    "        smoothed_data = data_series.rolling(window=5, min_periods=1).mean()\n",
    "        feature_vector.extend([\n",
    "            smoothed_data.mean(),                  # Mean\n",
    "            smoothed_data.median(),                # Median\n",
    "            smoothed_data.std(),                   # Standard deviation\n",
    "            smoothed_data.var(),                   # Variance\n",
    "            smoothed_data.min(),                   # Minimum\n",
    "            smoothed_data.max(),                   # Maximum\n",
    "        ])\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "287ad27b-8007-4ccd-b113-f8016c2aaec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data \n",
    "def fetch_data_for_stephen(collection_name, activities, time_start=500, time_end=6000):\n",
    "    data, docs = [], []\n",
    "    person_name = \"Stephen\"\n",
    "    \n",
    "    for activity in activities:\n",
    "        for recording in db.collection(collection_name).document(person_name).collection(activity).stream():\n",
    "            record = recording.to_dict()\n",
    "            if 'acceleration' not in record:\n",
    "                continue\n",
    "\n",
    "            docs.append(record)\n",
    "            df = pd.DataFrame(record['acceleration'])\n",
    "            \n",
    "            if 'time' in df.columns:\n",
    "                filtered_df = df[(df['time'] >= time_start) & (df['time'] <= time_end)]\n",
    "                data.append(filtered_df)\n",
    "            else:\n",
    "                raise ValueError(\"The 'acceleration' field must include a 'time' column.\")\n",
    "    return data, docs\n",
    "\n",
    "# Fetch data\n",
    "training_data_raw, training_docs = fetch_data_for_stephen(\"training\", ACTIVITIES)\n",
    "testing_data_raw, testing_docs = fetch_data_for_stephen(\"testing\", ACTIVITIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a1284a61-2522-4348-abbf-1d4e31e36bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chunking with overlap\n",
    "def chunk_data_with_overlap(data_raw, docs, chunk_size, activities, sampling_rate, overlap=0.5):\n",
    "    \"\"\"Chunk raw acceleration data into smaller labeled segments using overlapping windows.\"\"\"\n",
    "    data, labels = [], []\n",
    "    chunk_samples = int(chunk_size * sampling_rate)\n",
    "    step = int(chunk_samples * (1 - overlap))  # Compute step size based on overlap\n",
    "\n",
    "    for i, df in enumerate(data_raw):\n",
    "        # Slide over the data with the defined step\n",
    "        for start in range(0, len(df) - chunk_samples + 1, step):\n",
    "            end = start + chunk_samples     \n",
    "            chunk = df.iloc[start:end]\n",
    "            activity = docs[i]['activity']\n",
    "            label = activities.index(activity)\n",
    "            extradata.append(extract_features(chunk))\n",
    "            labels.append(label)\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "43de7965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset summary:\n",
      "+----------+------------------+\n",
      "| Dataset  | number of chunks |\n",
      "+----------+------------------+\n",
      "| training |        6         |\n",
      "| testing  |        6         |\n",
      "+----------+------------------+\n",
      "Training Activities Count\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'training_distribution' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Activities Count\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, activity \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ACTIVITIES):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactivity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(\u001b[43mtraining_distribution\u001b[49m[i])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTesting Activity Count\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, activity \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ACTIVITIES):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_distribution' is not defined"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate #for table formatting\n",
    "\n",
    "#Calculate the number of training and testing samples\n",
    "num_training_samples = len(training_data_raw)\n",
    "num_testing_samples = len(testing_data_raw)\n",
    "\n",
    "\n",
    "#table\n",
    "summary_table = [[\"training\", num_training_samples], [\"testing\", num_testing_samples]]\n",
    "\n",
    "#print\n",
    "print(\"dataset summary:\")\n",
    "print(tabulate(summary_table, headers = [\"Dataset\", \"number of chunks\"], tablefmt=\"pretty\"))\n",
    "\n",
    "print(\"Training Activities Count\")\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "    print(f\"{activity}: {int(training_distribution[i])} chunks\")\n",
    "\n",
    "print(\"\\nTesting Activity Count\")\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "    print(f\"{activity}:{int(testing_distribution[i])} chunks\")\n",
    "print(len(training_data_raw))\n",
    "print(len(testing_data_raw))\n",
    "print(np.array(training_data).shape)\n",
    "print(len(training_data_raw))\n",
    "print(len(testing_data_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fae1da3e-3e2c-40ad-82cb-6d090f2cc7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features using chunk_data_with_overlap\n",
    "X_train, y_train = chunk_data_with_overlap(training_data_raw, training_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE, OVERLAP)\n",
    "X_test, y_test = chunk_data_with_overlap(testing_data_raw, testing_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE, OVERLAP)\n",
    "\n",
    "# Combine train and test for cross-validation (Stephen's data only)\n",
    "X_all = np.concatenate([X_train, X_test], axis=0)\n",
    "y_all = np.concatenate([y_train, y_test], axis=0)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_all = scaler.fit_transform(X_all)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_all = torch.tensor(X_all, dtype=torch.float32)  # Shape: (n_samples, 18)\n",
    "y_all = torch.tensor(y_all, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7ecb31f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' class OptimizedLSTMModel(nn.Module):\\n    def __init__(self, num_classes, input_channels, seq_length, hidden_size=128, num_layers=2):\\n        super(OptimizedLSTMModel, self).__init__()\\n        self.seq_length = seq_length\\n        self.hidden_size = hidden_size\\n        self.num_layers = num_layers\\n        \\n        # LSTM layer expects input of shape (batch_size, seq_length, input_size)\\n        self.lstm = nn.LSTM(input_channels, hidden_size, num_layers, batch_first=True)\\n        \\n        # Fully connected layer to output predictions\\n        self.fc = nn.Linear(hidden_size, num_classes)\\n        \\n        # Dropout layer for regularization\\n        self.dropout = nn.Dropout(0.3)\\n\\n    def forward(self, x):\\n        # LSTM requires a hidden state and cell state to be initialized\\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\\n\\n        # Pass through LSTM\\n        out, _ = self.lstm(x, (h0, c0))\\n\\n        # Use the output of the last timestep for classification\\n        out = out[:, -1, :]  # Take the last hidden state\\n        out = self.dropout(out)\\n        \\n        # Pass through the fully connected layer\\n        out = self.fc(out)\\n        return out\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' class OptimizedLSTMModel(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels, seq_length, hidden_size=128, num_layers=2):\n",
    "        super(OptimizedLSTMModel, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer expects input of shape (batch_size, seq_length, input_size)\n",
    "        self.lstm = nn.LSTM(input_channels, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to output predictions\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM requires a hidden state and cell state to be initialized\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Use the output of the last timestep for classification\n",
    "        out = out[:, -1, :]  # Take the last hidden state\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3ec68f8d-85ff-48e1-b219-a547e9393c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for extracted features (MLP)\n",
    "class FeatureModel(nn.Module):\n",
    "    def __init__(self, num_classes, input_features=18):\n",
    "        super(FeatureModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_features, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2fea2d82-2721-4226-87c8-d0386cb60e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save best model and metadata\n",
    "BEST_MODEL_PATH = \"best_model.pth\"\n",
    "BEST_METADATA_PATH = \"best_model.json\"\n",
    "\n",
    "# Corrected save_best_model function\n",
    "def save_best_model(epoch, model, optimizer, loss, accuracy, train_losses, test_losses):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses,\n",
    "    }\n",
    "    torch.save(checkpoint, BEST_MODEL_PATH)\n",
    "\n",
    "    # Save metadata with correct variable names\n",
    "    with open(BEST_METADATA_PATH, \"w\") as f:\n",
    "        json.dump({\"epoch\": epoch, \"test_loss\": loss, \"test_accuracy\": accuracy}, f)\n",
    "\n",
    "# Function to load best model if exists\n",
    "def load_best_model(model, optimizer, best_model_path=BEST_MODEL_PATH):\n",
    "    if os.path.exists(best_model_path):\n",
    "        checkpoint = torch.load(best_model_path)\n",
    "        \n",
    "        # Print checkpoint keys to understand its structure\n",
    "        print(\"Checkpoint keys:\", checkpoint.keys())\n",
    "        \n",
    "        # Load model weights\n",
    "        model_state_dict = model.state_dict()\n",
    "        checkpoint_state_dict = checkpoint['model_state_dict']\n",
    "        \n",
    "        # Filter out incompatible keys\n",
    "        filtered_checkpoint_state_dict = {k: v for k, v in checkpoint_state_dict.items() if k in model_state_dict}\n",
    "        model_state_dict.update(filtered_checkpoint_state_dict)\n",
    "        model.load_state_dict(model_state_dict)\n",
    "        \n",
    "        # Load other metadata if available\n",
    "        start_epoch = checkpoint.get('epoch', 0)\n",
    "        best_loss = checkpoint.get('loss', float('inf'))  # Default to infinity if loss is missing\n",
    "        best_avg_accuracy = checkpoint.get('accuracy', 0)  # Default to 0 if accuracy is missing\n",
    "        \n",
    "        # If train_losses and test_losses aren't saved, return empty lists or placeholders\n",
    "        train_losses = checkpoint.get('train_losses', [])\n",
    "        test_losses = checkpoint.get('test_losses', [])\n",
    "        \n",
    "        return start_epoch, best_loss, best_avg_accuracy, train_losses, test_losses\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting from scratch.\")\n",
    "        # Return default values\n",
    "        return 0, float('inf'), 0, [], []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "afb6cc02-f6d5-441e-8ea1-736efc276305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model (no transpose for MLP)\n",
    "def evaluate_model(loader, model, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "01254ebf-d89d-4173-9f04-ba08155090cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Data Augmentation Functions\n",
    "def add_jitter(batch_X, noise_level=0.01):\n",
    "    noise = torch.randn_like(batch_X) * noise_level\n",
    "    return batch_X + noise\n",
    "\n",
    "def scale_signal(batch_X, scale_range=(0.9, 1.1)):\n",
    "    scale_factor = torch.FloatTensor(batch_X.shape[0], 1, 1).uniform_(*scale_range)\n",
    "    return batch_X * scale_factor\n",
    "\n",
    "def time_warp(batch_X, sigma=0.2):\n",
    "    batch_size, num_channels, seq_length = batch_X.shape\n",
    "    time_steps = np.arange(seq_length)\n",
    "    warping_curve = np.cumsum(np.random.normal(0, sigma, size=(batch_size, seq_length)), axis=1)\n",
    "    warping_curve = (warping_curve - warping_curve.min(axis=1, keepdims=True)) / \\\n",
    "                    (warping_curve.max(axis=1, keepdims=True) - warping_curve.min(axis=1, keepdims=True)) * seq_length\n",
    "    warped_X = torch.zeros_like(batch_X)\n",
    "    for i in range(batch_size):\n",
    "        for j in range(num_channels):\n",
    "            f = interp1d(time_steps, batch_X[i, j, :].cpu().numpy(), kind='linear', fill_value='extrapolate')\n",
    "            warped_X[i, j, :] = torch.tensor(f(warping_curve[i]), dtype=batch_X.dtype)\n",
    "    return warped_X\n",
    "\n",
    "def random_crop(batch_X, crop_size=0.9, target_length=None):\n",
    "    batch_size, num_channels, seq_length = batch_X.shape\n",
    "    new_length = int(seq_length * crop_size)\n",
    "    start_idx = torch.randint(0, seq_length - new_length + 1, (batch_size,))\n",
    "    cropped_X = torch.zeros(batch_size, num_channels, new_length)\n",
    "    for i in range(batch_size):\n",
    "        cropped_X[i] = batch_X[i, :, start_idx[i]:start_idx[i] + new_length]\n",
    "    \n",
    "    if target_length is not None:\n",
    "        if new_length < target_length:\n",
    "            padding = torch.zeros(batch_size, num_channels, target_length - new_length)\n",
    "            cropped_X = torch.cat([cropped_X, padding], dim=2)\n",
    "        elif new_length > target_length:\n",
    "            cropped_X = cropped_X[:, :, :target_length]\n",
    "    return cropped_X\n",
    "\n",
    "def permute_segments(batch_X, num_segments=5):\n",
    "    batch_size, num_channels, seq_length = batch_X.shape\n",
    "    segment_length = seq_length // num_segments\n",
    "    permuted_X = batch_X.clone()\n",
    "    for i in range(batch_size):\n",
    "        perm = torch.randperm(num_segments)\n",
    "        for j in range(num_segments):\n",
    "            permuted_X[i, :, j * segment_length:(j + 1) * segment_length] = \\\n",
    "                batch_X[i, :, perm[j] * segment_length:(perm[j] + 1) * segment_length]\n",
    "    return permuted_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fc68f1f4-225f-455c-b6d0-80a76a3ce598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint keys: dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss', 'accuracy', 'train_losses', 'test_losses'])\n"
     ]
    }
   ],
   "source": [
    "model = FeatureModel(num_classes=NUM_CLASSES, input_features=18)\n",
    "optimizer = Adam(model.parameters(), lr=0.02, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50, min_lr=1e-5)\n",
    "\n",
    "start_epoch, best_loss, best_avg_accuracy, train_losses, test_losses = load_best_model(model, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fd628c08-376b-468a-ab5f-6c1a88c7587c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Fold 1 Epoch [1/100], Train Loss: 1.0673, Train Acc: 39.29%, Test Loss: 1.2156, Test Acc: 12.50%\n",
      "Fold 1 Epoch [2/100], Train Loss: 1.0507, Train Acc: 39.29%, Test Loss: 1.2055, Test Acc: 12.50%\n",
      "Fold 1 Epoch [3/100], Train Loss: 1.0351, Train Acc: 39.29%, Test Loss: 1.1965, Test Acc: 12.50%\n",
      "Fold 1 Epoch [4/100], Train Loss: 1.0218, Train Acc: 39.29%, Test Loss: 1.1875, Test Acc: 12.50%\n",
      "Fold 1 Epoch [5/100], Train Loss: 1.0094, Train Acc: 39.29%, Test Loss: 1.1799, Test Acc: 12.50%\n",
      "Fold 1 Epoch [6/100], Train Loss: 0.9969, Train Acc: 39.29%, Test Loss: 1.1719, Test Acc: 25.00%\n",
      "Fold 1 Epoch [7/100], Train Loss: 0.9837, Train Acc: 50.00%, Test Loss: 1.1642, Test Acc: 25.00%\n",
      "Fold 1 Epoch [8/100], Train Loss: 0.9702, Train Acc: 57.14%, Test Loss: 1.1566, Test Acc: 25.00%\n",
      "Fold 1 Epoch [9/100], Train Loss: 0.9561, Train Acc: 57.14%, Test Loss: 1.1488, Test Acc: 25.00%\n",
      "Fold 1 Epoch [10/100], Train Loss: 0.9414, Train Acc: 78.57%, Test Loss: 1.1406, Test Acc: 25.00%\n",
      "Fold 1 Epoch [11/100], Train Loss: 0.9258, Train Acc: 78.57%, Test Loss: 1.1317, Test Acc: 25.00%\n",
      "Fold 1 Epoch [12/100], Train Loss: 0.9096, Train Acc: 78.57%, Test Loss: 1.1222, Test Acc: 25.00%\n",
      "Fold 1 Epoch [13/100], Train Loss: 0.8927, Train Acc: 78.57%, Test Loss: 1.1126, Test Acc: 25.00%\n",
      "Fold 1 Epoch [14/100], Train Loss: 0.8753, Train Acc: 78.57%, Test Loss: 1.1029, Test Acc: 25.00%\n",
      "Fold 1 Epoch [15/100], Train Loss: 0.8568, Train Acc: 78.57%, Test Loss: 1.0929, Test Acc: 25.00%\n",
      "Fold 1 Epoch [16/100], Train Loss: 0.8378, Train Acc: 78.57%, Test Loss: 1.0821, Test Acc: 25.00%\n",
      "Fold 1 Epoch [17/100], Train Loss: 0.8184, Train Acc: 78.57%, Test Loss: 1.0706, Test Acc: 25.00%\n",
      "Fold 1 Epoch [18/100], Train Loss: 0.7984, Train Acc: 82.14%, Test Loss: 1.0588, Test Acc: 25.00%\n",
      "Fold 1 Epoch [19/100], Train Loss: 0.7778, Train Acc: 85.71%, Test Loss: 1.0466, Test Acc: 25.00%\n",
      "Fold 1 Epoch [20/100], Train Loss: 0.7566, Train Acc: 85.71%, Test Loss: 1.0341, Test Acc: 37.50%\n",
      "Fold 1 Epoch [21/100], Train Loss: 0.7350, Train Acc: 85.71%, Test Loss: 1.0213, Test Acc: 37.50%\n",
      "Fold 1 Epoch [22/100], Train Loss: 0.7128, Train Acc: 85.71%, Test Loss: 1.0082, Test Acc: 37.50%\n",
      "Fold 1 Epoch [23/100], Train Loss: 0.6902, Train Acc: 85.71%, Test Loss: 0.9951, Test Acc: 37.50%\n",
      "Fold 1 Epoch [24/100], Train Loss: 0.6673, Train Acc: 85.71%, Test Loss: 0.9821, Test Acc: 37.50%\n",
      "Fold 1 Epoch [25/100], Train Loss: 0.6441, Train Acc: 85.71%, Test Loss: 0.9687, Test Acc: 37.50%\n",
      "Fold 1 Epoch [26/100], Train Loss: 0.6211, Train Acc: 92.86%, Test Loss: 0.9555, Test Acc: 37.50%\n",
      "Fold 1 Epoch [27/100], Train Loss: 0.5981, Train Acc: 92.86%, Test Loss: 0.9422, Test Acc: 37.50%\n",
      "Fold 1 Epoch [28/100], Train Loss: 0.5751, Train Acc: 92.86%, Test Loss: 0.9286, Test Acc: 37.50%\n",
      "Fold 1 Epoch [29/100], Train Loss: 0.5523, Train Acc: 92.86%, Test Loss: 0.9151, Test Acc: 50.00%\n",
      "Fold 1 Epoch [30/100], Train Loss: 0.5297, Train Acc: 92.86%, Test Loss: 0.9017, Test Acc: 50.00%\n",
      "Fold 1 Epoch [31/100], Train Loss: 0.5074, Train Acc: 92.86%, Test Loss: 0.8884, Test Acc: 50.00%\n",
      "Fold 1 Epoch [32/100], Train Loss: 0.4854, Train Acc: 92.86%, Test Loss: 0.8754, Test Acc: 50.00%\n",
      "Fold 1 Epoch [33/100], Train Loss: 0.4639, Train Acc: 96.43%, Test Loss: 0.8623, Test Acc: 50.00%\n",
      "Fold 1 Epoch [34/100], Train Loss: 0.4427, Train Acc: 96.43%, Test Loss: 0.8494, Test Acc: 50.00%\n",
      "Fold 1 Epoch [35/100], Train Loss: 0.4222, Train Acc: 96.43%, Test Loss: 0.8365, Test Acc: 62.50%\n",
      "Fold 1 Epoch [36/100], Train Loss: 0.4024, Train Acc: 96.43%, Test Loss: 0.8241, Test Acc: 62.50%\n",
      "Fold 1 Epoch [37/100], Train Loss: 0.3833, Train Acc: 96.43%, Test Loss: 0.8119, Test Acc: 62.50%\n",
      "Fold 1 Epoch [38/100], Train Loss: 0.3650, Train Acc: 96.43%, Test Loss: 0.7997, Test Acc: 62.50%\n",
      "Fold 1 Epoch [39/100], Train Loss: 0.3474, Train Acc: 96.43%, Test Loss: 0.7873, Test Acc: 62.50%\n",
      "Fold 1 Epoch [40/100], Train Loss: 0.3306, Train Acc: 96.43%, Test Loss: 0.7744, Test Acc: 62.50%\n",
      "Fold 1 Epoch [41/100], Train Loss: 0.3145, Train Acc: 96.43%, Test Loss: 0.7611, Test Acc: 62.50%\n",
      "Fold 1 Epoch [42/100], Train Loss: 0.2991, Train Acc: 96.43%, Test Loss: 0.7477, Test Acc: 62.50%\n",
      "Fold 1 Epoch [43/100], Train Loss: 0.2846, Train Acc: 96.43%, Test Loss: 0.7341, Test Acc: 62.50%\n",
      "Fold 1 Epoch [44/100], Train Loss: 0.2707, Train Acc: 96.43%, Test Loss: 0.7202, Test Acc: 62.50%\n",
      "Fold 1 Epoch [45/100], Train Loss: 0.2576, Train Acc: 96.43%, Test Loss: 0.7069, Test Acc: 62.50%\n",
      "Fold 1 Epoch [46/100], Train Loss: 0.2450, Train Acc: 96.43%, Test Loss: 0.6928, Test Acc: 62.50%\n",
      "Fold 1 Epoch [47/100], Train Loss: 0.2331, Train Acc: 96.43%, Test Loss: 0.6784, Test Acc: 62.50%\n",
      "Fold 1 Epoch [48/100], Train Loss: 0.2217, Train Acc: 96.43%, Test Loss: 0.6637, Test Acc: 62.50%\n",
      "Fold 1 Epoch [49/100], Train Loss: 0.2109, Train Acc: 96.43%, Test Loss: 0.6488, Test Acc: 62.50%\n",
      "Fold 1 Epoch [50/100], Train Loss: 0.2007, Train Acc: 96.43%, Test Loss: 0.6338, Test Acc: 62.50%\n",
      "Fold 1 Epoch [51/100], Train Loss: 0.1909, Train Acc: 96.43%, Test Loss: 0.6184, Test Acc: 62.50%\n",
      "Fold 1 Epoch [52/100], Train Loss: 0.1815, Train Acc: 96.43%, Test Loss: 0.6024, Test Acc: 62.50%\n",
      "Fold 1 Epoch [53/100], Train Loss: 0.1725, Train Acc: 96.43%, Test Loss: 0.5862, Test Acc: 62.50%\n",
      "Fold 1 Epoch [54/100], Train Loss: 0.1639, Train Acc: 96.43%, Test Loss: 0.5691, Test Acc: 87.50%\n",
      "Fold 1 Epoch [55/100], Train Loss: 0.1555, Train Acc: 96.43%, Test Loss: 0.5508, Test Acc: 87.50%\n",
      "Fold 1 Epoch [56/100], Train Loss: 0.1476, Train Acc: 100.00%, Test Loss: 0.5321, Test Acc: 87.50%\n",
      "Fold 1 Epoch [57/100], Train Loss: 0.1398, Train Acc: 100.00%, Test Loss: 0.5131, Test Acc: 87.50%\n",
      "Fold 1 Epoch [58/100], Train Loss: 0.1324, Train Acc: 100.00%, Test Loss: 0.4947, Test Acc: 87.50%\n",
      "Fold 1 Epoch [59/100], Train Loss: 0.1253, Train Acc: 100.00%, Test Loss: 0.4765, Test Acc: 87.50%\n",
      "Fold 1 Epoch [60/100], Train Loss: 0.1184, Train Acc: 100.00%, Test Loss: 0.4585, Test Acc: 87.50%\n",
      "Fold 1 Epoch [61/100], Train Loss: 0.1118, Train Acc: 100.00%, Test Loss: 0.4405, Test Acc: 87.50%\n",
      "Fold 1 Epoch [62/100], Train Loss: 0.1054, Train Acc: 100.00%, Test Loss: 0.4228, Test Acc: 87.50%\n",
      "Fold 1 Epoch [63/100], Train Loss: 0.0993, Train Acc: 100.00%, Test Loss: 0.4055, Test Acc: 87.50%\n",
      "Fold 1 Epoch [64/100], Train Loss: 0.0934, Train Acc: 100.00%, Test Loss: 0.3885, Test Acc: 87.50%\n",
      "Fold 1 Epoch [65/100], Train Loss: 0.0878, Train Acc: 100.00%, Test Loss: 0.3718, Test Acc: 87.50%\n",
      "Fold 1 Epoch [66/100], Train Loss: 0.0824, Train Acc: 100.00%, Test Loss: 0.3547, Test Acc: 87.50%\n",
      "Fold 1 Epoch [67/100], Train Loss: 0.0772, Train Acc: 100.00%, Test Loss: 0.3382, Test Acc: 87.50%\n",
      "Fold 1 Epoch [68/100], Train Loss: 0.0723, Train Acc: 100.00%, Test Loss: 0.3216, Test Acc: 87.50%\n",
      "Fold 1 Epoch [69/100], Train Loss: 0.0677, Train Acc: 100.00%, Test Loss: 0.3060, Test Acc: 87.50%\n",
      "Fold 1 Epoch [70/100], Train Loss: 0.0633, Train Acc: 100.00%, Test Loss: 0.2908, Test Acc: 87.50%\n",
      "Fold 1 Epoch [71/100], Train Loss: 0.0592, Train Acc: 100.00%, Test Loss: 0.2770, Test Acc: 87.50%\n",
      "Fold 1 Epoch [72/100], Train Loss: 0.0554, Train Acc: 100.00%, Test Loss: 0.2641, Test Acc: 87.50%\n",
      "Fold 1 Epoch [73/100], Train Loss: 0.0518, Train Acc: 100.00%, Test Loss: 0.2519, Test Acc: 87.50%\n",
      "Fold 1 Epoch [74/100], Train Loss: 0.0485, Train Acc: 100.00%, Test Loss: 0.2406, Test Acc: 87.50%\n",
      "Fold 1 Epoch [75/100], Train Loss: 0.0455, Train Acc: 100.00%, Test Loss: 0.2301, Test Acc: 87.50%\n",
      "Fold 1 Epoch [76/100], Train Loss: 0.0427, Train Acc: 100.00%, Test Loss: 0.2206, Test Acc: 87.50%\n",
      "Fold 1 Epoch [77/100], Train Loss: 0.0400, Train Acc: 100.00%, Test Loss: 0.2114, Test Acc: 87.50%\n",
      "Fold 1 Epoch [78/100], Train Loss: 0.0376, Train Acc: 100.00%, Test Loss: 0.2025, Test Acc: 87.50%\n",
      "Fold 1 Epoch [79/100], Train Loss: 0.0353, Train Acc: 100.00%, Test Loss: 0.1944, Test Acc: 87.50%\n",
      "Fold 1 Epoch [80/100], Train Loss: 0.0332, Train Acc: 100.00%, Test Loss: 0.1867, Test Acc: 87.50%\n",
      "Fold 1 Epoch [81/100], Train Loss: 0.0312, Train Acc: 100.00%, Test Loss: 0.1793, Test Acc: 87.50%\n",
      "Fold 1 Epoch [82/100], Train Loss: 0.0293, Train Acc: 100.00%, Test Loss: 0.1724, Test Acc: 87.50%\n",
      "Fold 1 Epoch [83/100], Train Loss: 0.0276, Train Acc: 100.00%, Test Loss: 0.1666, Test Acc: 87.50%\n",
      "Fold 1 Epoch [84/100], Train Loss: 0.0260, Train Acc: 100.00%, Test Loss: 0.1605, Test Acc: 87.50%\n",
      "Fold 1 Epoch [85/100], Train Loss: 0.0245, Train Acc: 100.00%, Test Loss: 0.1553, Test Acc: 87.50%\n",
      "Fold 1 Epoch [86/100], Train Loss: 0.0231, Train Acc: 100.00%, Test Loss: 0.1505, Test Acc: 87.50%\n",
      "Fold 1 Epoch [87/100], Train Loss: 0.0218, Train Acc: 100.00%, Test Loss: 0.1464, Test Acc: 87.50%\n",
      "Fold 1 Epoch [88/100], Train Loss: 0.0206, Train Acc: 100.00%, Test Loss: 0.1424, Test Acc: 87.50%\n",
      "Fold 1 Epoch [89/100], Train Loss: 0.0195, Train Acc: 100.00%, Test Loss: 0.1391, Test Acc: 87.50%\n",
      "Fold 1 Epoch [90/100], Train Loss: 0.0185, Train Acc: 100.00%, Test Loss: 0.1358, Test Acc: 87.50%\n",
      "Fold 1 Epoch [91/100], Train Loss: 0.0175, Train Acc: 100.00%, Test Loss: 0.1318, Test Acc: 87.50%\n",
      "Fold 1 Epoch [92/100], Train Loss: 0.0166, Train Acc: 100.00%, Test Loss: 0.1283, Test Acc: 100.00%\n",
      "Fold 1 Epoch [93/100], Train Loss: 0.0158, Train Acc: 100.00%, Test Loss: 0.1251, Test Acc: 100.00%\n",
      "Fold 1 Epoch [94/100], Train Loss: 0.0150, Train Acc: 100.00%, Test Loss: 0.1220, Test Acc: 100.00%\n",
      "Fold 1 Epoch [95/100], Train Loss: 0.0142, Train Acc: 100.00%, Test Loss: 0.1185, Test Acc: 100.00%\n",
      "Fold 1 Epoch [96/100], Train Loss: 0.0135, Train Acc: 100.00%, Test Loss: 0.1151, Test Acc: 100.00%\n",
      "Fold 1 Epoch [97/100], Train Loss: 0.0128, Train Acc: 100.00%, Test Loss: 0.1121, Test Acc: 100.00%\n",
      "Fold 1 Epoch [98/100], Train Loss: 0.0121, Train Acc: 100.00%, Test Loss: 0.1092, Test Acc: 100.00%\n",
      "Fold 1 Epoch [99/100], Train Loss: 0.0115, Train Acc: 100.00%, Test Loss: 0.1068, Test Acc: 100.00%\n",
      "Fold 1 Epoch [100/100], Train Loss: 0.0110, Train Acc: 100.00%, Test Loss: 0.1046, Test Acc: 100.00%\n",
      "Fold 1 Best Accuracy: 100.00%\n",
      "Fold 2/5\n",
      "Fold 2 Epoch [1/100], Train Loss: 1.0460, Train Acc: 86.21%, Test Loss: 1.0509, Test Acc: 71.43%\n",
      "Fold 2 Epoch [2/100], Train Loss: 1.0290, Train Acc: 86.21%, Test Loss: 1.0363, Test Acc: 71.43%\n",
      "Fold 2 Epoch [3/100], Train Loss: 1.0122, Train Acc: 86.21%, Test Loss: 1.0218, Test Acc: 71.43%\n",
      "Fold 2 Epoch [4/100], Train Loss: 0.9959, Train Acc: 86.21%, Test Loss: 1.0075, Test Acc: 71.43%\n",
      "Fold 2 Epoch [5/100], Train Loss: 0.9800, Train Acc: 89.66%, Test Loss: 0.9935, Test Acc: 71.43%\n",
      "Fold 2 Epoch [6/100], Train Loss: 0.9646, Train Acc: 89.66%, Test Loss: 0.9797, Test Acc: 71.43%\n",
      "Fold 2 Epoch [7/100], Train Loss: 0.9494, Train Acc: 89.66%, Test Loss: 0.9664, Test Acc: 85.71%\n",
      "Fold 2 Epoch [8/100], Train Loss: 0.9351, Train Acc: 89.66%, Test Loss: 0.9537, Test Acc: 85.71%\n",
      "Fold 2 Epoch [9/100], Train Loss: 0.9210, Train Acc: 93.10%, Test Loss: 0.9406, Test Acc: 100.00%\n",
      "Fold 2 Epoch [10/100], Train Loss: 0.9068, Train Acc: 96.55%, Test Loss: 0.9274, Test Acc: 100.00%\n",
      "Fold 2 Epoch [11/100], Train Loss: 0.8924, Train Acc: 96.55%, Test Loss: 0.9136, Test Acc: 100.00%\n",
      "Fold 2 Epoch [12/100], Train Loss: 0.8777, Train Acc: 96.55%, Test Loss: 0.8995, Test Acc: 100.00%\n",
      "Fold 2 Epoch [13/100], Train Loss: 0.8627, Train Acc: 96.55%, Test Loss: 0.8854, Test Acc: 100.00%\n",
      "Fold 2 Epoch [14/100], Train Loss: 0.8477, Train Acc: 100.00%, Test Loss: 0.8713, Test Acc: 100.00%\n",
      "Fold 2 Epoch [15/100], Train Loss: 0.8324, Train Acc: 100.00%, Test Loss: 0.8570, Test Acc: 100.00%\n",
      "Fold 2 Epoch [16/100], Train Loss: 0.8170, Train Acc: 100.00%, Test Loss: 0.8423, Test Acc: 100.00%\n",
      "Fold 2 Epoch [17/100], Train Loss: 0.8010, Train Acc: 100.00%, Test Loss: 0.8271, Test Acc: 100.00%\n",
      "Fold 2 Epoch [18/100], Train Loss: 0.7847, Train Acc: 100.00%, Test Loss: 0.8115, Test Acc: 100.00%\n",
      "Fold 2 Epoch [19/100], Train Loss: 0.7680, Train Acc: 100.00%, Test Loss: 0.7957, Test Acc: 100.00%\n",
      "Fold 2 Epoch [20/100], Train Loss: 0.7510, Train Acc: 100.00%, Test Loss: 0.7796, Test Acc: 100.00%\n",
      "Fold 2 Epoch [21/100], Train Loss: 0.7337, Train Acc: 100.00%, Test Loss: 0.7631, Test Acc: 100.00%\n",
      "Fold 2 Epoch [22/100], Train Loss: 0.7161, Train Acc: 100.00%, Test Loss: 0.7461, Test Acc: 100.00%\n",
      "Fold 2 Epoch [23/100], Train Loss: 0.6980, Train Acc: 100.00%, Test Loss: 0.7287, Test Acc: 100.00%\n",
      "Fold 2 Epoch [24/100], Train Loss: 0.6796, Train Acc: 100.00%, Test Loss: 0.7110, Test Acc: 100.00%\n",
      "Fold 2 Epoch [25/100], Train Loss: 0.6610, Train Acc: 100.00%, Test Loss: 0.6932, Test Acc: 100.00%\n",
      "Fold 2 Epoch [26/100], Train Loss: 0.6421, Train Acc: 100.00%, Test Loss: 0.6748, Test Acc: 100.00%\n",
      "Fold 2 Epoch [27/100], Train Loss: 0.6230, Train Acc: 100.00%, Test Loss: 0.6562, Test Acc: 100.00%\n",
      "Fold 2 Epoch [28/100], Train Loss: 0.6039, Train Acc: 100.00%, Test Loss: 0.6374, Test Acc: 100.00%\n",
      "Fold 2 Epoch [29/100], Train Loss: 0.5846, Train Acc: 100.00%, Test Loss: 0.6185, Test Acc: 100.00%\n",
      "Fold 2 Epoch [30/100], Train Loss: 0.5654, Train Acc: 100.00%, Test Loss: 0.5994, Test Acc: 100.00%\n",
      "Fold 2 Epoch [31/100], Train Loss: 0.5462, Train Acc: 100.00%, Test Loss: 0.5801, Test Acc: 100.00%\n",
      "Fold 2 Epoch [32/100], Train Loss: 0.5272, Train Acc: 100.00%, Test Loss: 0.5606, Test Acc: 100.00%\n",
      "Fold 2 Epoch [33/100], Train Loss: 0.5081, Train Acc: 100.00%, Test Loss: 0.5411, Test Acc: 100.00%\n",
      "Fold 2 Epoch [34/100], Train Loss: 0.4891, Train Acc: 100.00%, Test Loss: 0.5214, Test Acc: 100.00%\n",
      "Fold 2 Epoch [35/100], Train Loss: 0.4702, Train Acc: 100.00%, Test Loss: 0.5018, Test Acc: 100.00%\n",
      "Fold 2 Epoch [36/100], Train Loss: 0.4514, Train Acc: 100.00%, Test Loss: 0.4824, Test Acc: 100.00%\n",
      "Fold 2 Epoch [37/100], Train Loss: 0.4328, Train Acc: 100.00%, Test Loss: 0.4630, Test Acc: 100.00%\n",
      "Fold 2 Epoch [38/100], Train Loss: 0.4144, Train Acc: 100.00%, Test Loss: 0.4436, Test Acc: 100.00%\n",
      "Fold 2 Epoch [39/100], Train Loss: 0.3961, Train Acc: 100.00%, Test Loss: 0.4241, Test Acc: 100.00%\n",
      "Fold 2 Epoch [40/100], Train Loss: 0.3780, Train Acc: 100.00%, Test Loss: 0.4049, Test Acc: 100.00%\n",
      "Fold 2 Epoch [41/100], Train Loss: 0.3602, Train Acc: 100.00%, Test Loss: 0.3859, Test Acc: 100.00%\n",
      "Fold 2 Epoch [42/100], Train Loss: 0.3426, Train Acc: 100.00%, Test Loss: 0.3671, Test Acc: 100.00%\n",
      "Fold 2 Epoch [43/100], Train Loss: 0.3254, Train Acc: 100.00%, Test Loss: 0.3487, Test Acc: 100.00%\n",
      "Fold 2 Epoch [44/100], Train Loss: 0.3084, Train Acc: 100.00%, Test Loss: 0.3305, Test Acc: 100.00%\n",
      "Fold 2 Epoch [45/100], Train Loss: 0.2918, Train Acc: 100.00%, Test Loss: 0.3125, Test Acc: 100.00%\n",
      "Fold 2 Epoch [46/100], Train Loss: 0.2756, Train Acc: 100.00%, Test Loss: 0.2948, Test Acc: 100.00%\n",
      "Fold 2 Epoch [47/100], Train Loss: 0.2598, Train Acc: 100.00%, Test Loss: 0.2775, Test Acc: 100.00%\n",
      "Fold 2 Epoch [48/100], Train Loss: 0.2445, Train Acc: 100.00%, Test Loss: 0.2607, Test Acc: 100.00%\n",
      "Fold 2 Epoch [49/100], Train Loss: 0.2296, Train Acc: 100.00%, Test Loss: 0.2444, Test Acc: 100.00%\n",
      "Fold 2 Epoch [50/100], Train Loss: 0.2154, Train Acc: 100.00%, Test Loss: 0.2286, Test Acc: 100.00%\n",
      "Fold 2 Epoch [51/100], Train Loss: 0.2016, Train Acc: 100.00%, Test Loss: 0.2134, Test Acc: 100.00%\n",
      "Fold 2 Epoch [52/100], Train Loss: 0.1883, Train Acc: 100.00%, Test Loss: 0.1989, Test Acc: 100.00%\n",
      "Fold 2 Epoch [53/100], Train Loss: 0.1757, Train Acc: 100.00%, Test Loss: 0.1850, Test Acc: 100.00%\n",
      "Fold 2 Epoch [54/100], Train Loss: 0.1636, Train Acc: 100.00%, Test Loss: 0.1719, Test Acc: 100.00%\n",
      "Fold 2 Epoch [55/100], Train Loss: 0.1522, Train Acc: 100.00%, Test Loss: 0.1597, Test Acc: 100.00%\n",
      "Fold 2 Epoch [56/100], Train Loss: 0.1414, Train Acc: 100.00%, Test Loss: 0.1484, Test Acc: 100.00%\n",
      "Fold 2 Epoch [57/100], Train Loss: 0.1311, Train Acc: 100.00%, Test Loss: 0.1377, Test Acc: 100.00%\n",
      "Fold 2 Epoch [58/100], Train Loss: 0.1215, Train Acc: 100.00%, Test Loss: 0.1277, Test Acc: 100.00%\n",
      "Fold 2 Epoch [59/100], Train Loss: 0.1126, Train Acc: 100.00%, Test Loss: 0.1184, Test Acc: 100.00%\n",
      "Fold 2 Epoch [60/100], Train Loss: 0.1042, Train Acc: 100.00%, Test Loss: 0.1097, Test Acc: 100.00%\n",
      "Fold 2 Epoch [61/100], Train Loss: 0.0964, Train Acc: 100.00%, Test Loss: 0.1014, Test Acc: 100.00%\n",
      "Fold 2 Epoch [62/100], Train Loss: 0.0892, Train Acc: 100.00%, Test Loss: 0.0937, Test Acc: 100.00%\n",
      "Fold 2 Epoch [63/100], Train Loss: 0.0824, Train Acc: 100.00%, Test Loss: 0.0866, Test Acc: 100.00%\n",
      "Fold 2 Epoch [64/100], Train Loss: 0.0761, Train Acc: 100.00%, Test Loss: 0.0800, Test Acc: 100.00%\n",
      "Fold 2 Epoch [65/100], Train Loss: 0.0703, Train Acc: 100.00%, Test Loss: 0.0740, Test Acc: 100.00%\n",
      "Fold 2 Epoch [66/100], Train Loss: 0.0649, Train Acc: 100.00%, Test Loss: 0.0683, Test Acc: 100.00%\n",
      "Fold 2 Epoch [67/100], Train Loss: 0.0599, Train Acc: 100.00%, Test Loss: 0.0633, Test Acc: 100.00%\n",
      "Fold 2 Epoch [68/100], Train Loss: 0.0552, Train Acc: 100.00%, Test Loss: 0.0585, Test Acc: 100.00%\n",
      "Fold 2 Epoch [69/100], Train Loss: 0.0510, Train Acc: 100.00%, Test Loss: 0.0541, Test Acc: 100.00%\n",
      "Fold 2 Epoch [70/100], Train Loss: 0.0472, Train Acc: 100.00%, Test Loss: 0.0500, Test Acc: 100.00%\n",
      "Fold 2 Epoch [71/100], Train Loss: 0.0437, Train Acc: 100.00%, Test Loss: 0.0463, Test Acc: 100.00%\n",
      "Fold 2 Epoch [72/100], Train Loss: 0.0405, Train Acc: 100.00%, Test Loss: 0.0428, Test Acc: 100.00%\n",
      "Fold 2 Epoch [73/100], Train Loss: 0.0376, Train Acc: 100.00%, Test Loss: 0.0397, Test Acc: 100.00%\n",
      "Fold 2 Epoch [74/100], Train Loss: 0.0349, Train Acc: 100.00%, Test Loss: 0.0370, Test Acc: 100.00%\n",
      "Fold 2 Epoch [75/100], Train Loss: 0.0324, Train Acc: 100.00%, Test Loss: 0.0344, Test Acc: 100.00%\n",
      "Fold 2 Epoch [76/100], Train Loss: 0.0301, Train Acc: 100.00%, Test Loss: 0.0321, Test Acc: 100.00%\n",
      "Fold 2 Epoch [77/100], Train Loss: 0.0281, Train Acc: 100.00%, Test Loss: 0.0300, Test Acc: 100.00%\n",
      "Fold 2 Epoch [78/100], Train Loss: 0.0262, Train Acc: 100.00%, Test Loss: 0.0280, Test Acc: 100.00%\n",
      "Fold 2 Epoch [79/100], Train Loss: 0.0244, Train Acc: 100.00%, Test Loss: 0.0261, Test Acc: 100.00%\n",
      "Fold 2 Epoch [80/100], Train Loss: 0.0229, Train Acc: 100.00%, Test Loss: 0.0244, Test Acc: 100.00%\n",
      "Fold 2 Epoch [81/100], Train Loss: 0.0215, Train Acc: 100.00%, Test Loss: 0.0229, Test Acc: 100.00%\n",
      "Fold 2 Epoch [82/100], Train Loss: 0.0201, Train Acc: 100.00%, Test Loss: 0.0215, Test Acc: 100.00%\n",
      "Fold 2 Epoch [83/100], Train Loss: 0.0189, Train Acc: 100.00%, Test Loss: 0.0202, Test Acc: 100.00%\n",
      "Fold 2 Epoch [84/100], Train Loss: 0.0178, Train Acc: 100.00%, Test Loss: 0.0191, Test Acc: 100.00%\n",
      "Fold 2 Epoch [85/100], Train Loss: 0.0167, Train Acc: 100.00%, Test Loss: 0.0181, Test Acc: 100.00%\n",
      "Fold 2 Epoch [86/100], Train Loss: 0.0158, Train Acc: 100.00%, Test Loss: 0.0172, Test Acc: 100.00%\n",
      "Fold 2 Epoch [87/100], Train Loss: 0.0149, Train Acc: 100.00%, Test Loss: 0.0163, Test Acc: 100.00%\n",
      "Fold 2 Epoch [88/100], Train Loss: 0.0141, Train Acc: 100.00%, Test Loss: 0.0156, Test Acc: 100.00%\n",
      "Fold 2 Epoch [89/100], Train Loss: 0.0134, Train Acc: 100.00%, Test Loss: 0.0149, Test Acc: 100.00%\n",
      "Fold 2 Epoch [90/100], Train Loss: 0.0127, Train Acc: 100.00%, Test Loss: 0.0143, Test Acc: 100.00%\n",
      "Fold 2 Epoch [91/100], Train Loss: 0.0120, Train Acc: 100.00%, Test Loss: 0.0136, Test Acc: 100.00%\n",
      "Fold 2 Epoch [92/100], Train Loss: 0.0115, Train Acc: 100.00%, Test Loss: 0.0130, Test Acc: 100.00%\n",
      "Fold 2 Epoch [93/100], Train Loss: 0.0109, Train Acc: 100.00%, Test Loss: 0.0125, Test Acc: 100.00%\n",
      "Fold 2 Epoch [94/100], Train Loss: 0.0104, Train Acc: 100.00%, Test Loss: 0.0119, Test Acc: 100.00%\n",
      "Fold 2 Epoch [95/100], Train Loss: 0.0099, Train Acc: 100.00%, Test Loss: 0.0114, Test Acc: 100.00%\n",
      "Fold 2 Epoch [96/100], Train Loss: 0.0094, Train Acc: 100.00%, Test Loss: 0.0109, Test Acc: 100.00%\n",
      "Fold 2 Epoch [97/100], Train Loss: 0.0090, Train Acc: 100.00%, Test Loss: 0.0104, Test Acc: 100.00%\n",
      "Fold 2 Epoch [98/100], Train Loss: 0.0086, Train Acc: 100.00%, Test Loss: 0.0100, Test Acc: 100.00%\n",
      "Fold 2 Epoch [99/100], Train Loss: 0.0082, Train Acc: 100.00%, Test Loss: 0.0095, Test Acc: 100.00%\n",
      "Fold 2 Epoch [100/100], Train Loss: 0.0079, Train Acc: 100.00%, Test Loss: 0.0091, Test Acc: 100.00%\n",
      "Fold 2 Best Accuracy: 100.00%\n",
      "Fold 3/5\n",
      "Fold 3 Epoch [1/100], Train Loss: 1.1095, Train Acc: 55.17%, Test Loss: 1.1527, Test Acc: 28.57%\n",
      "Fold 3 Epoch [2/100], Train Loss: 1.0892, Train Acc: 58.62%, Test Loss: 1.1297, Test Acc: 42.86%\n",
      "Fold 3 Epoch [3/100], Train Loss: 1.0697, Train Acc: 65.52%, Test Loss: 1.1074, Test Acc: 42.86%\n",
      "Fold 3 Epoch [4/100], Train Loss: 1.0508, Train Acc: 68.97%, Test Loss: 1.0854, Test Acc: 42.86%\n",
      "Fold 3 Epoch [5/100], Train Loss: 1.0322, Train Acc: 68.97%, Test Loss: 1.0640, Test Acc: 42.86%\n",
      "Fold 3 Epoch [6/100], Train Loss: 1.0142, Train Acc: 68.97%, Test Loss: 1.0441, Test Acc: 42.86%\n",
      "Fold 3 Epoch [7/100], Train Loss: 0.9963, Train Acc: 72.41%, Test Loss: 1.0247, Test Acc: 42.86%\n",
      "Fold 3 Epoch [8/100], Train Loss: 0.9787, Train Acc: 82.76%, Test Loss: 1.0064, Test Acc: 71.43%\n",
      "Fold 3 Epoch [9/100], Train Loss: 0.9612, Train Acc: 100.00%, Test Loss: 0.9887, Test Acc: 100.00%\n",
      "Fold 3 Epoch [10/100], Train Loss: 0.9438, Train Acc: 100.00%, Test Loss: 0.9714, Test Acc: 100.00%\n",
      "Fold 3 Epoch [11/100], Train Loss: 0.9261, Train Acc: 100.00%, Test Loss: 0.9527, Test Acc: 100.00%\n",
      "Fold 3 Epoch [12/100], Train Loss: 0.9083, Train Acc: 100.00%, Test Loss: 0.9339, Test Acc: 100.00%\n",
      "Fold 3 Epoch [13/100], Train Loss: 0.8901, Train Acc: 100.00%, Test Loss: 0.9151, Test Acc: 100.00%\n",
      "Fold 3 Epoch [14/100], Train Loss: 0.8717, Train Acc: 100.00%, Test Loss: 0.8965, Test Acc: 100.00%\n",
      "Fold 3 Epoch [15/100], Train Loss: 0.8530, Train Acc: 100.00%, Test Loss: 0.8781, Test Acc: 100.00%\n",
      "Fold 3 Epoch [16/100], Train Loss: 0.8343, Train Acc: 100.00%, Test Loss: 0.8604, Test Acc: 100.00%\n",
      "Fold 3 Epoch [17/100], Train Loss: 0.8153, Train Acc: 100.00%, Test Loss: 0.8429, Test Acc: 100.00%\n",
      "Fold 3 Epoch [18/100], Train Loss: 0.7960, Train Acc: 100.00%, Test Loss: 0.8252, Test Acc: 100.00%\n",
      "Fold 3 Epoch [19/100], Train Loss: 0.7768, Train Acc: 100.00%, Test Loss: 0.8070, Test Acc: 100.00%\n",
      "Fold 3 Epoch [20/100], Train Loss: 0.7572, Train Acc: 100.00%, Test Loss: 0.7881, Test Acc: 100.00%\n",
      "Fold 3 Epoch [21/100], Train Loss: 0.7372, Train Acc: 100.00%, Test Loss: 0.7687, Test Acc: 100.00%\n",
      "Fold 3 Epoch [22/100], Train Loss: 0.7167, Train Acc: 100.00%, Test Loss: 0.7486, Test Acc: 100.00%\n",
      "Fold 3 Epoch [23/100], Train Loss: 0.6957, Train Acc: 100.00%, Test Loss: 0.7275, Test Acc: 100.00%\n",
      "Fold 3 Epoch [24/100], Train Loss: 0.6744, Train Acc: 100.00%, Test Loss: 0.7056, Test Acc: 100.00%\n",
      "Fold 3 Epoch [25/100], Train Loss: 0.6527, Train Acc: 100.00%, Test Loss: 0.6828, Test Acc: 100.00%\n",
      "Fold 3 Epoch [26/100], Train Loss: 0.6305, Train Acc: 100.00%, Test Loss: 0.6594, Test Acc: 100.00%\n",
      "Fold 3 Epoch [27/100], Train Loss: 0.6081, Train Acc: 100.00%, Test Loss: 0.6357, Test Acc: 100.00%\n",
      "Fold 3 Epoch [28/100], Train Loss: 0.5855, Train Acc: 100.00%, Test Loss: 0.6113, Test Acc: 100.00%\n",
      "Fold 3 Epoch [29/100], Train Loss: 0.5626, Train Acc: 100.00%, Test Loss: 0.5864, Test Acc: 100.00%\n",
      "Fold 3 Epoch [30/100], Train Loss: 0.5397, Train Acc: 100.00%, Test Loss: 0.5611, Test Acc: 100.00%\n",
      "Fold 3 Epoch [31/100], Train Loss: 0.5165, Train Acc: 100.00%, Test Loss: 0.5356, Test Acc: 100.00%\n",
      "Fold 3 Epoch [32/100], Train Loss: 0.4935, Train Acc: 100.00%, Test Loss: 0.5102, Test Acc: 100.00%\n",
      "Fold 3 Epoch [33/100], Train Loss: 0.4705, Train Acc: 100.00%, Test Loss: 0.4848, Test Acc: 100.00%\n",
      "Fold 3 Epoch [34/100], Train Loss: 0.4477, Train Acc: 100.00%, Test Loss: 0.4596, Test Acc: 100.00%\n",
      "Fold 3 Epoch [35/100], Train Loss: 0.4251, Train Acc: 100.00%, Test Loss: 0.4345, Test Acc: 100.00%\n",
      "Fold 3 Epoch [36/100], Train Loss: 0.4029, Train Acc: 100.00%, Test Loss: 0.4099, Test Acc: 100.00%\n",
      "Fold 3 Epoch [37/100], Train Loss: 0.3810, Train Acc: 100.00%, Test Loss: 0.3857, Test Acc: 100.00%\n",
      "Fold 3 Epoch [38/100], Train Loss: 0.3596, Train Acc: 100.00%, Test Loss: 0.3622, Test Acc: 100.00%\n",
      "Fold 3 Epoch [39/100], Train Loss: 0.3389, Train Acc: 100.00%, Test Loss: 0.3394, Test Acc: 100.00%\n",
      "Fold 3 Epoch [40/100], Train Loss: 0.3188, Train Acc: 100.00%, Test Loss: 0.3174, Test Acc: 100.00%\n",
      "Fold 3 Epoch [41/100], Train Loss: 0.2993, Train Acc: 100.00%, Test Loss: 0.2963, Test Acc: 100.00%\n",
      "Fold 3 Epoch [42/100], Train Loss: 0.2806, Train Acc: 100.00%, Test Loss: 0.2759, Test Acc: 100.00%\n",
      "Fold 3 Epoch [43/100], Train Loss: 0.2626, Train Acc: 100.00%, Test Loss: 0.2564, Test Acc: 100.00%\n",
      "Fold 3 Epoch [44/100], Train Loss: 0.2454, Train Acc: 100.00%, Test Loss: 0.2380, Test Acc: 100.00%\n",
      "Fold 3 Epoch [45/100], Train Loss: 0.2290, Train Acc: 100.00%, Test Loss: 0.2207, Test Acc: 100.00%\n",
      "Fold 3 Epoch [46/100], Train Loss: 0.2134, Train Acc: 100.00%, Test Loss: 0.2047, Test Acc: 100.00%\n",
      "Fold 3 Epoch [47/100], Train Loss: 0.1988, Train Acc: 100.00%, Test Loss: 0.1898, Test Acc: 100.00%\n",
      "Fold 3 Epoch [48/100], Train Loss: 0.1849, Train Acc: 100.00%, Test Loss: 0.1759, Test Acc: 100.00%\n",
      "Fold 3 Epoch [49/100], Train Loss: 0.1718, Train Acc: 100.00%, Test Loss: 0.1627, Test Acc: 100.00%\n",
      "Fold 3 Epoch [50/100], Train Loss: 0.1595, Train Acc: 100.00%, Test Loss: 0.1505, Test Acc: 100.00%\n",
      "Fold 3 Epoch [51/100], Train Loss: 0.1480, Train Acc: 100.00%, Test Loss: 0.1390, Test Acc: 100.00%\n",
      "Fold 3 Epoch [52/100], Train Loss: 0.1372, Train Acc: 100.00%, Test Loss: 0.1284, Test Acc: 100.00%\n",
      "Fold 3 Epoch [53/100], Train Loss: 0.1272, Train Acc: 100.00%, Test Loss: 0.1187, Test Acc: 100.00%\n",
      "Fold 3 Epoch [54/100], Train Loss: 0.1180, Train Acc: 100.00%, Test Loss: 0.1096, Test Acc: 100.00%\n",
      "Fold 3 Epoch [55/100], Train Loss: 0.1094, Train Acc: 100.00%, Test Loss: 0.1015, Test Acc: 100.00%\n",
      "Fold 3 Epoch [56/100], Train Loss: 0.1014, Train Acc: 100.00%, Test Loss: 0.0940, Test Acc: 100.00%\n",
      "Fold 3 Epoch [57/100], Train Loss: 0.0941, Train Acc: 100.00%, Test Loss: 0.0872, Test Acc: 100.00%\n",
      "Fold 3 Epoch [58/100], Train Loss: 0.0872, Train Acc: 100.00%, Test Loss: 0.0809, Test Acc: 100.00%\n",
      "Fold 3 Epoch [59/100], Train Loss: 0.0809, Train Acc: 100.00%, Test Loss: 0.0752, Test Acc: 100.00%\n",
      "Fold 3 Epoch [60/100], Train Loss: 0.0750, Train Acc: 100.00%, Test Loss: 0.0700, Test Acc: 100.00%\n",
      "Fold 3 Epoch [61/100], Train Loss: 0.0696, Train Acc: 100.00%, Test Loss: 0.0652, Test Acc: 100.00%\n",
      "Fold 3 Epoch [62/100], Train Loss: 0.0645, Train Acc: 100.00%, Test Loss: 0.0607, Test Acc: 100.00%\n",
      "Fold 3 Epoch [63/100], Train Loss: 0.0599, Train Acc: 100.00%, Test Loss: 0.0566, Test Acc: 100.00%\n",
      "Fold 3 Epoch [64/100], Train Loss: 0.0557, Train Acc: 100.00%, Test Loss: 0.0529, Test Acc: 100.00%\n",
      "Fold 3 Epoch [65/100], Train Loss: 0.0518, Train Acc: 100.00%, Test Loss: 0.0495, Test Acc: 100.00%\n",
      "Fold 3 Epoch [66/100], Train Loss: 0.0482, Train Acc: 100.00%, Test Loss: 0.0463, Test Acc: 100.00%\n",
      "Fold 3 Epoch [67/100], Train Loss: 0.0449, Train Acc: 100.00%, Test Loss: 0.0435, Test Acc: 100.00%\n",
      "Fold 3 Epoch [68/100], Train Loss: 0.0419, Train Acc: 100.00%, Test Loss: 0.0409, Test Acc: 100.00%\n",
      "Fold 3 Epoch [69/100], Train Loss: 0.0391, Train Acc: 100.00%, Test Loss: 0.0384, Test Acc: 100.00%\n",
      "Fold 3 Epoch [70/100], Train Loss: 0.0366, Train Acc: 100.00%, Test Loss: 0.0362, Test Acc: 100.00%\n",
      "Fold 3 Epoch [71/100], Train Loss: 0.0343, Train Acc: 100.00%, Test Loss: 0.0340, Test Acc: 100.00%\n",
      "Fold 3 Epoch [72/100], Train Loss: 0.0321, Train Acc: 100.00%, Test Loss: 0.0320, Test Acc: 100.00%\n",
      "Fold 3 Epoch [73/100], Train Loss: 0.0302, Train Acc: 100.00%, Test Loss: 0.0302, Test Acc: 100.00%\n",
      "Fold 3 Epoch [74/100], Train Loss: 0.0284, Train Acc: 100.00%, Test Loss: 0.0286, Test Acc: 100.00%\n",
      "Fold 3 Epoch [75/100], Train Loss: 0.0267, Train Acc: 100.00%, Test Loss: 0.0271, Test Acc: 100.00%\n",
      "Fold 3 Epoch [76/100], Train Loss: 0.0252, Train Acc: 100.00%, Test Loss: 0.0256, Test Acc: 100.00%\n",
      "Fold 3 Epoch [77/100], Train Loss: 0.0237, Train Acc: 100.00%, Test Loss: 0.0243, Test Acc: 100.00%\n",
      "Fold 3 Epoch [78/100], Train Loss: 0.0224, Train Acc: 100.00%, Test Loss: 0.0231, Test Acc: 100.00%\n",
      "Fold 3 Epoch [79/100], Train Loss: 0.0212, Train Acc: 100.00%, Test Loss: 0.0219, Test Acc: 100.00%\n",
      "Fold 3 Epoch [80/100], Train Loss: 0.0201, Train Acc: 100.00%, Test Loss: 0.0209, Test Acc: 100.00%\n",
      "Fold 3 Epoch [81/100], Train Loss: 0.0191, Train Acc: 100.00%, Test Loss: 0.0199, Test Acc: 100.00%\n",
      "Fold 3 Epoch [82/100], Train Loss: 0.0181, Train Acc: 100.00%, Test Loss: 0.0190, Test Acc: 100.00%\n",
      "Fold 3 Epoch [83/100], Train Loss: 0.0172, Train Acc: 100.00%, Test Loss: 0.0181, Test Acc: 100.00%\n",
      "Fold 3 Epoch [84/100], Train Loss: 0.0163, Train Acc: 100.00%, Test Loss: 0.0173, Test Acc: 100.00%\n",
      "Fold 3 Epoch [85/100], Train Loss: 0.0156, Train Acc: 100.00%, Test Loss: 0.0165, Test Acc: 100.00%\n",
      "Fold 3 Epoch [86/100], Train Loss: 0.0148, Train Acc: 100.00%, Test Loss: 0.0158, Test Acc: 100.00%\n",
      "Fold 3 Epoch [87/100], Train Loss: 0.0141, Train Acc: 100.00%, Test Loss: 0.0151, Test Acc: 100.00%\n",
      "Fold 3 Epoch [88/100], Train Loss: 0.0134, Train Acc: 100.00%, Test Loss: 0.0144, Test Acc: 100.00%\n",
      "Fold 3 Epoch [89/100], Train Loss: 0.0128, Train Acc: 100.00%, Test Loss: 0.0138, Test Acc: 100.00%\n",
      "Fold 3 Epoch [90/100], Train Loss: 0.0122, Train Acc: 100.00%, Test Loss: 0.0133, Test Acc: 100.00%\n",
      "Fold 3 Epoch [91/100], Train Loss: 0.0117, Train Acc: 100.00%, Test Loss: 0.0128, Test Acc: 100.00%\n",
      "Fold 3 Epoch [92/100], Train Loss: 0.0112, Train Acc: 100.00%, Test Loss: 0.0123, Test Acc: 100.00%\n",
      "Fold 3 Epoch [93/100], Train Loss: 0.0107, Train Acc: 100.00%, Test Loss: 0.0118, Test Acc: 100.00%\n",
      "Fold 3 Epoch [94/100], Train Loss: 0.0102, Train Acc: 100.00%, Test Loss: 0.0114, Test Acc: 100.00%\n",
      "Fold 3 Epoch [95/100], Train Loss: 0.0098, Train Acc: 100.00%, Test Loss: 0.0110, Test Acc: 100.00%\n",
      "Fold 3 Epoch [96/100], Train Loss: 0.0094, Train Acc: 100.00%, Test Loss: 0.0106, Test Acc: 100.00%\n",
      "Fold 3 Epoch [97/100], Train Loss: 0.0090, Train Acc: 100.00%, Test Loss: 0.0102, Test Acc: 100.00%\n",
      "Fold 3 Epoch [98/100], Train Loss: 0.0087, Train Acc: 100.00%, Test Loss: 0.0099, Test Acc: 100.00%\n",
      "Fold 3 Epoch [99/100], Train Loss: 0.0083, Train Acc: 100.00%, Test Loss: 0.0096, Test Acc: 100.00%\n",
      "Fold 3 Epoch [100/100], Train Loss: 0.0080, Train Acc: 100.00%, Test Loss: 0.0092, Test Acc: 100.00%\n",
      "Fold 3 Best Accuracy: 100.00%\n",
      "Fold 4/5\n",
      "Fold 4 Epoch [1/100], Train Loss: 1.0485, Train Acc: 34.48%, Test Loss: 1.0649, Test Acc: 28.57%\n",
      "Fold 4 Epoch [2/100], Train Loss: 1.0294, Train Acc: 34.48%, Test Loss: 1.0441, Test Acc: 28.57%\n",
      "Fold 4 Epoch [3/100], Train Loss: 1.0102, Train Acc: 34.48%, Test Loss: 1.0236, Test Acc: 28.57%\n",
      "Fold 4 Epoch [4/100], Train Loss: 0.9913, Train Acc: 34.48%, Test Loss: 1.0031, Test Acc: 28.57%\n",
      "Fold 4 Epoch [5/100], Train Loss: 0.9724, Train Acc: 58.62%, Test Loss: 0.9821, Test Acc: 28.57%\n",
      "Fold 4 Epoch [6/100], Train Loss: 0.9535, Train Acc: 65.52%, Test Loss: 0.9609, Test Acc: 57.14%\n",
      "Fold 4 Epoch [7/100], Train Loss: 0.9341, Train Acc: 72.41%, Test Loss: 0.9386, Test Acc: 57.14%\n",
      "Fold 4 Epoch [8/100], Train Loss: 0.9144, Train Acc: 72.41%, Test Loss: 0.9162, Test Acc: 71.43%\n",
      "Fold 4 Epoch [9/100], Train Loss: 0.8942, Train Acc: 79.31%, Test Loss: 0.8938, Test Acc: 100.00%\n",
      "Fold 4 Epoch [10/100], Train Loss: 0.8747, Train Acc: 86.21%, Test Loss: 0.8719, Test Acc: 100.00%\n",
      "Fold 4 Epoch [11/100], Train Loss: 0.8549, Train Acc: 96.55%, Test Loss: 0.8502, Test Acc: 100.00%\n",
      "Fold 4 Epoch [12/100], Train Loss: 0.8347, Train Acc: 96.55%, Test Loss: 0.8277, Test Acc: 100.00%\n",
      "Fold 4 Epoch [13/100], Train Loss: 0.8144, Train Acc: 96.55%, Test Loss: 0.8049, Test Acc: 100.00%\n",
      "Fold 4 Epoch [14/100], Train Loss: 0.7935, Train Acc: 100.00%, Test Loss: 0.7815, Test Acc: 100.00%\n",
      "Fold 4 Epoch [15/100], Train Loss: 0.7721, Train Acc: 100.00%, Test Loss: 0.7578, Test Acc: 100.00%\n",
      "Fold 4 Epoch [16/100], Train Loss: 0.7501, Train Acc: 100.00%, Test Loss: 0.7337, Test Acc: 100.00%\n",
      "Fold 4 Epoch [17/100], Train Loss: 0.7277, Train Acc: 100.00%, Test Loss: 0.7095, Test Acc: 100.00%\n",
      "Fold 4 Epoch [18/100], Train Loss: 0.7050, Train Acc: 100.00%, Test Loss: 0.6854, Test Acc: 100.00%\n",
      "Fold 4 Epoch [19/100], Train Loss: 0.6822, Train Acc: 100.00%, Test Loss: 0.6612, Test Acc: 100.00%\n",
      "Fold 4 Epoch [20/100], Train Loss: 0.6591, Train Acc: 100.00%, Test Loss: 0.6368, Test Acc: 100.00%\n",
      "Fold 4 Epoch [21/100], Train Loss: 0.6360, Train Acc: 100.00%, Test Loss: 0.6124, Test Acc: 100.00%\n",
      "Fold 4 Epoch [22/100], Train Loss: 0.6128, Train Acc: 100.00%, Test Loss: 0.5879, Test Acc: 100.00%\n",
      "Fold 4 Epoch [23/100], Train Loss: 0.5895, Train Acc: 100.00%, Test Loss: 0.5633, Test Acc: 100.00%\n",
      "Fold 4 Epoch [24/100], Train Loss: 0.5662, Train Acc: 100.00%, Test Loss: 0.5389, Test Acc: 100.00%\n",
      "Fold 4 Epoch [25/100], Train Loss: 0.5430, Train Acc: 100.00%, Test Loss: 0.5147, Test Acc: 100.00%\n",
      "Fold 4 Epoch [26/100], Train Loss: 0.5198, Train Acc: 100.00%, Test Loss: 0.4905, Test Acc: 100.00%\n",
      "Fold 4 Epoch [27/100], Train Loss: 0.4968, Train Acc: 100.00%, Test Loss: 0.4666, Test Acc: 100.00%\n",
      "Fold 4 Epoch [28/100], Train Loss: 0.4740, Train Acc: 100.00%, Test Loss: 0.4433, Test Acc: 100.00%\n",
      "Fold 4 Epoch [29/100], Train Loss: 0.4516, Train Acc: 100.00%, Test Loss: 0.4205, Test Acc: 100.00%\n",
      "Fold 4 Epoch [30/100], Train Loss: 0.4296, Train Acc: 100.00%, Test Loss: 0.3982, Test Acc: 100.00%\n",
      "Fold 4 Epoch [31/100], Train Loss: 0.4080, Train Acc: 100.00%, Test Loss: 0.3765, Test Acc: 100.00%\n",
      "Fold 4 Epoch [32/100], Train Loss: 0.3869, Train Acc: 100.00%, Test Loss: 0.3553, Test Acc: 100.00%\n",
      "Fold 4 Epoch [33/100], Train Loss: 0.3662, Train Acc: 100.00%, Test Loss: 0.3347, Test Acc: 100.00%\n",
      "Fold 4 Epoch [34/100], Train Loss: 0.3461, Train Acc: 100.00%, Test Loss: 0.3151, Test Acc: 100.00%\n",
      "Fold 4 Epoch [35/100], Train Loss: 0.3267, Train Acc: 100.00%, Test Loss: 0.2962, Test Acc: 100.00%\n",
      "Fold 4 Epoch [36/100], Train Loss: 0.3081, Train Acc: 100.00%, Test Loss: 0.2783, Test Acc: 100.00%\n",
      "Fold 4 Epoch [37/100], Train Loss: 0.2901, Train Acc: 100.00%, Test Loss: 0.2612, Test Acc: 100.00%\n",
      "Fold 4 Epoch [38/100], Train Loss: 0.2728, Train Acc: 100.00%, Test Loss: 0.2448, Test Acc: 100.00%\n",
      "Fold 4 Epoch [39/100], Train Loss: 0.2561, Train Acc: 100.00%, Test Loss: 0.2293, Test Acc: 100.00%\n",
      "Fold 4 Epoch [40/100], Train Loss: 0.2402, Train Acc: 100.00%, Test Loss: 0.2145, Test Acc: 100.00%\n",
      "Fold 4 Epoch [41/100], Train Loss: 0.2251, Train Acc: 100.00%, Test Loss: 0.2008, Test Acc: 100.00%\n",
      "Fold 4 Epoch [42/100], Train Loss: 0.2106, Train Acc: 100.00%, Test Loss: 0.1877, Test Acc: 100.00%\n",
      "Fold 4 Epoch [43/100], Train Loss: 0.1968, Train Acc: 100.00%, Test Loss: 0.1753, Test Acc: 100.00%\n",
      "Fold 4 Epoch [44/100], Train Loss: 0.1838, Train Acc: 100.00%, Test Loss: 0.1636, Test Acc: 100.00%\n",
      "Fold 4 Epoch [45/100], Train Loss: 0.1716, Train Acc: 100.00%, Test Loss: 0.1526, Test Acc: 100.00%\n",
      "Fold 4 Epoch [46/100], Train Loss: 0.1599, Train Acc: 100.00%, Test Loss: 0.1423, Test Acc: 100.00%\n",
      "Fold 4 Epoch [47/100], Train Loss: 0.1489, Train Acc: 100.00%, Test Loss: 0.1326, Test Acc: 100.00%\n",
      "Fold 4 Epoch [48/100], Train Loss: 0.1386, Train Acc: 100.00%, Test Loss: 0.1236, Test Acc: 100.00%\n",
      "Fold 4 Epoch [49/100], Train Loss: 0.1290, Train Acc: 100.00%, Test Loss: 0.1152, Test Acc: 100.00%\n",
      "Fold 4 Epoch [50/100], Train Loss: 0.1200, Train Acc: 100.00%, Test Loss: 0.1073, Test Acc: 100.00%\n",
      "Fold 4 Epoch [51/100], Train Loss: 0.1116, Train Acc: 100.00%, Test Loss: 0.1000, Test Acc: 100.00%\n",
      "Fold 4 Epoch [52/100], Train Loss: 0.1037, Train Acc: 100.00%, Test Loss: 0.0931, Test Acc: 100.00%\n",
      "Fold 4 Epoch [53/100], Train Loss: 0.0964, Train Acc: 100.00%, Test Loss: 0.0868, Test Acc: 100.00%\n",
      "Fold 4 Epoch [54/100], Train Loss: 0.0896, Train Acc: 100.00%, Test Loss: 0.0808, Test Acc: 100.00%\n",
      "Fold 4 Epoch [55/100], Train Loss: 0.0833, Train Acc: 100.00%, Test Loss: 0.0753, Test Acc: 100.00%\n",
      "Fold 4 Epoch [56/100], Train Loss: 0.0773, Train Acc: 100.00%, Test Loss: 0.0701, Test Acc: 100.00%\n",
      "Fold 4 Epoch [57/100], Train Loss: 0.0718, Train Acc: 100.00%, Test Loss: 0.0653, Test Acc: 100.00%\n",
      "Fold 4 Epoch [58/100], Train Loss: 0.0668, Train Acc: 100.00%, Test Loss: 0.0610, Test Acc: 100.00%\n",
      "Fold 4 Epoch [59/100], Train Loss: 0.0622, Train Acc: 100.00%, Test Loss: 0.0569, Test Acc: 100.00%\n",
      "Fold 4 Epoch [60/100], Train Loss: 0.0580, Train Acc: 100.00%, Test Loss: 0.0532, Test Acc: 100.00%\n",
      "Fold 4 Epoch [61/100], Train Loss: 0.0540, Train Acc: 100.00%, Test Loss: 0.0497, Test Acc: 100.00%\n",
      "Fold 4 Epoch [62/100], Train Loss: 0.0503, Train Acc: 100.00%, Test Loss: 0.0465, Test Acc: 100.00%\n",
      "Fold 4 Epoch [63/100], Train Loss: 0.0470, Train Acc: 100.00%, Test Loss: 0.0435, Test Acc: 100.00%\n",
      "Fold 4 Epoch [64/100], Train Loss: 0.0439, Train Acc: 100.00%, Test Loss: 0.0408, Test Acc: 100.00%\n",
      "Fold 4 Epoch [65/100], Train Loss: 0.0410, Train Acc: 100.00%, Test Loss: 0.0382, Test Acc: 100.00%\n",
      "Fold 4 Epoch [66/100], Train Loss: 0.0383, Train Acc: 100.00%, Test Loss: 0.0358, Test Acc: 100.00%\n",
      "Fold 4 Epoch [67/100], Train Loss: 0.0358, Train Acc: 100.00%, Test Loss: 0.0336, Test Acc: 100.00%\n",
      "Fold 4 Epoch [68/100], Train Loss: 0.0336, Train Acc: 100.00%, Test Loss: 0.0316, Test Acc: 100.00%\n",
      "Fold 4 Epoch [69/100], Train Loss: 0.0315, Train Acc: 100.00%, Test Loss: 0.0297, Test Acc: 100.00%\n",
      "Fold 4 Epoch [70/100], Train Loss: 0.0295, Train Acc: 100.00%, Test Loss: 0.0279, Test Acc: 100.00%\n",
      "Fold 4 Epoch [71/100], Train Loss: 0.0277, Train Acc: 100.00%, Test Loss: 0.0263, Test Acc: 100.00%\n",
      "Fold 4 Epoch [72/100], Train Loss: 0.0260, Train Acc: 100.00%, Test Loss: 0.0247, Test Acc: 100.00%\n",
      "Fold 4 Epoch [73/100], Train Loss: 0.0244, Train Acc: 100.00%, Test Loss: 0.0233, Test Acc: 100.00%\n",
      "Fold 4 Epoch [74/100], Train Loss: 0.0230, Train Acc: 100.00%, Test Loss: 0.0219, Test Acc: 100.00%\n",
      "Fold 4 Epoch [75/100], Train Loss: 0.0216, Train Acc: 100.00%, Test Loss: 0.0207, Test Acc: 100.00%\n",
      "Fold 4 Epoch [76/100], Train Loss: 0.0204, Train Acc: 100.00%, Test Loss: 0.0195, Test Acc: 100.00%\n",
      "Fold 4 Epoch [77/100], Train Loss: 0.0193, Train Acc: 100.00%, Test Loss: 0.0185, Test Acc: 100.00%\n",
      "Fold 4 Epoch [78/100], Train Loss: 0.0182, Train Acc: 100.00%, Test Loss: 0.0175, Test Acc: 100.00%\n",
      "Fold 4 Epoch [79/100], Train Loss: 0.0173, Train Acc: 100.00%, Test Loss: 0.0166, Test Acc: 100.00%\n",
      "Fold 4 Epoch [80/100], Train Loss: 0.0164, Train Acc: 100.00%, Test Loss: 0.0158, Test Acc: 100.00%\n",
      "Fold 4 Epoch [81/100], Train Loss: 0.0156, Train Acc: 100.00%, Test Loss: 0.0151, Test Acc: 100.00%\n",
      "Fold 4 Epoch [82/100], Train Loss: 0.0148, Train Acc: 100.00%, Test Loss: 0.0144, Test Acc: 100.00%\n",
      "Fold 4 Epoch [83/100], Train Loss: 0.0141, Train Acc: 100.00%, Test Loss: 0.0137, Test Acc: 100.00%\n",
      "Fold 4 Epoch [84/100], Train Loss: 0.0134, Train Acc: 100.00%, Test Loss: 0.0130, Test Acc: 100.00%\n",
      "Fold 4 Epoch [85/100], Train Loss: 0.0128, Train Acc: 100.00%, Test Loss: 0.0124, Test Acc: 100.00%\n",
      "Fold 4 Epoch [86/100], Train Loss: 0.0122, Train Acc: 100.00%, Test Loss: 0.0119, Test Acc: 100.00%\n",
      "Fold 4 Epoch [87/100], Train Loss: 0.0116, Train Acc: 100.00%, Test Loss: 0.0113, Test Acc: 100.00%\n",
      "Fold 4 Epoch [88/100], Train Loss: 0.0111, Train Acc: 100.00%, Test Loss: 0.0108, Test Acc: 100.00%\n",
      "Fold 4 Epoch [89/100], Train Loss: 0.0106, Train Acc: 100.00%, Test Loss: 0.0103, Test Acc: 100.00%\n",
      "Fold 4 Epoch [90/100], Train Loss: 0.0101, Train Acc: 100.00%, Test Loss: 0.0098, Test Acc: 100.00%\n",
      "Fold 4 Epoch [91/100], Train Loss: 0.0096, Train Acc: 100.00%, Test Loss: 0.0094, Test Acc: 100.00%\n",
      "Fold 4 Epoch [92/100], Train Loss: 0.0092, Train Acc: 100.00%, Test Loss: 0.0089, Test Acc: 100.00%\n",
      "Fold 4 Epoch [93/100], Train Loss: 0.0088, Train Acc: 100.00%, Test Loss: 0.0086, Test Acc: 100.00%\n",
      "Fold 4 Epoch [94/100], Train Loss: 0.0084, Train Acc: 100.00%, Test Loss: 0.0082, Test Acc: 100.00%\n",
      "Fold 4 Epoch [95/100], Train Loss: 0.0081, Train Acc: 100.00%, Test Loss: 0.0079, Test Acc: 100.00%\n",
      "Fold 4 Epoch [96/100], Train Loss: 0.0078, Train Acc: 100.00%, Test Loss: 0.0076, Test Acc: 100.00%\n",
      "Fold 4 Epoch [97/100], Train Loss: 0.0075, Train Acc: 100.00%, Test Loss: 0.0073, Test Acc: 100.00%\n",
      "Fold 4 Epoch [98/100], Train Loss: 0.0072, Train Acc: 100.00%, Test Loss: 0.0070, Test Acc: 100.00%\n",
      "Fold 4 Epoch [99/100], Train Loss: 0.0069, Train Acc: 100.00%, Test Loss: 0.0067, Test Acc: 100.00%\n",
      "Fold 4 Epoch [100/100], Train Loss: 0.0067, Train Acc: 100.00%, Test Loss: 0.0065, Test Acc: 100.00%\n",
      "Fold 4 Best Accuracy: 100.00%\n",
      "Fold 5/5\n",
      "Fold 5 Epoch [1/100], Train Loss: 1.0592, Train Acc: 65.52%, Test Loss: 1.0560, Test Acc: 85.71%\n",
      "Fold 5 Epoch [2/100], Train Loss: 1.0428, Train Acc: 75.86%, Test Loss: 1.0451, Test Acc: 85.71%\n",
      "Fold 5 Epoch [3/100], Train Loss: 1.0264, Train Acc: 79.31%, Test Loss: 1.0334, Test Acc: 85.71%\n",
      "Fold 5 Epoch [4/100], Train Loss: 1.0103, Train Acc: 86.21%, Test Loss: 1.0223, Test Acc: 85.71%\n",
      "Fold 5 Epoch [5/100], Train Loss: 0.9944, Train Acc: 89.66%, Test Loss: 1.0113, Test Acc: 85.71%\n",
      "Fold 5 Epoch [6/100], Train Loss: 0.9787, Train Acc: 96.55%, Test Loss: 1.0008, Test Acc: 85.71%\n",
      "Fold 5 Epoch [7/100], Train Loss: 0.9630, Train Acc: 96.55%, Test Loss: 0.9904, Test Acc: 85.71%\n",
      "Fold 5 Epoch [8/100], Train Loss: 0.9473, Train Acc: 100.00%, Test Loss: 0.9797, Test Acc: 85.71%\n",
      "Fold 5 Epoch [9/100], Train Loss: 0.9320, Train Acc: 100.00%, Test Loss: 0.9689, Test Acc: 85.71%\n",
      "Fold 5 Epoch [10/100], Train Loss: 0.9170, Train Acc: 100.00%, Test Loss: 0.9584, Test Acc: 85.71%\n",
      "Fold 5 Epoch [11/100], Train Loss: 0.9015, Train Acc: 100.00%, Test Loss: 0.9478, Test Acc: 100.00%\n",
      "Fold 5 Epoch [12/100], Train Loss: 0.8857, Train Acc: 100.00%, Test Loss: 0.9368, Test Acc: 100.00%\n",
      "Fold 5 Epoch [13/100], Train Loss: 0.8694, Train Acc: 100.00%, Test Loss: 0.9256, Test Acc: 100.00%\n",
      "Fold 5 Epoch [14/100], Train Loss: 0.8526, Train Acc: 100.00%, Test Loss: 0.9139, Test Acc: 100.00%\n",
      "Fold 5 Epoch [15/100], Train Loss: 0.8354, Train Acc: 100.00%, Test Loss: 0.9020, Test Acc: 100.00%\n",
      "Fold 5 Epoch [16/100], Train Loss: 0.8180, Train Acc: 100.00%, Test Loss: 0.8901, Test Acc: 100.00%\n",
      "Fold 5 Epoch [17/100], Train Loss: 0.8002, Train Acc: 100.00%, Test Loss: 0.8779, Test Acc: 100.00%\n",
      "Fold 5 Epoch [18/100], Train Loss: 0.7819, Train Acc: 100.00%, Test Loss: 0.8649, Test Acc: 100.00%\n",
      "Fold 5 Epoch [19/100], Train Loss: 0.7631, Train Acc: 100.00%, Test Loss: 0.8513, Test Acc: 100.00%\n",
      "Fold 5 Epoch [20/100], Train Loss: 0.7439, Train Acc: 100.00%, Test Loss: 0.8371, Test Acc: 100.00%\n",
      "Fold 5 Epoch [21/100], Train Loss: 0.7243, Train Acc: 100.00%, Test Loss: 0.8223, Test Acc: 100.00%\n",
      "Fold 5 Epoch [22/100], Train Loss: 0.7042, Train Acc: 100.00%, Test Loss: 0.8072, Test Acc: 100.00%\n",
      "Fold 5 Epoch [23/100], Train Loss: 0.6836, Train Acc: 100.00%, Test Loss: 0.7917, Test Acc: 100.00%\n",
      "Fold 5 Epoch [24/100], Train Loss: 0.6626, Train Acc: 100.00%, Test Loss: 0.7755, Test Acc: 100.00%\n",
      "Fold 5 Epoch [25/100], Train Loss: 0.6412, Train Acc: 100.00%, Test Loss: 0.7588, Test Acc: 100.00%\n",
      "Fold 5 Epoch [26/100], Train Loss: 0.6195, Train Acc: 100.00%, Test Loss: 0.7411, Test Acc: 100.00%\n",
      "Fold 5 Epoch [27/100], Train Loss: 0.5976, Train Acc: 100.00%, Test Loss: 0.7231, Test Acc: 100.00%\n",
      "Fold 5 Epoch [28/100], Train Loss: 0.5755, Train Acc: 100.00%, Test Loss: 0.7047, Test Acc: 100.00%\n",
      "Fold 5 Epoch [29/100], Train Loss: 0.5534, Train Acc: 100.00%, Test Loss: 0.6857, Test Acc: 100.00%\n",
      "Fold 5 Epoch [30/100], Train Loss: 0.5314, Train Acc: 100.00%, Test Loss: 0.6655, Test Acc: 100.00%\n",
      "Fold 5 Epoch [31/100], Train Loss: 0.5094, Train Acc: 100.00%, Test Loss: 0.6451, Test Acc: 100.00%\n",
      "Fold 5 Epoch [32/100], Train Loss: 0.4874, Train Acc: 100.00%, Test Loss: 0.6246, Test Acc: 100.00%\n",
      "Fold 5 Epoch [33/100], Train Loss: 0.4655, Train Acc: 100.00%, Test Loss: 0.6041, Test Acc: 100.00%\n",
      "Fold 5 Epoch [34/100], Train Loss: 0.4436, Train Acc: 100.00%, Test Loss: 0.5827, Test Acc: 100.00%\n",
      "Fold 5 Epoch [35/100], Train Loss: 0.4218, Train Acc: 100.00%, Test Loss: 0.5616, Test Acc: 100.00%\n",
      "Fold 5 Epoch [36/100], Train Loss: 0.4005, Train Acc: 100.00%, Test Loss: 0.5406, Test Acc: 100.00%\n",
      "Fold 5 Epoch [37/100], Train Loss: 0.3795, Train Acc: 100.00%, Test Loss: 0.5193, Test Acc: 100.00%\n",
      "Fold 5 Epoch [38/100], Train Loss: 0.3591, Train Acc: 100.00%, Test Loss: 0.4978, Test Acc: 100.00%\n",
      "Fold 5 Epoch [39/100], Train Loss: 0.3393, Train Acc: 100.00%, Test Loss: 0.4757, Test Acc: 100.00%\n",
      "Fold 5 Epoch [40/100], Train Loss: 0.3203, Train Acc: 100.00%, Test Loss: 0.4535, Test Acc: 100.00%\n",
      "Fold 5 Epoch [41/100], Train Loss: 0.3018, Train Acc: 100.00%, Test Loss: 0.4311, Test Acc: 100.00%\n",
      "Fold 5 Epoch [42/100], Train Loss: 0.2839, Train Acc: 100.00%, Test Loss: 0.4083, Test Acc: 100.00%\n",
      "Fold 5 Epoch [43/100], Train Loss: 0.2667, Train Acc: 100.00%, Test Loss: 0.3857, Test Acc: 100.00%\n",
      "Fold 5 Epoch [44/100], Train Loss: 0.2501, Train Acc: 100.00%, Test Loss: 0.3635, Test Acc: 100.00%\n",
      "Fold 5 Epoch [45/100], Train Loss: 0.2341, Train Acc: 100.00%, Test Loss: 0.3414, Test Acc: 100.00%\n",
      "Fold 5 Epoch [46/100], Train Loss: 0.2189, Train Acc: 100.00%, Test Loss: 0.3195, Test Acc: 100.00%\n",
      "Fold 5 Epoch [47/100], Train Loss: 0.2044, Train Acc: 100.00%, Test Loss: 0.2983, Test Acc: 100.00%\n",
      "Fold 5 Epoch [48/100], Train Loss: 0.1905, Train Acc: 100.00%, Test Loss: 0.2778, Test Acc: 100.00%\n",
      "Fold 5 Epoch [49/100], Train Loss: 0.1774, Train Acc: 100.00%, Test Loss: 0.2583, Test Acc: 100.00%\n",
      "Fold 5 Epoch [50/100], Train Loss: 0.1650, Train Acc: 100.00%, Test Loss: 0.2397, Test Acc: 100.00%\n",
      "Fold 5 Epoch [51/100], Train Loss: 0.1533, Train Acc: 100.00%, Test Loss: 0.2221, Test Acc: 100.00%\n",
      "Fold 5 Epoch [52/100], Train Loss: 0.1423, Train Acc: 100.00%, Test Loss: 0.2052, Test Acc: 100.00%\n",
      "Fold 5 Epoch [53/100], Train Loss: 0.1320, Train Acc: 100.00%, Test Loss: 0.1893, Test Acc: 100.00%\n",
      "Fold 5 Epoch [54/100], Train Loss: 0.1224, Train Acc: 100.00%, Test Loss: 0.1743, Test Acc: 100.00%\n",
      "Fold 5 Epoch [55/100], Train Loss: 0.1134, Train Acc: 100.00%, Test Loss: 0.1603, Test Acc: 100.00%\n",
      "Fold 5 Epoch [56/100], Train Loss: 0.1050, Train Acc: 100.00%, Test Loss: 0.1472, Test Acc: 100.00%\n",
      "Fold 5 Epoch [57/100], Train Loss: 0.0972, Train Acc: 100.00%, Test Loss: 0.1350, Test Acc: 100.00%\n",
      "Fold 5 Epoch [58/100], Train Loss: 0.0900, Train Acc: 100.00%, Test Loss: 0.1237, Test Acc: 100.00%\n",
      "Fold 5 Epoch [59/100], Train Loss: 0.0834, Train Acc: 100.00%, Test Loss: 0.1135, Test Acc: 100.00%\n",
      "Fold 5 Epoch [60/100], Train Loss: 0.0773, Train Acc: 100.00%, Test Loss: 0.1042, Test Acc: 100.00%\n",
      "Fold 5 Epoch [61/100], Train Loss: 0.0716, Train Acc: 100.00%, Test Loss: 0.0956, Test Acc: 100.00%\n",
      "Fold 5 Epoch [62/100], Train Loss: 0.0664, Train Acc: 100.00%, Test Loss: 0.0877, Test Acc: 100.00%\n",
      "Fold 5 Epoch [63/100], Train Loss: 0.0616, Train Acc: 100.00%, Test Loss: 0.0805, Test Acc: 100.00%\n",
      "Fold 5 Epoch [64/100], Train Loss: 0.0571, Train Acc: 100.00%, Test Loss: 0.0740, Test Acc: 100.00%\n",
      "Fold 5 Epoch [65/100], Train Loss: 0.0530, Train Acc: 100.00%, Test Loss: 0.0682, Test Acc: 100.00%\n",
      "Fold 5 Epoch [66/100], Train Loss: 0.0492, Train Acc: 100.00%, Test Loss: 0.0630, Test Acc: 100.00%\n",
      "Fold 5 Epoch [67/100], Train Loss: 0.0458, Train Acc: 100.00%, Test Loss: 0.0582, Test Acc: 100.00%\n",
      "Fold 5 Epoch [68/100], Train Loss: 0.0426, Train Acc: 100.00%, Test Loss: 0.0539, Test Acc: 100.00%\n",
      "Fold 5 Epoch [69/100], Train Loss: 0.0397, Train Acc: 100.00%, Test Loss: 0.0500, Test Acc: 100.00%\n",
      "Fold 5 Epoch [70/100], Train Loss: 0.0370, Train Acc: 100.00%, Test Loss: 0.0464, Test Acc: 100.00%\n",
      "Fold 5 Epoch [71/100], Train Loss: 0.0345, Train Acc: 100.00%, Test Loss: 0.0430, Test Acc: 100.00%\n",
      "Fold 5 Epoch [72/100], Train Loss: 0.0323, Train Acc: 100.00%, Test Loss: 0.0400, Test Acc: 100.00%\n",
      "Fold 5 Epoch [73/100], Train Loss: 0.0302, Train Acc: 100.00%, Test Loss: 0.0372, Test Acc: 100.00%\n",
      "Fold 5 Epoch [74/100], Train Loss: 0.0282, Train Acc: 100.00%, Test Loss: 0.0347, Test Acc: 100.00%\n",
      "Fold 5 Epoch [75/100], Train Loss: 0.0265, Train Acc: 100.00%, Test Loss: 0.0324, Test Acc: 100.00%\n",
      "Fold 5 Epoch [76/100], Train Loss: 0.0249, Train Acc: 100.00%, Test Loss: 0.0303, Test Acc: 100.00%\n",
      "Fold 5 Epoch [77/100], Train Loss: 0.0234, Train Acc: 100.00%, Test Loss: 0.0285, Test Acc: 100.00%\n",
      "Fold 5 Epoch [78/100], Train Loss: 0.0220, Train Acc: 100.00%, Test Loss: 0.0268, Test Acc: 100.00%\n",
      "Fold 5 Epoch [79/100], Train Loss: 0.0208, Train Acc: 100.00%, Test Loss: 0.0252, Test Acc: 100.00%\n",
      "Fold 5 Epoch [80/100], Train Loss: 0.0196, Train Acc: 100.00%, Test Loss: 0.0237, Test Acc: 100.00%\n",
      "Fold 5 Epoch [81/100], Train Loss: 0.0185, Train Acc: 100.00%, Test Loss: 0.0223, Test Acc: 100.00%\n",
      "Fold 5 Epoch [82/100], Train Loss: 0.0176, Train Acc: 100.00%, Test Loss: 0.0211, Test Acc: 100.00%\n",
      "Fold 5 Epoch [83/100], Train Loss: 0.0167, Train Acc: 100.00%, Test Loss: 0.0200, Test Acc: 100.00%\n",
      "Fold 5 Epoch [84/100], Train Loss: 0.0158, Train Acc: 100.00%, Test Loss: 0.0190, Test Acc: 100.00%\n",
      "Fold 5 Epoch [85/100], Train Loss: 0.0150, Train Acc: 100.00%, Test Loss: 0.0180, Test Acc: 100.00%\n",
      "Fold 5 Epoch [86/100], Train Loss: 0.0143, Train Acc: 100.00%, Test Loss: 0.0172, Test Acc: 100.00%\n",
      "Fold 5 Epoch [87/100], Train Loss: 0.0136, Train Acc: 100.00%, Test Loss: 0.0163, Test Acc: 100.00%\n",
      "Fold 5 Epoch [88/100], Train Loss: 0.0129, Train Acc: 100.00%, Test Loss: 0.0156, Test Acc: 100.00%\n",
      "Fold 5 Epoch [89/100], Train Loss: 0.0123, Train Acc: 100.00%, Test Loss: 0.0149, Test Acc: 100.00%\n",
      "Fold 5 Epoch [90/100], Train Loss: 0.0117, Train Acc: 100.00%, Test Loss: 0.0143, Test Acc: 100.00%\n",
      "Fold 5 Epoch [91/100], Train Loss: 0.0112, Train Acc: 100.00%, Test Loss: 0.0137, Test Acc: 100.00%\n",
      "Fold 5 Epoch [92/100], Train Loss: 0.0107, Train Acc: 100.00%, Test Loss: 0.0130, Test Acc: 100.00%\n",
      "Fold 5 Epoch [93/100], Train Loss: 0.0102, Train Acc: 100.00%, Test Loss: 0.0125, Test Acc: 100.00%\n",
      "Fold 5 Epoch [94/100], Train Loss: 0.0098, Train Acc: 100.00%, Test Loss: 0.0119, Test Acc: 100.00%\n",
      "Fold 5 Epoch [95/100], Train Loss: 0.0093, Train Acc: 100.00%, Test Loss: 0.0114, Test Acc: 100.00%\n",
      "Fold 5 Epoch [96/100], Train Loss: 0.0089, Train Acc: 100.00%, Test Loss: 0.0109, Test Acc: 100.00%\n",
      "Fold 5 Epoch [97/100], Train Loss: 0.0086, Train Acc: 100.00%, Test Loss: 0.0105, Test Acc: 100.00%\n",
      "Fold 5 Epoch [98/100], Train Loss: 0.0082, Train Acc: 100.00%, Test Loss: 0.0101, Test Acc: 100.00%\n",
      "Fold 5 Epoch [99/100], Train Loss: 0.0079, Train Acc: 100.00%, Test Loss: 0.0097, Test Acc: 100.00%\n",
      "Fold 5 Epoch [100/100], Train Loss: 0.0076, Train Acc: 100.00%, Test Loss: 0.0093, Test Acc: 100.00%\n",
      "Fold 5 Best Accuracy: 100.00%\n",
      "\n",
      "5-Fold CV Results (Stephen Only):\n",
      "Average Accuracy: 100.00% (±0.00)\n",
      "Fold Accuracies: ['100.00%', '100.00%', '100.00%', '100.00%', '100.00%']\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation setup\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "fold_losses = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X_all)):\n",
    "    print(f'Fold {fold + 1}/{n_folds}')\n",
    "    \n",
    "    # Split data\n",
    "    X_train_fold = X_all[train_idx]\n",
    "    y_train_fold = y_all[train_idx]\n",
    "    X_test_fold = X_all[test_idx]\n",
    "    y_test_fold = y_all[test_idx]\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_train_fold, y_train_fold)\n",
    "    test_dataset = TensorDataset(X_test_fold, y_test_fold)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = FeatureModel(num_classes=NUM_CLASSES, input_features=18)\n",
    "    optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-4)\n",
    "    \n",
    "    # Training loop\n",
    "    best_test_loss = float('inf')\n",
    "    best_test_accuracy = 0\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        train_loss, train_accuracy = evaluate_model(train_loader, model, criterion)\n",
    "        test_loss, test_accuracy = evaluate_model(test_loader, model, criterion)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        # Log and print\n",
    "        log_message = (f\"Fold {fold+1} Epoch [{epoch+1}/100], \"\n",
    "                      f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
    "                      f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\")\n",
    "        logging.info(log_message)\n",
    "        print(log_message)\n",
    "        \n",
    "        # Save best model\n",
    "        if test_accuracy > best_test_accuracy:\n",
    "            best_test_accuracy = test_accuracy\n",
    "            best_test_loss = test_loss\n",
    "            save_best_model(epoch, model, optimizer, test_loss, test_accuracy, train_losses, test_losses)\n",
    "    \n",
    "    # Store fold results\n",
    "    fold_accuracies.append(best_test_accuracy)\n",
    "    fold_losses.append(best_test_loss)\n",
    "    print(f'Fold {fold + 1} Best Accuracy: {best_test_accuracy:.2f}%')\n",
    "    \n",
    "# Print summary\n",
    "mean_accuracy = np.mean(fold_accuracies)\n",
    "std_accuracy = np.std(fold_accuracies)\n",
    "print(f'\\n5-Fold CV Results (Stephen Only):')\n",
    "print(f'Average Accuracy: {mean_accuracy:.2f}% (±{std_accuracy:.2f})')\n",
    "print(f'Fold Accuracies: {[f\"{acc:.2f}%\" for acc in fold_accuracies]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37f5a0a4-374a-40dd-9d27-81279b433394",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunk_sizes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plotting\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mchunk_sizes\u001b[49m, fold_accuracies, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbo-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChunk Size (seconds)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy (\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chunk_sizes' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(chunk_sizes, fold_accuracies, 'bo-')\n",
    "plt.xlabel('Chunk Size (seconds)')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.title('LSTM Model Accuracy vs Chunk Size')\n",
    "plt.grid(True)\n",
    "\n",
    "# Add accuracy values as text\n",
    "for i, (x, y) in enumerate(zip(chunk_sizes, fold_accuracies)):\n",
    "    plt.text(x, y, f'{y:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.savefig('lstm_chunk_size_vs_accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "# Print results table\n",
    "results = pd.DataFrame({\n",
    "    'Chunk Size (s)': chunk_sizes,\n",
    "    'Accuracy (%)': fold_accuracies\n",
    "})\n",
    "print(\"\\nLSTM Results Summary:\")\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6c79cb82-164f-4ed0-ba0b-63dcc6f6bbc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Activities for the Confusion matrix\n",
    "ACTIVITIES = ['sit', 'walk','upstair']\n",
    "\n",
    "# Model evaluation (confusion matrix)\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Loop through the test loader to collect predictions and true labels\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Compute the confusion matrix, explicitly specifying the labels\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions, labels=[0, 1, 2]) \n",
    "\n",
    "# Assuming conf_matrix and ACTIVITIES are already defined\n",
    "class_accuracies = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)  # Compute per-class accuracy\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(conf_matrix, display_labels=ACTIVITIES)\n",
    "fig, ax = plt.subplots()\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "\n",
    "# Add per-class accuracy text\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "    acc_text = f\"{class_accuracies[i] * 100:.2f}%\"\n",
    "    ax.text(\n",
    "        len(ACTIVITIES) + 0.3, i, acc_text, \n",
    "        fontsize=12, verticalalignment='center', color='black'\n",
    "    )\n",
    "\n",
    "plt.title(\"Confusion Matrix for Activities with Per-Class Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d0f841cd-23c7-425c-a8c0-6e363b2c94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "incorrect_samples = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)  # Get raw logits\n",
    "        probabilities = F.softmax(outputs, dim=1)  # Convert to probabilities\n",
    "        predicted_labels = torch.argmax(probabilities, dim=1)  # Get predicted class\n",
    "        confidence_scores = torch.max(probabilities, dim=1).values  # Get max confidence\n",
    "\n",
    "        # Find incorrect predictions\n",
    "        incorrect_indices = (predicted_labels != batch_y)\n",
    "        for i in range(len(batch_y)):\n",
    "            if incorrect_indices[i]:\n",
    "                incorrect_samples.append({\n",
    "                    \"True Label\": batch_y[i].item(),\n",
    "                    \"Predicted Label\": predicted_labels[i].item(),\n",
    "                    \"Confidence\": confidence_scores[i].item(),\n",
    "                    \"Probabilities\": probabilities[i].tolist()\n",
    "                })\n",
    "\n",
    "# Print results\n",
    "for sample in incorrect_samples:\n",
    "    print(f\"True Label: {sample['True Label']}, Predicted: {sample['Predicted Label']}, Confidence: {sample['Confidence']:.4f}\")\n",
    "    print(f\"Full Probabilities: {sample['Probabilities']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5e8554-cc92-4315-a317-edeb8c823675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
