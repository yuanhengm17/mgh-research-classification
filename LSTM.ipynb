{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30e998e0-f6cd-4d3d-82f0-08637032af9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The default Firebase app already exists. This means you called initialize_app() more than once without providing an app name as the second argument. In most cases you only need to call initialize_app() once. But if you do want to initialize multiple apps, pass a second argument to initialize_app() to give each app a unique name.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Firebase Initialization\u001b[39;00m\n\u001b[0;32m     17\u001b[0m cred \u001b[38;5;241m=\u001b[39m credentials\u001b[38;5;241m.\u001b[39mCertificate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madminkey.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mfirebase_admin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m db \u001b[38;5;241m=\u001b[39m firestore\u001b[38;5;241m.\u001b[39mclient()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\firebase_admin\\__init__.py:74\u001b[0m, in \u001b[0;36minitialize_app\u001b[1;34m(credential, options, name)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m app\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m _DEFAULT_APP_NAME:\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m((\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe default Firebase app already exists. This means you called \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitialize_app() more than once without providing an app name as \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe second argument. In most cases you only need to call \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitialize_app() once. But if you do want to initialize multiple \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapps, pass a second argument to initialize_app() to give each app \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma unique name.\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m((\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFirebase app named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m already exists. This means you called \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitialize_app() more than once with the same app name as the \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecond argument. Make sure you provide a unique name every time \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myou call initialize_app().\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(name))\n",
      "\u001b[1;31mValueError\u001b[0m: The default Firebase app already exists. This means you called initialize_app() more than once without providing an app name as the second argument. In most cases you only need to call initialize_app() once. But if you do want to initialize multiple apps, pass a second argument to initialize_app() to give each app a unique name."
     ]
    }
   ],
   "source": [
    "import firebase_admin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from firebase_admin import credentials, firestore\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Firebase Initialization\n",
    "cred = credentials.Certificate(\"adminkey.json\")\n",
    "firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename='training_log.txt', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "INCLUDE_ONLY = ['Stephen', 'Lillian', 'Ren', 'Yuanheng', 'Ethan Shao']\n",
    "ACTIVITIES = ['sit','walk','upstair']\n",
    "CHUNK_SIZE = 2.375  # in seconds (can be a decimal)\n",
    "SAMPLING_RATE = 100  # Hz\n",
    "NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "287ad27b-8007-4ccd-b113-f8016c2aaec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Structures\n",
    "def fetch_data(collection_name, activities, include_only, time_start=500, time_end=6000):\n",
    "    \"\"\"Fetch and preprocess data from Firestore.\"\"\"\n",
    "    data, docs = [], []\n",
    "    for person in db.collection(collection_name).stream():\n",
    "        person_name = str(person.to_dict().get('name', ''))\n",
    "        if person_name not in include_only:\n",
    "            continue\n",
    "\n",
    "        for activity in activities:\n",
    "            for recording in db.collection(collection_name).document(person_name).collection(activity).stream():\n",
    "                record = recording.to_dict()\n",
    "                if 'acceleration' not in record:\n",
    "                    continue\n",
    "\n",
    "                docs.append(record)\n",
    "                df = pd.DataFrame(record['acceleration'])\n",
    "                \n",
    "                if 'time' in df.columns:\n",
    "                    filtered_df = df[(df['time'] >= time_start) & (df['time'] <= time_end)]\n",
    "                    data.append(filtered_df)\n",
    "                else:\n",
    "                    raise ValueError(\"The 'acceleration' field must include a 'time' column.\")\n",
    "    return data, docs\n",
    "\n",
    "# Fetch and preprocess training/testing data\n",
    "training_data_raw, training_docs = fetch_data(\"training\", ACTIVITIES, INCLUDE_ONLY)\n",
    "testing_data_raw, testing_docs = fetch_data(\"testing\", ACTIVITIES, INCLUDE_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1284a61-2522-4348-abbf-1d4e31e36bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to chunk data into smaller segments for training/testing\n",
    "def chunk_data(data_raw, docs, chunk_size, activities, sampling_rate):\n",
    "    \"\"\"Split data into chunks and assign labels.\"\"\"\n",
    "    data, labels = [], []\n",
    "    activity_distribution = np.zeros(len(activities))\n",
    "    chunk_samples = int(chunk_size * sampling_rate)  # Convert time to sample count\n",
    "\n",
    "    for i in range(len(data_raw)):\n",
    "        num_chunks = len(data_raw[i]) // chunk_samples  # Number of full chunks\n",
    "        for j in range(num_chunks):\n",
    "            start = j * chunk_samples\n",
    "            end = start + chunk_samples\n",
    "            x = list(data_raw[i][\"x\"])[start:end]\n",
    "            y = list(data_raw[i][\"y\"])[start:end]\n",
    "            z = list(data_raw[i][\"z\"])[start:end]\n",
    "            activity = docs[i]['activity']\n",
    "            label = activities.index(activity)\n",
    "\n",
    "            activity_distribution[label] += 1\n",
    "            data.append([x, y, z])\n",
    "            labels.append(label)\n",
    "\n",
    "    return data, labels, activity_distribution\n",
    "\n",
    "# Chunk the data\n",
    "training_data, training_labels, training_distribution = chunk_data(training_data_raw, training_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE)\n",
    "testing_data, testing_labels, testing_distribution = chunk_data(testing_data_raw, testing_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43de7965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset summary:\n",
      "+----------+------------------+\n",
      "| Dataset  | number of chunks |\n",
      "+----------+------------------+\n",
      "| training |        60        |\n",
      "| testing  |        60        |\n",
      "+----------+------------------+\n",
      "Training Activities Count\n",
      "sit: 20 chunks\n",
      "walk: 20 chunks\n",
      "upstair: 20 chunks\n",
      "\n",
      "Testing Activity Count\n",
      "sit:20 chunks\n",
      "walk:20 chunks\n",
      "upstair:20 chunks\n",
      "60\n",
      "60\n",
      "(60, 3, 237)\n",
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate #for table formatting\n",
    "\n",
    "#Calculate the number of training and testing samples\n",
    "num_training_samples = len(training_data)\n",
    "num_testing_samples = len(testing_data)\n",
    "\n",
    "#table\n",
    "summary_table = [[\"training\", num_training_samples], [\"testing\", num_testing_samples]]\n",
    "\n",
    "#print\n",
    "print(\"dataset summary:\")\n",
    "print(tabulate(summary_table, headers = [\"Dataset\", \"number of chunks\"], tablefmt=\"pretty\"))\n",
    "\n",
    "print(\"Training Activities Count\")\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "    print(f\"{activity}: {int(training_distribution[i])} chunks\")\n",
    "\n",
    "print(\"\\nTesting Activity Count\")\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "    print(f\"{activity}:{int(testing_distribution[i])} chunks\")\n",
    "print(len(training_data))\n",
    "print(len(testing_data))\n",
    "print(np.array(training_data).shape)\n",
    "print(len(training_data_raw))\n",
    "print(len(testing_data_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fae1da3e-3e2c-40ad-82cb-6d090f2cc7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all data for cross-validation\n",
    "all_data = np.array(training_data + testing_data)\n",
    "all_labels = np.array(training_labels + testing_labels)\n",
    "\n",
    "# Normalize the entire dataset\n",
    "scaler = StandardScaler()\n",
    "X_all = scaler.fit_transform(all_data.reshape(-1, all_data.shape[-1])).reshape(all_data.shape)\n",
    "y_all = all_labels\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_all = torch.tensor(X_all, dtype=torch.float32)\n",
    "y_all = torch.tensor(y_all, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ec68f8d-85ff-48e1-b219-a547e9393c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedLSTMModel(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels, seq_length, hidden_size=128, num_layers=2):\n",
    "        super(OptimizedLSTMModel, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer expects input of shape (batch_size, seq_length, input_size)\n",
    "        self.lstm = nn.LSTM(input_channels, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to output predictions\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM requires a hidden state and cell state to be initialized\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Use the output of the last timestep for classification\n",
    "        out = out[:, -1, :]  # Take the last hidden state\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fea2d82-2721-4226-87c8-d0386cb60e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save best model and metadata\n",
    "BEST_MODEL_PATH = \"best_model.pth\"\n",
    "BEST_METADATA_PATH = \"best_model.json\"\n",
    "\n",
    "# Function to save best model\n",
    "def save_best_model(epoch, model, optimizer, loss, accuracy, train_losses, test_losses):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses,\n",
    "    }\n",
    "    torch.save(checkpoint, BEST_MODEL_PATH)\n",
    "\n",
    "\n",
    "    # Save metadata\n",
    "    with open(BEST_METADATA_PATH, \"w\") as f:\n",
    "        json.dump({\"epoch\": epoch, \"test_loss\": test_loss, \"avg_accuracy\": avg_accuracy}, f)\n",
    "\n",
    "# Function to load best model if exists\n",
    "def load_best_model(model, optimizer, best_model_path=BEST_MODEL_PATH):\n",
    "    if os.path.exists(best_model_path):\n",
    "        checkpoint = torch.load(best_model_path)\n",
    "        \n",
    "        # Print checkpoint keys to understand its structure\n",
    "        print(\"Checkpoint keys:\", checkpoint.keys())\n",
    "        \n",
    "        # Load model weights\n",
    "        model_state_dict = model.state_dict()\n",
    "        checkpoint_state_dict = checkpoint['model_state_dict']\n",
    "        \n",
    "        # Filter out incompatible keys\n",
    "        filtered_checkpoint_state_dict = {k: v for k, v in checkpoint_state_dict.items() if k in model_state_dict}\n",
    "        model_state_dict.update(filtered_checkpoint_state_dict)\n",
    "        model.load_state_dict(model_state_dict)\n",
    "        \n",
    "        # Optionally, don't load the optimizer state to avoid the error\n",
    "        # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        # Load other metadata if available\n",
    "        start_epoch = checkpoint.get('epoch', 0)\n",
    "        best_loss = checkpoint.get('loss', float('inf'))  # Default to infinity if loss is missing\n",
    "        best_avg_accuracy = checkpoint.get('accuracy', 0)  # Default to 0 if accuracy is missing\n",
    "        \n",
    "        # If train_losses and test_losses aren't saved, return empty lists or placeholders\n",
    "        train_losses = checkpoint.get('train_losses', [])\n",
    "        test_losses = checkpoint.get('test_losses', [])\n",
    "        \n",
    "        return start_epoch, best_loss, best_avg_accuracy, train_losses, test_losses\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting from scratch.\")\n",
    "        # Return default values\n",
    "        return 0, float('inf'), 0, [], []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afb6cc02-f6d5-441e-8ea1-736efc276305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(loader, model, criterion, transpose=False):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in loader:\n",
    "            if transpose:\n",
    "                batch_X = batch_X.transpose(1, 2)  # Transpose for LSTM\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01254ebf-d89d-4173-9f04-ba08155090cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Data Augmentation Functions\n",
    "def add_jitter(batch_X, noise_level=0.01):\n",
    "    noise = torch.randn_like(batch_X) * noise_level\n",
    "    return batch_X + noise\n",
    "\n",
    "def scale_signal(batch_X, scale_range=(0.9, 1.1)):\n",
    "    scale_factor = torch.FloatTensor(batch_X.shape[0], 1, 1).uniform_(*scale_range)\n",
    "    return batch_X * scale_factor\n",
    "\n",
    "def time_warp(batch_X, sigma=0.2):\n",
    "    batch_size, num_channels, seq_length = batch_X.shape\n",
    "    time_steps = np.arange(seq_length)\n",
    "    warping_curve = np.cumsum(np.random.normal(0, sigma, size=(batch_size, seq_length)), axis=1)\n",
    "    warping_curve = (warping_curve - warping_curve.min(axis=1, keepdims=True)) / \\\n",
    "                    (warping_curve.max(axis=1, keepdims=True) - warping_curve.min(axis=1, keepdims=True)) * seq_length\n",
    "    warped_X = torch.zeros_like(batch_X)\n",
    "    for i in range(batch_size):\n",
    "        for j in range(num_channels):\n",
    "            f = interp1d(time_steps, batch_X[i, j, :].cpu().numpy(), kind='linear', fill_value='extrapolate')\n",
    "            warped_X[i, j, :] = torch.tensor(f(warping_curve[i]), dtype=batch_X.dtype)\n",
    "    return warped_X\n",
    "\n",
    "def random_crop(batch_X, crop_size=0.9, target_length=None):\n",
    "    batch_size, num_channels, seq_length = batch_X.shape\n",
    "    new_length = int(seq_length * crop_size)\n",
    "    start_idx = torch.randint(0, seq_length - new_length + 1, (batch_size,))\n",
    "    cropped_X = torch.zeros(batch_size, num_channels, new_length)\n",
    "    for i in range(batch_size):\n",
    "        cropped_X[i] = batch_X[i, :, start_idx[i]:start_idx[i] + new_length]\n",
    "    \n",
    "    if target_length is not None:\n",
    "        if new_length < target_length:\n",
    "            padding = torch.zeros(batch_size, num_channels, target_length - new_length)\n",
    "            cropped_X = torch.cat([cropped_X, padding], dim=2)\n",
    "        elif new_length > target_length:\n",
    "            cropped_X = cropped_X[:, :, :target_length]\n",
    "    return cropped_X\n",
    "\n",
    "def permute_segments(batch_X, num_segments=5):\n",
    "    batch_size, num_channels, seq_length = batch_X.shape\n",
    "    segment_length = seq_length // num_segments\n",
    "    permuted_X = batch_X.clone()\n",
    "    for i in range(batch_size):\n",
    "        perm = torch.randperm(num_segments)\n",
    "        for j in range(num_segments):\n",
    "            permuted_X[i, :, j * segment_length:(j + 1) * segment_length] = \\\n",
    "                batch_X[i, :, perm[j] * segment_length:(perm[j] + 1) * segment_length]\n",
    "    return permuted_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc68f1f4-225f-455c-b6d0-80a76a3ce598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint keys: dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'test_loss', 'train_losses', 'test_losses'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minsu\\AppData\\Local\\Temp\\ipykernel_21472\\2561067316.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(best_model_path)\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, optimizer, and loss function\n",
    "model = OptimizedLSTMModel(num_classes=NUM_CLASSES, input_channels=3, seq_length=int(CHUNK_SIZE * 100))\n",
    "optimizer = Adam(model.parameters(), lr=0.02, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50, min_lr=1e-5)\n",
    "\n",
    "# Load best model if exists\n",
    "start_epoch, best_loss, best_avg_accuracy, train_losses, test_losses = load_best_model(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd628c08-376b-468a-ab5f-6c1a88c7587c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Fold 1 Epoch [1/200], Train Loss: 1.4501, Train Acc: 31.25%, Val Loss: 1.4317, Val Acc: 41.67%\n",
      "Fold 1 Epoch [2/200], Train Loss: 0.9862, Train Acc: 47.92%, Val Loss: 1.0609, Val Acc: 41.67%\n",
      "Fold 1 Epoch [3/200], Train Loss: 0.8763, Train Acc: 54.17%, Val Loss: 1.0216, Val Acc: 54.17%\n",
      "Fold 1 Epoch [4/200], Train Loss: 0.8233, Train Acc: 64.58%, Val Loss: 1.2758, Val Acc: 45.83%\n",
      "Fold 1 Epoch [5/200], Train Loss: 0.6915, Train Acc: 71.88%, Val Loss: 0.9299, Val Acc: 66.67%\n",
      "Fold 1 Epoch [6/200], Train Loss: 0.9864, Train Acc: 59.38%, Val Loss: 1.0635, Val Acc: 45.83%\n",
      "Fold 1 Epoch [7/200], Train Loss: 0.7901, Train Acc: 66.67%, Val Loss: 1.1106, Val Acc: 54.17%\n",
      "Fold 1 Epoch [8/200], Train Loss: 0.6931, Train Acc: 69.79%, Val Loss: 0.9893, Val Acc: 58.33%\n",
      "Fold 1 Epoch [9/200], Train Loss: 0.6750, Train Acc: 67.71%, Val Loss: 0.9378, Val Acc: 62.50%\n",
      "Fold 1 Epoch [10/200], Train Loss: 0.6499, Train Acc: 72.92%, Val Loss: 0.8324, Val Acc: 70.83%\n",
      "Fold 1 Epoch [11/200], Train Loss: 0.6027, Train Acc: 71.88%, Val Loss: 0.6744, Val Acc: 66.67%\n",
      "Fold 1 Epoch [12/200], Train Loss: 0.9919, Train Acc: 65.62%, Val Loss: 0.9136, Val Acc: 66.67%\n",
      "Fold 1 Epoch [13/200], Train Loss: 1.3285, Train Acc: 46.88%, Val Loss: 1.0806, Val Acc: 54.17%\n",
      "Fold 1 Epoch [14/200], Train Loss: 1.1660, Train Acc: 47.92%, Val Loss: 1.3214, Val Acc: 45.83%\n",
      "Fold 1 Epoch [15/200], Train Loss: 1.0219, Train Acc: 48.96%, Val Loss: 1.0951, Val Acc: 45.83%\n",
      "Fold 1 Epoch [16/200], Train Loss: 0.9069, Train Acc: 66.67%, Val Loss: 0.9697, Val Acc: 58.33%\n",
      "Fold 1 Epoch [17/200], Train Loss: 1.0722, Train Acc: 44.79%, Val Loss: 1.4139, Val Acc: 29.17%\n",
      "Fold 1 Epoch [18/200], Train Loss: 0.8354, Train Acc: 58.33%, Val Loss: 1.1286, Val Acc: 41.67%\n",
      "Fold 1 Epoch [19/200], Train Loss: 1.0865, Train Acc: 52.08%, Val Loss: 1.0473, Val Acc: 41.67%\n",
      "Fold 1 Epoch [20/200], Train Loss: 1.1006, Train Acc: 35.42%, Val Loss: 1.2361, Val Acc: 29.17%\n",
      "Fold 1 Epoch [21/200], Train Loss: 1.1551, Train Acc: 36.46%, Val Loss: 1.3484, Val Acc: 29.17%\n",
      "Fold 1 Epoch [22/200], Train Loss: 1.0510, Train Acc: 36.46%, Val Loss: 1.1001, Val Acc: 33.33%\n",
      "Fold 1 Epoch [23/200], Train Loss: 1.0308, Train Acc: 29.17%, Val Loss: 1.0131, Val Acc: 41.67%\n",
      "Fold 1 Epoch [24/200], Train Loss: 0.9832, Train Acc: 62.50%, Val Loss: 1.0476, Val Acc: 50.00%\n",
      "Fold 1 Epoch [25/200], Train Loss: 1.0798, Train Acc: 31.25%, Val Loss: 1.0419, Val Acc: 37.50%\n",
      "Fold 1 Epoch [26/200], Train Loss: 0.9494, Train Acc: 61.46%, Val Loss: 1.1346, Val Acc: 45.83%\n",
      "Fold 1 Epoch [27/200], Train Loss: 0.8861, Train Acc: 69.79%, Val Loss: 1.1430, Val Acc: 54.17%\n",
      "Fold 1 Epoch [28/200], Train Loss: 0.7853, Train Acc: 73.96%, Val Loss: 1.2144, Val Acc: 62.50%\n",
      "Fold 1 Epoch [29/200], Train Loss: 0.7794, Train Acc: 68.75%, Val Loss: 1.2342, Val Acc: 50.00%\n",
      "Fold 1 Epoch [30/200], Train Loss: 0.6904, Train Acc: 67.71%, Val Loss: 1.3256, Val Acc: 50.00%\n",
      "Fold 1 Epoch [31/200], Train Loss: 0.7020, Train Acc: 67.71%, Val Loss: 1.2050, Val Acc: 50.00%\n",
      "Fold 1 Epoch [32/200], Train Loss: 0.7506, Train Acc: 70.83%, Val Loss: 1.1410, Val Acc: 62.50%\n",
      "Fold 1 Epoch [33/200], Train Loss: 0.7321, Train Acc: 70.83%, Val Loss: 1.0882, Val Acc: 58.33%\n",
      "Fold 1 Epoch [34/200], Train Loss: 0.7173, Train Acc: 69.79%, Val Loss: 1.1410, Val Acc: 50.00%\n",
      "Fold 1 Epoch [35/200], Train Loss: 0.6560, Train Acc: 69.79%, Val Loss: 1.0638, Val Acc: 58.33%\n",
      "Fold 1 Epoch [36/200], Train Loss: 0.7882, Train Acc: 68.75%, Val Loss: 1.2329, Val Acc: 58.33%\n",
      "Fold 1 Epoch [37/200], Train Loss: 0.7639, Train Acc: 63.54%, Val Loss: 1.1181, Val Acc: 45.83%\n",
      "Fold 1 Epoch [38/200], Train Loss: 0.9144, Train Acc: 51.04%, Val Loss: 1.0731, Val Acc: 37.50%\n",
      "Fold 1 Epoch [39/200], Train Loss: 0.8896, Train Acc: 72.92%, Val Loss: 0.9407, Val Acc: 70.83%\n",
      "Fold 1 Epoch [40/200], Train Loss: 0.7749, Train Acc: 71.88%, Val Loss: 0.9585, Val Acc: 66.67%\n",
      "Fold 1 Epoch [41/200], Train Loss: 0.6829, Train Acc: 78.12%, Val Loss: 0.9117, Val Acc: 54.17%\n",
      "Fold 1 Epoch [42/200], Train Loss: 0.7319, Train Acc: 64.58%, Val Loss: 1.0010, Val Acc: 41.67%\n",
      "Fold 1 Epoch [43/200], Train Loss: 0.6307, Train Acc: 80.21%, Val Loss: 0.9808, Val Acc: 66.67%\n",
      "Fold 1 Epoch [44/200], Train Loss: 0.6423, Train Acc: 76.04%, Val Loss: 1.0501, Val Acc: 58.33%\n",
      "Fold 1 Epoch [45/200], Train Loss: 0.6471, Train Acc: 72.92%, Val Loss: 1.2084, Val Acc: 62.50%\n",
      "Fold 1 Epoch [46/200], Train Loss: 0.7237, Train Acc: 58.33%, Val Loss: 1.1576, Val Acc: 37.50%\n",
      "Fold 1 Epoch [47/200], Train Loss: 0.8161, Train Acc: 60.42%, Val Loss: 1.4494, Val Acc: 58.33%\n",
      "Fold 1 Epoch [48/200], Train Loss: 0.7090, Train Acc: 66.67%, Val Loss: 1.0055, Val Acc: 45.83%\n",
      "Fold 1 Epoch [49/200], Train Loss: 0.7333, Train Acc: 64.58%, Val Loss: 1.0055, Val Acc: 50.00%\n",
      "Fold 1 Epoch [50/200], Train Loss: 0.7259, Train Acc: 73.96%, Val Loss: 0.9440, Val Acc: 70.83%\n",
      "Fold 1 Epoch [51/200], Train Loss: 0.7608, Train Acc: 70.83%, Val Loss: 1.0951, Val Acc: 66.67%\n",
      "Fold 1 Epoch [52/200], Train Loss: 0.7794, Train Acc: 65.62%, Val Loss: 1.3352, Val Acc: 50.00%\n",
      "Fold 1 Epoch [53/200], Train Loss: 1.8116, Train Acc: 34.38%, Val Loss: 1.7023, Val Acc: 37.50%\n",
      "Fold 1 Epoch [54/200], Train Loss: 0.8418, Train Acc: 40.62%, Val Loss: 1.2259, Val Acc: 41.67%\n",
      "Fold 1 Epoch [55/200], Train Loss: 1.1535, Train Acc: 45.83%, Val Loss: 1.6341, Val Acc: 33.33%\n",
      "Fold 1 Epoch [56/200], Train Loss: 0.9257, Train Acc: 52.08%, Val Loss: 1.4152, Val Acc: 37.50%\n",
      "Fold 1 Epoch [57/200], Train Loss: 0.8162, Train Acc: 50.00%, Val Loss: 1.1857, Val Acc: 41.67%\n",
      "Fold 1 Epoch [58/200], Train Loss: 0.8753, Train Acc: 50.00%, Val Loss: 0.9613, Val Acc: 37.50%\n",
      "Fold 1 Epoch [59/200], Train Loss: 0.8295, Train Acc: 52.08%, Val Loss: 0.9993, Val Acc: 45.83%\n",
      "Fold 1 Epoch [60/200], Train Loss: 0.8050, Train Acc: 62.50%, Val Loss: 1.0087, Val Acc: 54.17%\n",
      "Fold 1 Epoch [61/200], Train Loss: 0.8146, Train Acc: 56.25%, Val Loss: 1.0730, Val Acc: 37.50%\n",
      "Fold 1 Epoch [62/200], Train Loss: 0.7508, Train Acc: 64.58%, Val Loss: 1.3097, Val Acc: 45.83%\n",
      "Fold 1 Epoch [63/200], Train Loss: 0.7396, Train Acc: 62.50%, Val Loss: 1.3053, Val Acc: 54.17%\n",
      "Fold 1 Epoch [64/200], Train Loss: 0.7346, Train Acc: 66.67%, Val Loss: 1.4994, Val Acc: 58.33%\n",
      "Fold 1 Epoch [65/200], Train Loss: 0.7195, Train Acc: 73.96%, Val Loss: 1.6307, Val Acc: 50.00%\n",
      "Fold 1 Epoch [66/200], Train Loss: 0.7961, Train Acc: 71.88%, Val Loss: 1.3361, Val Acc: 50.00%\n",
      "Fold 1 Epoch [67/200], Train Loss: 0.6442, Train Acc: 76.04%, Val Loss: 1.0917, Val Acc: 62.50%\n",
      "Fold 1 Epoch [68/200], Train Loss: 0.6383, Train Acc: 70.83%, Val Loss: 1.2319, Val Acc: 62.50%\n",
      "Fold 1 Epoch [69/200], Train Loss: 0.5958, Train Acc: 73.96%, Val Loss: 1.2565, Val Acc: 62.50%\n",
      "Fold 1 Epoch [70/200], Train Loss: 0.6864, Train Acc: 77.08%, Val Loss: 1.1340, Val Acc: 66.67%\n",
      "Fold 1 Epoch [71/200], Train Loss: 0.6752, Train Acc: 75.00%, Val Loss: 0.8054, Val Acc: 75.00%\n",
      "Fold 1 Epoch [72/200], Train Loss: 0.6900, Train Acc: 69.79%, Val Loss: 0.9841, Val Acc: 62.50%\n",
      "Fold 1 Epoch [73/200], Train Loss: 0.5909, Train Acc: 79.17%, Val Loss: 0.7593, Val Acc: 66.67%\n",
      "Fold 1 Epoch [74/200], Train Loss: 0.5865, Train Acc: 78.12%, Val Loss: 0.9165, Val Acc: 54.17%\n",
      "Fold 1 Epoch [75/200], Train Loss: 0.6300, Train Acc: 75.00%, Val Loss: 1.0514, Val Acc: 45.83%\n",
      "Fold 1 Epoch [76/200], Train Loss: 1.2560, Train Acc: 34.38%, Val Loss: 1.3529, Val Acc: 41.67%\n",
      "Fold 1 Epoch [77/200], Train Loss: 1.0883, Train Acc: 50.00%, Val Loss: 1.1881, Val Acc: 58.33%\n",
      "Fold 1 Epoch [78/200], Train Loss: 1.1856, Train Acc: 56.25%, Val Loss: 1.4715, Val Acc: 58.33%\n",
      "Fold 1 Epoch [79/200], Train Loss: 0.9343, Train Acc: 56.25%, Val Loss: 1.2718, Val Acc: 54.17%\n",
      "Fold 1 Epoch [80/200], Train Loss: 0.8166, Train Acc: 59.38%, Val Loss: 1.1725, Val Acc: 45.83%\n",
      "Fold 1 Epoch [81/200], Train Loss: 0.8587, Train Acc: 58.33%, Val Loss: 1.2564, Val Acc: 41.67%\n",
      "Fold 1 Epoch [82/200], Train Loss: 0.8688, Train Acc: 58.33%, Val Loss: 1.1892, Val Acc: 41.67%\n",
      "Fold 1 Epoch [83/200], Train Loss: 0.8381, Train Acc: 62.50%, Val Loss: 1.1448, Val Acc: 41.67%\n",
      "Fold 1 Epoch [84/200], Train Loss: 0.8291, Train Acc: 62.50%, Val Loss: 1.1710, Val Acc: 41.67%\n",
      "Fold 1 Epoch [85/200], Train Loss: 0.7968, Train Acc: 62.50%, Val Loss: 1.1713, Val Acc: 41.67%\n",
      "Fold 1 Epoch [86/200], Train Loss: 0.8214, Train Acc: 64.58%, Val Loss: 1.0563, Val Acc: 45.83%\n",
      "Fold 1 Epoch [87/200], Train Loss: 0.7609, Train Acc: 62.50%, Val Loss: 1.1105, Val Acc: 45.83%\n",
      "Fold 1 Epoch [88/200], Train Loss: 0.7606, Train Acc: 67.71%, Val Loss: 1.1185, Val Acc: 45.83%\n",
      "Fold 1 Epoch [89/200], Train Loss: 0.7210, Train Acc: 69.79%, Val Loss: 0.9824, Val Acc: 45.83%\n",
      "Fold 1 Epoch [90/200], Train Loss: 0.6822, Train Acc: 78.12%, Val Loss: 0.9198, Val Acc: 70.83%\n",
      "Fold 1 Epoch [91/200], Train Loss: 0.6842, Train Acc: 78.12%, Val Loss: 0.8078, Val Acc: 66.67%\n",
      "Fold 1 Epoch [92/200], Train Loss: 0.6264, Train Acc: 78.12%, Val Loss: 0.7676, Val Acc: 50.00%\n",
      "Fold 1 Epoch [93/200], Train Loss: 0.6968, Train Acc: 70.83%, Val Loss: 0.9300, Val Acc: 50.00%\n",
      "Fold 1 Epoch [94/200], Train Loss: 0.7664, Train Acc: 69.79%, Val Loss: 0.8973, Val Acc: 75.00%\n",
      "Fold 1 Epoch [95/200], Train Loss: 0.7215, Train Acc: 69.79%, Val Loss: 0.8774, Val Acc: 75.00%\n",
      "Fold 1 Epoch [96/200], Train Loss: 0.7459, Train Acc: 69.79%, Val Loss: 0.8414, Val Acc: 70.83%\n",
      "Fold 1 Epoch [97/200], Train Loss: 0.7348, Train Acc: 70.83%, Val Loss: 0.9419, Val Acc: 62.50%\n",
      "Fold 1 Epoch [98/200], Train Loss: 0.7634, Train Acc: 70.83%, Val Loss: 0.9999, Val Acc: 54.17%\n",
      "Fold 1 Epoch [99/200], Train Loss: 0.6529, Train Acc: 72.92%, Val Loss: 0.8975, Val Acc: 62.50%\n",
      "Fold 1 Epoch [100/200], Train Loss: 0.6409, Train Acc: 70.83%, Val Loss: 0.8430, Val Acc: 66.67%\n",
      "Fold 1 Epoch [101/200], Train Loss: 0.6088, Train Acc: 76.04%, Val Loss: 0.8170, Val Acc: 58.33%\n",
      "Fold 1 Epoch [102/200], Train Loss: 0.5857, Train Acc: 77.08%, Val Loss: 0.8290, Val Acc: 58.33%\n",
      "Fold 1 Epoch [103/200], Train Loss: 0.5390, Train Acc: 78.12%, Val Loss: 0.8737, Val Acc: 62.50%\n",
      "Fold 1 Epoch [104/200], Train Loss: 0.5740, Train Acc: 78.12%, Val Loss: 0.8234, Val Acc: 58.33%\n",
      "Fold 1 Epoch [105/200], Train Loss: 0.5160, Train Acc: 76.04%, Val Loss: 0.8205, Val Acc: 54.17%\n",
      "Fold 1 Epoch [106/200], Train Loss: 0.5053, Train Acc: 78.12%, Val Loss: 0.7941, Val Acc: 58.33%\n",
      "Fold 1 Epoch [107/200], Train Loss: 0.5504, Train Acc: 79.17%, Val Loss: 0.8269, Val Acc: 58.33%\n",
      "Fold 1 Epoch [108/200], Train Loss: 0.5642, Train Acc: 80.21%, Val Loss: 0.8636, Val Acc: 58.33%\n",
      "Fold 1 Epoch [109/200], Train Loss: 0.5292, Train Acc: 78.12%, Val Loss: 0.9470, Val Acc: 58.33%\n",
      "Fold 1 Epoch [110/200], Train Loss: 0.5188, Train Acc: 77.08%, Val Loss: 0.9488, Val Acc: 58.33%\n",
      "Fold 1 Epoch [111/200], Train Loss: 0.4799, Train Acc: 81.25%, Val Loss: 0.8338, Val Acc: 62.50%\n",
      "Fold 1 Epoch [112/200], Train Loss: 0.5041, Train Acc: 82.29%, Val Loss: 0.7855, Val Acc: 66.67%\n",
      "Fold 1 Epoch [113/200], Train Loss: 0.5047, Train Acc: 81.25%, Val Loss: 0.8339, Val Acc: 62.50%\n",
      "Fold 1 Epoch [114/200], Train Loss: 0.5353, Train Acc: 76.04%, Val Loss: 0.9715, Val Acc: 62.50%\n",
      "Fold 1 Epoch [115/200], Train Loss: 0.4901, Train Acc: 77.08%, Val Loss: 1.0698, Val Acc: 62.50%\n",
      "Fold 1 Epoch [116/200], Train Loss: 0.5003, Train Acc: 80.21%, Val Loss: 1.0619, Val Acc: 66.67%\n",
      "Fold 1 Epoch [117/200], Train Loss: 0.6752, Train Acc: 77.08%, Val Loss: 1.2032, Val Acc: 58.33%\n",
      "Fold 1 Epoch [118/200], Train Loss: 0.8347, Train Acc: 65.62%, Val Loss: 1.1598, Val Acc: 50.00%\n",
      "Fold 1 Epoch [119/200], Train Loss: 1.0773, Train Acc: 52.08%, Val Loss: 1.2761, Val Acc: 45.83%\n",
      "Fold 1 Epoch [120/200], Train Loss: 1.0303, Train Acc: 41.67%, Val Loss: 1.0682, Val Acc: 37.50%\n",
      "Fold 1 Epoch [121/200], Train Loss: 1.0326, Train Acc: 35.42%, Val Loss: 0.9862, Val Acc: 33.33%\n",
      "Fold 1 Epoch [122/200], Train Loss: 1.0808, Train Acc: 32.29%, Val Loss: 1.0160, Val Acc: 41.67%\n",
      "Fold 1 Epoch [123/200], Train Loss: 1.0044, Train Acc: 36.46%, Val Loss: 0.9988, Val Acc: 50.00%\n",
      "Fold 1 Epoch [124/200], Train Loss: 0.9894, Train Acc: 48.96%, Val Loss: 1.0446, Val Acc: 45.83%\n",
      "Fold 1 Epoch [125/200], Train Loss: 0.9654, Train Acc: 57.29%, Val Loss: 1.0516, Val Acc: 50.00%\n",
      "Fold 1 Epoch [126/200], Train Loss: 0.9320, Train Acc: 57.29%, Val Loss: 1.0413, Val Acc: 50.00%\n",
      "Fold 1 Epoch [127/200], Train Loss: 0.8988, Train Acc: 60.42%, Val Loss: 1.0009, Val Acc: 50.00%\n",
      "Fold 1 Epoch [128/200], Train Loss: 0.8863, Train Acc: 64.58%, Val Loss: 0.9939, Val Acc: 66.67%\n",
      "Fold 1 Epoch [129/200], Train Loss: 0.8484, Train Acc: 65.62%, Val Loss: 0.9904, Val Acc: 66.67%\n",
      "Fold 1 Epoch [130/200], Train Loss: 0.8610, Train Acc: 63.54%, Val Loss: 0.9820, Val Acc: 50.00%\n",
      "Fold 1 Epoch [131/200], Train Loss: 0.8265, Train Acc: 61.46%, Val Loss: 0.9843, Val Acc: 50.00%\n",
      "Fold 1 Epoch [132/200], Train Loss: 0.8052, Train Acc: 64.58%, Val Loss: 1.0003, Val Acc: 58.33%\n",
      "Fold 1 Epoch [133/200], Train Loss: 0.7515, Train Acc: 66.67%, Val Loss: 0.9811, Val Acc: 66.67%\n",
      "Fold 1 Epoch [134/200], Train Loss: 0.8020, Train Acc: 71.88%, Val Loss: 1.0330, Val Acc: 50.00%\n",
      "Fold 1 Epoch [135/200], Train Loss: 0.6974, Train Acc: 69.79%, Val Loss: 1.0439, Val Acc: 58.33%\n",
      "Fold 1 Epoch [136/200], Train Loss: 0.6796, Train Acc: 72.92%, Val Loss: 1.0558, Val Acc: 62.50%\n",
      "Fold 1 Epoch [137/200], Train Loss: 0.6896, Train Acc: 69.79%, Val Loss: 1.1255, Val Acc: 62.50%\n",
      "Fold 1 Epoch [138/200], Train Loss: 0.7118, Train Acc: 72.92%, Val Loss: 0.9942, Val Acc: 70.83%\n",
      "Fold 1 Epoch [139/200], Train Loss: 0.6680, Train Acc: 72.92%, Val Loss: 0.9285, Val Acc: 66.67%\n",
      "Fold 1 Epoch [140/200], Train Loss: 0.6678, Train Acc: 71.88%, Val Loss: 0.9418, Val Acc: 66.67%\n",
      "Fold 1 Epoch [141/200], Train Loss: 0.6723, Train Acc: 69.79%, Val Loss: 0.9855, Val Acc: 58.33%\n",
      "Fold 1 Epoch [142/200], Train Loss: 0.6411, Train Acc: 73.96%, Val Loss: 0.9977, Val Acc: 66.67%\n",
      "Fold 1 Epoch [143/200], Train Loss: 0.6180, Train Acc: 76.04%, Val Loss: 1.0410, Val Acc: 58.33%\n",
      "Fold 1 Epoch [144/200], Train Loss: 0.5967, Train Acc: 79.17%, Val Loss: 0.9521, Val Acc: 66.67%\n",
      "Fold 1 Epoch [145/200], Train Loss: 0.5529, Train Acc: 77.08%, Val Loss: 0.9643, Val Acc: 66.67%\n",
      "Fold 1 Epoch [146/200], Train Loss: 0.5946, Train Acc: 76.04%, Val Loss: 0.9936, Val Acc: 66.67%\n",
      "Fold 1 Epoch [147/200], Train Loss: 0.5591, Train Acc: 80.21%, Val Loss: 0.9885, Val Acc: 62.50%\n",
      "Fold 1 Epoch [148/200], Train Loss: 0.5497, Train Acc: 77.08%, Val Loss: 1.0342, Val Acc: 62.50%\n",
      "Fold 1 Epoch [149/200], Train Loss: 0.6559, Train Acc: 75.00%, Val Loss: 1.2028, Val Acc: 58.33%\n",
      "Fold 1 Epoch [150/200], Train Loss: 0.5458, Train Acc: 79.17%, Val Loss: 0.9205, Val Acc: 66.67%\n",
      "Fold 1 Epoch [151/200], Train Loss: 0.4893, Train Acc: 81.25%, Val Loss: 0.9275, Val Acc: 62.50%\n",
      "Fold 1 Epoch [152/200], Train Loss: 0.4859, Train Acc: 81.25%, Val Loss: 0.9604, Val Acc: 66.67%\n",
      "Fold 1 Epoch [153/200], Train Loss: 0.4636, Train Acc: 79.17%, Val Loss: 0.9386, Val Acc: 62.50%\n",
      "Fold 1 Epoch [154/200], Train Loss: 0.4426, Train Acc: 84.38%, Val Loss: 0.9094, Val Acc: 62.50%\n",
      "Fold 1 Epoch [155/200], Train Loss: 0.4388, Train Acc: 84.38%, Val Loss: 0.8442, Val Acc: 66.67%\n",
      "Fold 1 Epoch [156/200], Train Loss: 0.4504, Train Acc: 81.25%, Val Loss: 0.7547, Val Acc: 62.50%\n",
      "Fold 1 Epoch [157/200], Train Loss: 0.4084, Train Acc: 84.38%, Val Loss: 0.7186, Val Acc: 66.67%\n",
      "Fold 1 Epoch [158/200], Train Loss: 0.5051, Train Acc: 78.12%, Val Loss: 1.0421, Val Acc: 66.67%\n",
      "Fold 1 Epoch [159/200], Train Loss: 0.4481, Train Acc: 81.25%, Val Loss: 0.9260, Val Acc: 62.50%\n",
      "Fold 1 Epoch [160/200], Train Loss: 0.4762, Train Acc: 78.12%, Val Loss: 0.9241, Val Acc: 62.50%\n",
      "Fold 1 Epoch [161/200], Train Loss: 0.4082, Train Acc: 82.29%, Val Loss: 0.9382, Val Acc: 70.83%\n",
      "Fold 1 Epoch [162/200], Train Loss: 0.3769, Train Acc: 85.42%, Val Loss: 0.7369, Val Acc: 66.67%\n",
      "Fold 1 Epoch [163/200], Train Loss: 0.4580, Train Acc: 82.29%, Val Loss: 0.9156, Val Acc: 66.67%\n",
      "Fold 1 Epoch [164/200], Train Loss: 0.4394, Train Acc: 83.33%, Val Loss: 0.9657, Val Acc: 70.83%\n",
      "Fold 1 Epoch [165/200], Train Loss: 0.4315, Train Acc: 84.38%, Val Loss: 0.8759, Val Acc: 70.83%\n",
      "Fold 1 Epoch [166/200], Train Loss: 0.4146, Train Acc: 83.33%, Val Loss: 0.7495, Val Acc: 70.83%\n",
      "Fold 1 Epoch [167/200], Train Loss: 0.5031, Train Acc: 81.25%, Val Loss: 0.8812, Val Acc: 70.83%\n",
      "Fold 1 Epoch [168/200], Train Loss: 0.5321, Train Acc: 80.21%, Val Loss: 0.9211, Val Acc: 70.83%\n",
      "Fold 1 Epoch [169/200], Train Loss: 0.5587, Train Acc: 75.00%, Val Loss: 1.0062, Val Acc: 62.50%\n",
      "Fold 1 Epoch [170/200], Train Loss: 0.4322, Train Acc: 86.46%, Val Loss: 0.8877, Val Acc: 62.50%\n",
      "Fold 1 Epoch [171/200], Train Loss: 0.4448, Train Acc: 85.42%, Val Loss: 0.9542, Val Acc: 70.83%\n",
      "Fold 1 Epoch [172/200], Train Loss: 0.4226, Train Acc: 84.38%, Val Loss: 0.9931, Val Acc: 70.83%\n",
      "Fold 1 Epoch [173/200], Train Loss: 0.4014, Train Acc: 82.29%, Val Loss: 1.0642, Val Acc: 62.50%\n",
      "Fold 1 Epoch [174/200], Train Loss: 0.3550, Train Acc: 88.54%, Val Loss: 1.0642, Val Acc: 66.67%\n",
      "Fold 1 Epoch [175/200], Train Loss: 0.3675, Train Acc: 89.58%, Val Loss: 0.9663, Val Acc: 66.67%\n",
      "Fold 1 Epoch [176/200], Train Loss: 0.3450, Train Acc: 86.46%, Val Loss: 0.9673, Val Acc: 66.67%\n",
      "Fold 1 Epoch [177/200], Train Loss: 0.3797, Train Acc: 85.42%, Val Loss: 1.1635, Val Acc: 70.83%\n",
      "Fold 1 Epoch [178/200], Train Loss: 0.4082, Train Acc: 85.42%, Val Loss: 1.2655, Val Acc: 62.50%\n",
      "Fold 1 Epoch [179/200], Train Loss: 0.3522, Train Acc: 88.54%, Val Loss: 0.9212, Val Acc: 70.83%\n",
      "Fold 1 Epoch [180/200], Train Loss: 0.3508, Train Acc: 90.62%, Val Loss: 0.8166, Val Acc: 75.00%\n",
      "Fold 1 Epoch [181/200], Train Loss: 0.3410, Train Acc: 91.67%, Val Loss: 0.7512, Val Acc: 75.00%\n",
      "Fold 1 Epoch [182/200], Train Loss: 0.3141, Train Acc: 93.75%, Val Loss: 0.7151, Val Acc: 70.83%\n",
      "Fold 1 Epoch [183/200], Train Loss: 0.5543, Train Acc: 83.33%, Val Loss: 0.9819, Val Acc: 70.83%\n",
      "Fold 1 Epoch [184/200], Train Loss: 0.5932, Train Acc: 77.08%, Val Loss: 1.1060, Val Acc: 70.83%\n",
      "Fold 1 Epoch [185/200], Train Loss: 0.3504, Train Acc: 85.42%, Val Loss: 1.0431, Val Acc: 75.00%\n",
      "Fold 1 Epoch [186/200], Train Loss: 0.4400, Train Acc: 88.54%, Val Loss: 0.9940, Val Acc: 66.67%\n",
      "Fold 1 Epoch [187/200], Train Loss: 0.3915, Train Acc: 85.42%, Val Loss: 0.9387, Val Acc: 70.83%\n",
      "Fold 1 Epoch [188/200], Train Loss: 0.4528, Train Acc: 81.25%, Val Loss: 1.0111, Val Acc: 70.83%\n",
      "Fold 1 Epoch [189/200], Train Loss: 0.4180, Train Acc: 82.29%, Val Loss: 1.2028, Val Acc: 66.67%\n",
      "Fold 1 Epoch [190/200], Train Loss: 0.5310, Train Acc: 82.29%, Val Loss: 1.0232, Val Acc: 75.00%\n",
      "Fold 1 Epoch [191/200], Train Loss: 0.4952, Train Acc: 87.50%, Val Loss: 1.0500, Val Acc: 66.67%\n",
      "Fold 1 Epoch [192/200], Train Loss: 0.6548, Train Acc: 84.38%, Val Loss: 1.3612, Val Acc: 62.50%\n",
      "Fold 1 Epoch [193/200], Train Loss: 0.5009, Train Acc: 78.12%, Val Loss: 1.0038, Val Acc: 62.50%\n",
      "Fold 1 Epoch [194/200], Train Loss: 0.4624, Train Acc: 80.21%, Val Loss: 0.9494, Val Acc: 75.00%\n",
      "Fold 1 Epoch [195/200], Train Loss: 0.4720, Train Acc: 79.17%, Val Loss: 0.8819, Val Acc: 70.83%\n",
      "Fold 1 Epoch [196/200], Train Loss: 0.4358, Train Acc: 80.21%, Val Loss: 0.8029, Val Acc: 70.83%\n",
      "Fold 1 Epoch [197/200], Train Loss: 0.4081, Train Acc: 83.33%, Val Loss: 0.7671, Val Acc: 66.67%\n",
      "Fold 1 Epoch [198/200], Train Loss: 0.4121, Train Acc: 89.58%, Val Loss: 0.8159, Val Acc: 66.67%\n",
      "Fold 1 Epoch [199/200], Train Loss: 0.3494, Train Acc: 88.54%, Val Loss: 0.8586, Val Acc: 66.67%\n",
      "Fold 1 Epoch [200/200], Train Loss: 0.3787, Train Acc: 86.46%, Val Loss: 0.8069, Val Acc: 66.67%\n",
      "Fold 1 Best Accuracy: 75.00%\n",
      "Fold 2/5\n",
      "Fold 2 Epoch [1/200], Train Loss: 0.8774, Train Acc: 51.04%, Val Loss: 0.8884, Val Acc: 62.50%\n",
      "Fold 2 Epoch [2/200], Train Loss: 0.8983, Train Acc: 59.38%, Val Loss: 1.1891, Val Acc: 58.33%\n",
      "Fold 2 Epoch [3/200], Train Loss: 1.0345, Train Acc: 52.08%, Val Loss: 1.1012, Val Acc: 50.00%\n",
      "Fold 2 Epoch [4/200], Train Loss: 0.9535, Train Acc: 35.42%, Val Loss: 1.0075, Val Acc: 25.00%\n",
      "Fold 2 Epoch [5/200], Train Loss: 0.8654, Train Acc: 57.29%, Val Loss: 0.8117, Val Acc: 62.50%\n",
      "Fold 2 Epoch [6/200], Train Loss: 0.9277, Train Acc: 54.17%, Val Loss: 0.9149, Val Acc: 62.50%\n",
      "Fold 2 Epoch [7/200], Train Loss: 0.9179, Train Acc: 53.12%, Val Loss: 0.8451, Val Acc: 62.50%\n",
      "Fold 2 Epoch [8/200], Train Loss: 0.9254, Train Acc: 48.96%, Val Loss: 0.9503, Val Acc: 50.00%\n",
      "Fold 2 Epoch [9/200], Train Loss: 0.9884, Train Acc: 35.42%, Val Loss: 0.9975, Val Acc: 29.17%\n",
      "Fold 2 Epoch [10/200], Train Loss: 0.8716, Train Acc: 56.25%, Val Loss: 0.9353, Val Acc: 33.33%\n",
      "Fold 2 Epoch [11/200], Train Loss: 1.9975, Train Acc: 32.29%, Val Loss: 2.2339, Val Acc: 37.50%\n",
      "Fold 2 Epoch [12/200], Train Loss: 1.3625, Train Acc: 32.29%, Val Loss: 1.4396, Val Acc: 37.50%\n",
      "Fold 2 Epoch [13/200], Train Loss: 1.0948, Train Acc: 35.42%, Val Loss: 1.1296, Val Acc: 25.00%\n",
      "Fold 2 Epoch [14/200], Train Loss: 1.1317, Train Acc: 32.29%, Val Loss: 1.1374, Val Acc: 37.50%\n",
      "Fold 2 Epoch [15/200], Train Loss: 1.1435, Train Acc: 32.29%, Val Loss: 1.1174, Val Acc: 37.50%\n",
      "Fold 2 Epoch [16/200], Train Loss: 1.1114, Train Acc: 32.29%, Val Loss: 1.0918, Val Acc: 37.50%\n",
      "Fold 2 Epoch [17/200], Train Loss: 1.0944, Train Acc: 41.67%, Val Loss: 1.0992, Val Acc: 29.17%\n",
      "Fold 2 Epoch [18/200], Train Loss: 1.0918, Train Acc: 36.46%, Val Loss: 1.1068, Val Acc: 25.00%\n",
      "Fold 2 Epoch [19/200], Train Loss: 1.0887, Train Acc: 45.83%, Val Loss: 1.0914, Val Acc: 33.33%\n",
      "Fold 2 Epoch [20/200], Train Loss: 1.0835, Train Acc: 35.42%, Val Loss: 1.0946, Val Acc: 25.00%\n",
      "Fold 2 Epoch [21/200], Train Loss: 1.0870, Train Acc: 45.83%, Val Loss: 1.0923, Val Acc: 33.33%\n",
      "Fold 2 Epoch [22/200], Train Loss: 1.0787, Train Acc: 37.50%, Val Loss: 1.0896, Val Acc: 33.33%\n",
      "Fold 2 Epoch [23/200], Train Loss: 1.0734, Train Acc: 37.50%, Val Loss: 1.0876, Val Acc: 33.33%\n",
      "Fold 2 Epoch [24/200], Train Loss: 1.0434, Train Acc: 51.04%, Val Loss: 1.0442, Val Acc: 50.00%\n",
      "Fold 2 Epoch [25/200], Train Loss: 1.0931, Train Acc: 38.54%, Val Loss: 1.0589, Val Acc: 37.50%\n",
      "Fold 2 Epoch [26/200], Train Loss: 1.0481, Train Acc: 53.12%, Val Loss: 1.0547, Val Acc: 41.67%\n",
      "Fold 2 Epoch [27/200], Train Loss: 0.9918, Train Acc: 60.42%, Val Loss: 0.9796, Val Acc: 58.33%\n",
      "Fold 2 Epoch [28/200], Train Loss: 1.0177, Train Acc: 46.88%, Val Loss: 0.9940, Val Acc: 50.00%\n",
      "Fold 2 Epoch [29/200], Train Loss: 0.9088, Train Acc: 55.21%, Val Loss: 0.9658, Val Acc: 50.00%\n",
      "Fold 2 Epoch [30/200], Train Loss: 0.9759, Train Acc: 47.92%, Val Loss: 0.9832, Val Acc: 45.83%\n",
      "Fold 2 Epoch [31/200], Train Loss: 0.8433, Train Acc: 63.54%, Val Loss: 0.8392, Val Acc: 66.67%\n",
      "Fold 2 Epoch [32/200], Train Loss: 1.1563, Train Acc: 42.71%, Val Loss: 1.0539, Val Acc: 50.00%\n",
      "Fold 2 Epoch [33/200], Train Loss: 1.0566, Train Acc: 34.38%, Val Loss: 1.0359, Val Acc: 37.50%\n",
      "Fold 2 Epoch [34/200], Train Loss: 0.9163, Train Acc: 60.42%, Val Loss: 0.9132, Val Acc: 66.67%\n",
      "Fold 2 Epoch [35/200], Train Loss: 0.8679, Train Acc: 62.50%, Val Loss: 0.9241, Val Acc: 50.00%\n",
      "Fold 2 Epoch [36/200], Train Loss: 0.8578, Train Acc: 52.08%, Val Loss: 0.9858, Val Acc: 41.67%\n",
      "Fold 2 Epoch [37/200], Train Loss: 0.8597, Train Acc: 56.25%, Val Loss: 0.8830, Val Acc: 50.00%\n",
      "Fold 2 Epoch [38/200], Train Loss: 0.8569, Train Acc: 63.54%, Val Loss: 0.8167, Val Acc: 62.50%\n",
      "Fold 2 Epoch [39/200], Train Loss: 0.7920, Train Acc: 54.17%, Val Loss: 0.8983, Val Acc: 45.83%\n",
      "Fold 2 Epoch [40/200], Train Loss: 0.7829, Train Acc: 70.83%, Val Loss: 0.7386, Val Acc: 58.33%\n",
      "Fold 2 Epoch [41/200], Train Loss: 1.3941, Train Acc: 41.67%, Val Loss: 1.2093, Val Acc: 54.17%\n",
      "Fold 2 Epoch [42/200], Train Loss: 1.0156, Train Acc: 59.38%, Val Loss: 1.0081, Val Acc: 66.67%\n",
      "Fold 2 Epoch [43/200], Train Loss: 1.0885, Train Acc: 55.21%, Val Loss: 1.1848, Val Acc: 50.00%\n",
      "Fold 2 Epoch [44/200], Train Loss: 1.0441, Train Acc: 47.92%, Val Loss: 1.0678, Val Acc: 41.67%\n",
      "Fold 2 Epoch [45/200], Train Loss: 1.0173, Train Acc: 48.96%, Val Loss: 1.0522, Val Acc: 37.50%\n",
      "Fold 2 Epoch [46/200], Train Loss: 1.0154, Train Acc: 39.58%, Val Loss: 1.0619, Val Acc: 41.67%\n",
      "Fold 2 Epoch [47/200], Train Loss: 1.0030, Train Acc: 41.67%, Val Loss: 1.0203, Val Acc: 45.83%\n",
      "Fold 2 Epoch [48/200], Train Loss: 0.9711, Train Acc: 44.79%, Val Loss: 0.9687, Val Acc: 45.83%\n",
      "Fold 2 Epoch [49/200], Train Loss: 1.0384, Train Acc: 52.08%, Val Loss: 0.9604, Val Acc: 62.50%\n",
      "Fold 2 Epoch [50/200], Train Loss: 1.0230, Train Acc: 39.58%, Val Loss: 0.9654, Val Acc: 45.83%\n",
      "Fold 2 Epoch [51/200], Train Loss: 0.9717, Train Acc: 55.21%, Val Loss: 1.0798, Val Acc: 58.33%\n",
      "Fold 2 Epoch [52/200], Train Loss: 1.0569, Train Acc: 47.92%, Val Loss: 0.9999, Val Acc: 45.83%\n",
      "Fold 2 Epoch [53/200], Train Loss: 1.0282, Train Acc: 34.38%, Val Loss: 0.9963, Val Acc: 37.50%\n",
      "Fold 2 Epoch [54/200], Train Loss: 0.9442, Train Acc: 54.17%, Val Loss: 0.9159, Val Acc: 62.50%\n",
      "Fold 2 Epoch [55/200], Train Loss: 1.0211, Train Acc: 58.33%, Val Loss: 1.1773, Val Acc: 45.83%\n",
      "Fold 2 Epoch [56/200], Train Loss: 1.3060, Train Acc: 47.92%, Val Loss: 1.0357, Val Acc: 54.17%\n",
      "Fold 2 Epoch [57/200], Train Loss: 1.1083, Train Acc: 34.38%, Val Loss: 1.0737, Val Acc: 29.17%\n",
      "Fold 2 Epoch [58/200], Train Loss: 1.0412, Train Acc: 47.92%, Val Loss: 0.9939, Val Acc: 54.17%\n",
      "Fold 2 Epoch [59/200], Train Loss: 1.0961, Train Acc: 31.25%, Val Loss: 1.0287, Val Acc: 41.67%\n",
      "Fold 2 Epoch [60/200], Train Loss: 0.9853, Train Acc: 42.71%, Val Loss: 0.9423, Val Acc: 33.33%\n",
      "Fold 2 Epoch [61/200], Train Loss: 0.9054, Train Acc: 58.33%, Val Loss: 0.8753, Val Acc: 66.67%\n",
      "Fold 2 Epoch [62/200], Train Loss: 0.7537, Train Acc: 67.71%, Val Loss: 0.7734, Val Acc: 70.83%\n",
      "Fold 2 Epoch [63/200], Train Loss: 0.8019, Train Acc: 68.75%, Val Loss: 0.8171, Val Acc: 70.83%\n",
      "Fold 2 Epoch [64/200], Train Loss: 0.9022, Train Acc: 60.42%, Val Loss: 0.9727, Val Acc: 54.17%\n",
      "Fold 2 Epoch [65/200], Train Loss: 0.8215, Train Acc: 65.62%, Val Loss: 0.8691, Val Acc: 58.33%\n",
      "Fold 2 Epoch [66/200], Train Loss: 0.8655, Train Acc: 59.38%, Val Loss: 0.7249, Val Acc: 75.00%\n",
      "Fold 2 Epoch [67/200], Train Loss: 0.7428, Train Acc: 63.54%, Val Loss: 0.7126, Val Acc: 66.67%\n",
      "Fold 2 Epoch [68/200], Train Loss: 0.7191, Train Acc: 67.71%, Val Loss: 0.7651, Val Acc: 62.50%\n",
      "Fold 2 Epoch [69/200], Train Loss: 1.1681, Train Acc: 58.33%, Val Loss: 1.3110, Val Acc: 58.33%\n",
      "Fold 2 Epoch [70/200], Train Loss: 0.8788, Train Acc: 53.12%, Val Loss: 0.9210, Val Acc: 45.83%\n",
      "Fold 2 Epoch [71/200], Train Loss: 1.2588, Train Acc: 31.25%, Val Loss: 1.2139, Val Acc: 37.50%\n",
      "Fold 2 Epoch [72/200], Train Loss: 1.0818, Train Acc: 34.38%, Val Loss: 1.0592, Val Acc: 41.67%\n",
      "Fold 2 Epoch [73/200], Train Loss: 1.0082, Train Acc: 47.92%, Val Loss: 1.0131, Val Acc: 41.67%\n",
      "Fold 2 Epoch [74/200], Train Loss: 1.0845, Train Acc: 42.71%, Val Loss: 1.1066, Val Acc: 37.50%\n",
      "Fold 2 Epoch [75/200], Train Loss: 1.1536, Train Acc: 27.08%, Val Loss: 1.2012, Val Acc: 20.83%\n",
      "Fold 2 Epoch [76/200], Train Loss: 1.0888, Train Acc: 36.46%, Val Loss: 1.0564, Val Acc: 54.17%\n",
      "Fold 2 Epoch [77/200], Train Loss: 1.0814, Train Acc: 34.38%, Val Loss: 1.0686, Val Acc: 37.50%\n",
      "Fold 2 Epoch [78/200], Train Loss: 1.0974, Train Acc: 35.42%, Val Loss: 1.0913, Val Acc: 33.33%\n",
      "Fold 2 Epoch [79/200], Train Loss: 1.0616, Train Acc: 42.71%, Val Loss: 1.0899, Val Acc: 37.50%\n",
      "Fold 2 Epoch [80/200], Train Loss: 1.0438, Train Acc: 36.46%, Val Loss: 1.1030, Val Acc: 25.00%\n",
      "Fold 2 Epoch [81/200], Train Loss: 1.0695, Train Acc: 36.46%, Val Loss: 1.1676, Val Acc: 25.00%\n",
      "Fold 2 Epoch [82/200], Train Loss: 1.0507, Train Acc: 42.71%, Val Loss: 1.0687, Val Acc: 29.17%\n",
      "Fold 2 Epoch [83/200], Train Loss: 1.0241, Train Acc: 46.88%, Val Loss: 1.0655, Val Acc: 37.50%\n",
      "Fold 2 Epoch [84/200], Train Loss: 1.0034, Train Acc: 60.42%, Val Loss: 0.9928, Val Acc: 66.67%\n",
      "Fold 2 Epoch [85/200], Train Loss: 0.9944, Train Acc: 47.92%, Val Loss: 0.9723, Val Acc: 54.17%\n",
      "Fold 2 Epoch [86/200], Train Loss: 0.9782, Train Acc: 57.29%, Val Loss: 0.9661, Val Acc: 58.33%\n",
      "Fold 2 Epoch [87/200], Train Loss: 0.9449, Train Acc: 51.04%, Val Loss: 0.9864, Val Acc: 45.83%\n",
      "Fold 2 Epoch [88/200], Train Loss: 0.8899, Train Acc: 52.08%, Val Loss: 0.9469, Val Acc: 50.00%\n",
      "Fold 2 Epoch [89/200], Train Loss: 0.8919, Train Acc: 68.75%, Val Loss: 0.8632, Val Acc: 70.83%\n",
      "Fold 2 Epoch [90/200], Train Loss: 0.7867, Train Acc: 61.46%, Val Loss: 0.7906, Val Acc: 66.67%\n",
      "Fold 2 Epoch [91/200], Train Loss: 1.0411, Train Acc: 52.08%, Val Loss: 1.0892, Val Acc: 41.67%\n",
      "Fold 2 Epoch [92/200], Train Loss: 1.0938, Train Acc: 33.33%, Val Loss: 1.1361, Val Acc: 33.33%\n",
      "Fold 2 Epoch [93/200], Train Loss: 1.1499, Train Acc: 29.17%, Val Loss: 1.1580, Val Acc: 37.50%\n",
      "Fold 2 Epoch [94/200], Train Loss: 1.1381, Train Acc: 29.17%, Val Loss: 1.1912, Val Acc: 25.00%\n",
      "Fold 2 Epoch [95/200], Train Loss: 1.1133, Train Acc: 29.17%, Val Loss: 1.1561, Val Acc: 20.83%\n",
      "Fold 2 Epoch [96/200], Train Loss: 1.0568, Train Acc: 50.00%, Val Loss: 1.0762, Val Acc: 37.50%\n",
      "Fold 2 Epoch [97/200], Train Loss: 1.0137, Train Acc: 51.04%, Val Loss: 1.0619, Val Acc: 37.50%\n",
      "Fold 2 Epoch [98/200], Train Loss: 0.9877, Train Acc: 56.25%, Val Loss: 0.9784, Val Acc: 58.33%\n",
      "Fold 2 Epoch [99/200], Train Loss: 0.9908, Train Acc: 51.04%, Val Loss: 0.9945, Val Acc: 54.17%\n",
      "Fold 2 Epoch [100/200], Train Loss: 0.9866, Train Acc: 52.08%, Val Loss: 1.0741, Val Acc: 50.00%\n",
      "Fold 2 Epoch [101/200], Train Loss: 0.9829, Train Acc: 51.04%, Val Loss: 1.1140, Val Acc: 33.33%\n",
      "Fold 2 Epoch [102/200], Train Loss: 0.9521, Train Acc: 55.21%, Val Loss: 1.2811, Val Acc: 33.33%\n",
      "Fold 2 Epoch [103/200], Train Loss: 0.9770, Train Acc: 51.04%, Val Loss: 1.3167, Val Acc: 45.83%\n",
      "Fold 2 Epoch [104/200], Train Loss: 0.9765, Train Acc: 56.25%, Val Loss: 1.2667, Val Acc: 41.67%\n",
      "Fold 2 Epoch [105/200], Train Loss: 0.9074, Train Acc: 60.42%, Val Loss: 1.3071, Val Acc: 45.83%\n",
      "Fold 2 Epoch [106/200], Train Loss: 0.9084, Train Acc: 57.29%, Val Loss: 1.3027, Val Acc: 45.83%\n",
      "Fold 2 Epoch [107/200], Train Loss: 0.9108, Train Acc: 60.42%, Val Loss: 1.3641, Val Acc: 41.67%\n",
      "Fold 2 Epoch [108/200], Train Loss: 0.8375, Train Acc: 59.38%, Val Loss: 1.3328, Val Acc: 33.33%\n",
      "Fold 2 Epoch [109/200], Train Loss: 0.8318, Train Acc: 56.25%, Val Loss: 1.3324, Val Acc: 29.17%\n",
      "Fold 2 Epoch [110/200], Train Loss: 0.8618, Train Acc: 62.50%, Val Loss: 1.3355, Val Acc: 41.67%\n",
      "Fold 2 Epoch [111/200], Train Loss: 0.8414, Train Acc: 63.54%, Val Loss: 1.3070, Val Acc: 41.67%\n",
      "Fold 2 Epoch [112/200], Train Loss: 0.7912, Train Acc: 61.46%, Val Loss: 1.3433, Val Acc: 41.67%\n",
      "Fold 2 Epoch [113/200], Train Loss: 0.7538, Train Acc: 61.46%, Val Loss: 1.3751, Val Acc: 41.67%\n",
      "Fold 2 Epoch [114/200], Train Loss: 0.7719, Train Acc: 61.46%, Val Loss: 1.4054, Val Acc: 37.50%\n",
      "Fold 2 Epoch [115/200], Train Loss: 0.7758, Train Acc: 59.38%, Val Loss: 1.4005, Val Acc: 37.50%\n",
      "Fold 2 Epoch [116/200], Train Loss: 0.7812, Train Acc: 59.38%, Val Loss: 1.3446, Val Acc: 41.67%\n",
      "Fold 2 Epoch [117/200], Train Loss: 0.7676, Train Acc: 59.38%, Val Loss: 1.2946, Val Acc: 41.67%\n",
      "Fold 2 Epoch [118/200], Train Loss: 0.7610, Train Acc: 62.50%, Val Loss: 1.3027, Val Acc: 54.17%\n",
      "Fold 2 Epoch [119/200], Train Loss: 0.7538, Train Acc: 62.50%, Val Loss: 1.3327, Val Acc: 41.67%\n",
      "Fold 2 Epoch [120/200], Train Loss: 0.7279, Train Acc: 62.50%, Val Loss: 1.3022, Val Acc: 33.33%\n",
      "Fold 2 Epoch [121/200], Train Loss: 0.7304, Train Acc: 69.79%, Val Loss: 1.3108, Val Acc: 45.83%\n",
      "Fold 2 Epoch [122/200], Train Loss: 0.7286, Train Acc: 63.54%, Val Loss: 1.3412, Val Acc: 41.67%\n",
      "Fold 2 Epoch [123/200], Train Loss: 0.6566, Train Acc: 63.54%, Val Loss: 1.3414, Val Acc: 45.83%\n",
      "Fold 2 Epoch [124/200], Train Loss: 0.6917, Train Acc: 67.71%, Val Loss: 1.3587, Val Acc: 41.67%\n",
      "Fold 2 Epoch [125/200], Train Loss: 0.7093, Train Acc: 66.67%, Val Loss: 1.2654, Val Acc: 50.00%\n",
      "Fold 2 Epoch [126/200], Train Loss: 0.6690, Train Acc: 67.71%, Val Loss: 1.2097, Val Acc: 54.17%\n",
      "Fold 2 Epoch [127/200], Train Loss: 0.6740, Train Acc: 67.71%, Val Loss: 1.2061, Val Acc: 54.17%\n",
      "Fold 2 Epoch [128/200], Train Loss: 0.6932, Train Acc: 65.62%, Val Loss: 1.2124, Val Acc: 54.17%\n",
      "Fold 2 Epoch [129/200], Train Loss: 0.7083, Train Acc: 61.46%, Val Loss: 1.2639, Val Acc: 54.17%\n",
      "Fold 2 Epoch [130/200], Train Loss: 0.6657, Train Acc: 70.83%, Val Loss: 1.2668, Val Acc: 54.17%\n",
      "Fold 2 Epoch [131/200], Train Loss: 0.6162, Train Acc: 69.79%, Val Loss: 1.1940, Val Acc: 54.17%\n",
      "Fold 2 Epoch [132/200], Train Loss: 0.7015, Train Acc: 62.50%, Val Loss: 1.2785, Val Acc: 45.83%\n",
      "Fold 2 Epoch [133/200], Train Loss: 0.6230, Train Acc: 71.88%, Val Loss: 1.1721, Val Acc: 50.00%\n",
      "Fold 2 Epoch [134/200], Train Loss: 0.5843, Train Acc: 71.88%, Val Loss: 1.1242, Val Acc: 62.50%\n",
      "Fold 2 Epoch [135/200], Train Loss: 0.6079, Train Acc: 69.79%, Val Loss: 1.1827, Val Acc: 66.67%\n",
      "Fold 2 Epoch [136/200], Train Loss: 0.6642, Train Acc: 68.75%, Val Loss: 1.3748, Val Acc: 54.17%\n",
      "Fold 2 Epoch [137/200], Train Loss: 0.5954, Train Acc: 65.62%, Val Loss: 1.3006, Val Acc: 50.00%\n",
      "Fold 2 Epoch [138/200], Train Loss: 0.5992, Train Acc: 76.04%, Val Loss: 1.2905, Val Acc: 45.83%\n",
      "Fold 2 Epoch [139/200], Train Loss: 0.5675, Train Acc: 77.08%, Val Loss: 1.2575, Val Acc: 50.00%\n",
      "Fold 2 Epoch [140/200], Train Loss: 0.6193, Train Acc: 69.79%, Val Loss: 1.2901, Val Acc: 58.33%\n",
      "Fold 2 Epoch [141/200], Train Loss: 0.6833, Train Acc: 67.71%, Val Loss: 1.3214, Val Acc: 50.00%\n",
      "Fold 2 Epoch [142/200], Train Loss: 0.5961, Train Acc: 68.75%, Val Loss: 1.3120, Val Acc: 45.83%\n",
      "Fold 2 Epoch [143/200], Train Loss: 0.5934, Train Acc: 70.83%, Val Loss: 1.2475, Val Acc: 54.17%\n",
      "Fold 2 Epoch [144/200], Train Loss: 0.5981, Train Acc: 67.71%, Val Loss: 1.2045, Val Acc: 50.00%\n",
      "Fold 2 Epoch [145/200], Train Loss: 0.6035, Train Acc: 69.79%, Val Loss: 1.1074, Val Acc: 58.33%\n",
      "Fold 2 Epoch [146/200], Train Loss: 0.5456, Train Acc: 70.83%, Val Loss: 1.1332, Val Acc: 58.33%\n",
      "Fold 2 Epoch [147/200], Train Loss: 0.7061, Train Acc: 66.67%, Val Loss: 1.3888, Val Acc: 58.33%\n",
      "Fold 2 Epoch [148/200], Train Loss: 0.6196, Train Acc: 67.71%, Val Loss: 1.2969, Val Acc: 45.83%\n",
      "Fold 2 Epoch [149/200], Train Loss: 0.6347, Train Acc: 68.75%, Val Loss: 1.3099, Val Acc: 50.00%\n",
      "Fold 2 Epoch [150/200], Train Loss: 0.6045, Train Acc: 66.67%, Val Loss: 1.4420, Val Acc: 37.50%\n",
      "Fold 2 Epoch [151/200], Train Loss: 0.6095, Train Acc: 67.71%, Val Loss: 1.4145, Val Acc: 41.67%\n",
      "Fold 2 Epoch [152/200], Train Loss: 0.5631, Train Acc: 72.92%, Val Loss: 1.3588, Val Acc: 54.17%\n",
      "Fold 2 Epoch [153/200], Train Loss: 0.5630, Train Acc: 77.08%, Val Loss: 1.3441, Val Acc: 58.33%\n",
      "Fold 2 Epoch [154/200], Train Loss: 0.5884, Train Acc: 75.00%, Val Loss: 1.2958, Val Acc: 58.33%\n",
      "Fold 2 Epoch [155/200], Train Loss: 0.5706, Train Acc: 77.08%, Val Loss: 1.2634, Val Acc: 50.00%\n",
      "Fold 2 Epoch [156/200], Train Loss: 0.5234, Train Acc: 79.17%, Val Loss: 1.2409, Val Acc: 41.67%\n",
      "Fold 2 Epoch [157/200], Train Loss: 0.5480, Train Acc: 73.96%, Val Loss: 1.0670, Val Acc: 45.83%\n",
      "Fold 2 Epoch [158/200], Train Loss: 0.5046, Train Acc: 81.25%, Val Loss: 1.4124, Val Acc: 41.67%\n",
      "Fold 2 Epoch [159/200], Train Loss: 0.5989, Train Acc: 73.96%, Val Loss: 1.5118, Val Acc: 45.83%\n",
      "Fold 2 Epoch [160/200], Train Loss: 0.5931, Train Acc: 79.17%, Val Loss: 1.3938, Val Acc: 58.33%\n",
      "Fold 2 Epoch [161/200], Train Loss: 0.5464, Train Acc: 76.04%, Val Loss: 1.2199, Val Acc: 58.33%\n",
      "Fold 2 Epoch [162/200], Train Loss: 0.5702, Train Acc: 80.21%, Val Loss: 1.0266, Val Acc: 70.83%\n",
      "Fold 2 Epoch [163/200], Train Loss: 0.4810, Train Acc: 80.21%, Val Loss: 0.9787, Val Acc: 66.67%\n",
      "Fold 2 Epoch [164/200], Train Loss: 0.4513, Train Acc: 83.33%, Val Loss: 1.2306, Val Acc: 62.50%\n",
      "Fold 2 Epoch [165/200], Train Loss: 0.4203, Train Acc: 82.29%, Val Loss: 1.2587, Val Acc: 62.50%\n",
      "Fold 2 Epoch [166/200], Train Loss: 0.5234, Train Acc: 73.96%, Val Loss: 1.3784, Val Acc: 62.50%\n",
      "Fold 2 Epoch [167/200], Train Loss: 0.5147, Train Acc: 77.08%, Val Loss: 0.9445, Val Acc: 70.83%\n",
      "Fold 2 Epoch [168/200], Train Loss: 0.4575, Train Acc: 82.29%, Val Loss: 1.1508, Val Acc: 66.67%\n",
      "Fold 2 Epoch [169/200], Train Loss: 0.4811, Train Acc: 78.12%, Val Loss: 0.7152, Val Acc: 70.83%\n",
      "Fold 2 Epoch [170/200], Train Loss: 1.0585, Train Acc: 60.42%, Val Loss: 1.1524, Val Acc: 62.50%\n",
      "Fold 2 Epoch [171/200], Train Loss: 0.7297, Train Acc: 66.67%, Val Loss: 0.9643, Val Acc: 66.67%\n",
      "Fold 2 Epoch [172/200], Train Loss: 0.5807, Train Acc: 80.21%, Val Loss: 0.8477, Val Acc: 58.33%\n",
      "Fold 2 Epoch [173/200], Train Loss: 0.8102, Train Acc: 65.62%, Val Loss: 1.1306, Val Acc: 41.67%\n",
      "Fold 2 Epoch [174/200], Train Loss: 0.6194, Train Acc: 76.04%, Val Loss: 0.9420, Val Acc: 58.33%\n",
      "Fold 2 Epoch [175/200], Train Loss: 0.6782, Train Acc: 67.71%, Val Loss: 0.9288, Val Acc: 66.67%\n",
      "Fold 2 Epoch [176/200], Train Loss: 0.6437, Train Acc: 69.79%, Val Loss: 0.8093, Val Acc: 66.67%\n",
      "Fold 2 Epoch [177/200], Train Loss: 0.6513, Train Acc: 77.08%, Val Loss: 0.8097, Val Acc: 70.83%\n",
      "Fold 2 Epoch [178/200], Train Loss: 0.5626, Train Acc: 75.00%, Val Loss: 0.8830, Val Acc: 54.17%\n",
      "Fold 2 Epoch [179/200], Train Loss: 0.6511, Train Acc: 75.00%, Val Loss: 1.0511, Val Acc: 45.83%\n",
      "Fold 2 Epoch [180/200], Train Loss: 0.5630, Train Acc: 78.12%, Val Loss: 0.9438, Val Acc: 54.17%\n",
      "Fold 2 Epoch [181/200], Train Loss: 0.5284, Train Acc: 80.21%, Val Loss: 0.8529, Val Acc: 75.00%\n",
      "Fold 2 Epoch [182/200], Train Loss: 0.5393, Train Acc: 81.25%, Val Loss: 0.8055, Val Acc: 75.00%\n",
      "Fold 2 Epoch [183/200], Train Loss: 0.5003, Train Acc: 82.29%, Val Loss: 0.7072, Val Acc: 75.00%\n",
      "Fold 2 Epoch [184/200], Train Loss: 0.5057, Train Acc: 80.21%, Val Loss: 0.6770, Val Acc: 66.67%\n",
      "Fold 2 Epoch [185/200], Train Loss: 0.5128, Train Acc: 81.25%, Val Loss: 0.6275, Val Acc: 70.83%\n",
      "Fold 2 Epoch [186/200], Train Loss: 0.4853, Train Acc: 82.29%, Val Loss: 0.6008, Val Acc: 75.00%\n",
      "Fold 2 Epoch [187/200], Train Loss: 0.5363, Train Acc: 71.88%, Val Loss: 0.6906, Val Acc: 66.67%\n",
      "Fold 2 Epoch [188/200], Train Loss: 0.5464, Train Acc: 72.92%, Val Loss: 0.6688, Val Acc: 62.50%\n",
      "Fold 2 Epoch [189/200], Train Loss: 0.5077, Train Acc: 78.12%, Val Loss: 0.6749, Val Acc: 58.33%\n",
      "Fold 2 Epoch [190/200], Train Loss: 0.4259, Train Acc: 83.33%, Val Loss: 0.7456, Val Acc: 54.17%\n",
      "Fold 2 Epoch [191/200], Train Loss: 0.4719, Train Acc: 81.25%, Val Loss: 0.8722, Val Acc: 58.33%\n",
      "Fold 2 Epoch [192/200], Train Loss: 0.4349, Train Acc: 81.25%, Val Loss: 0.6936, Val Acc: 54.17%\n",
      "Fold 2 Epoch [193/200], Train Loss: 0.3876, Train Acc: 80.21%, Val Loss: 0.6573, Val Acc: 58.33%\n",
      "Fold 2 Epoch [194/200], Train Loss: 0.3960, Train Acc: 83.33%, Val Loss: 0.7620, Val Acc: 66.67%\n",
      "Fold 2 Epoch [195/200], Train Loss: 0.4189, Train Acc: 81.25%, Val Loss: 0.7046, Val Acc: 75.00%\n",
      "Fold 2 Epoch [196/200], Train Loss: 0.4278, Train Acc: 85.42%, Val Loss: 0.6276, Val Acc: 70.83%\n",
      "Fold 2 Epoch [197/200], Train Loss: 0.4708, Train Acc: 81.25%, Val Loss: 0.4628, Val Acc: 83.33%\n",
      "Fold 2 Epoch [198/200], Train Loss: 0.3858, Train Acc: 85.42%, Val Loss: 0.5759, Val Acc: 70.83%\n",
      "Fold 2 Epoch [199/200], Train Loss: 0.3863, Train Acc: 88.54%, Val Loss: 0.7014, Val Acc: 70.83%\n",
      "Fold 2 Epoch [200/200], Train Loss: 0.4261, Train Acc: 83.33%, Val Loss: 0.7708, Val Acc: 70.83%\n",
      "Fold 2 Best Accuracy: 83.33%\n",
      "Fold 3/5\n",
      "Fold 3 Epoch [1/200], Train Loss: 0.9584, Train Acc: 51.04%, Val Loss: 1.0117, Val Acc: 54.17%\n",
      "Fold 3 Epoch [2/200], Train Loss: 1.0552, Train Acc: 54.17%, Val Loss: 0.9814, Val Acc: 58.33%\n",
      "Fold 3 Epoch [3/200], Train Loss: 1.0035, Train Acc: 40.62%, Val Loss: 1.0619, Val Acc: 33.33%\n",
      "Fold 3 Epoch [4/200], Train Loss: 0.9529, Train Acc: 50.00%, Val Loss: 1.0129, Val Acc: 37.50%\n",
      "Fold 3 Epoch [5/200], Train Loss: 0.8951, Train Acc: 60.42%, Val Loss: 0.9476, Val Acc: 50.00%\n",
      "Fold 3 Epoch [6/200], Train Loss: 0.8393, Train Acc: 62.50%, Val Loss: 0.8528, Val Acc: 66.67%\n",
      "Fold 3 Epoch [7/200], Train Loss: 0.8380, Train Acc: 64.58%, Val Loss: 0.7836, Val Acc: 75.00%\n",
      "Fold 3 Epoch [8/200], Train Loss: 1.0168, Train Acc: 52.08%, Val Loss: 1.0723, Val Acc: 54.17%\n",
      "Fold 3 Epoch [9/200], Train Loss: 1.2040, Train Acc: 58.33%, Val Loss: 1.2182, Val Acc: 37.50%\n",
      "Fold 3 Epoch [10/200], Train Loss: 1.2823, Train Acc: 35.42%, Val Loss: 1.2786, Val Acc: 29.17%\n",
      "Fold 3 Epoch [11/200], Train Loss: 1.1462, Train Acc: 45.83%, Val Loss: 1.0596, Val Acc: 50.00%\n",
      "Fold 3 Epoch [12/200], Train Loss: 1.0949, Train Acc: 40.62%, Val Loss: 1.0492, Val Acc: 50.00%\n",
      "Fold 3 Epoch [13/200], Train Loss: 1.1324, Train Acc: 35.42%, Val Loss: 1.0949, Val Acc: 37.50%\n",
      "Fold 3 Epoch [14/200], Train Loss: 1.1293, Train Acc: 33.33%, Val Loss: 1.0982, Val Acc: 33.33%\n",
      "Fold 3 Epoch [15/200], Train Loss: 1.1077, Train Acc: 39.58%, Val Loss: 1.0930, Val Acc: 45.83%\n",
      "Fold 3 Epoch [16/200], Train Loss: 1.0990, Train Acc: 32.29%, Val Loss: 1.0850, Val Acc: 33.33%\n",
      "Fold 3 Epoch [17/200], Train Loss: 1.0902, Train Acc: 56.25%, Val Loss: 1.0841, Val Acc: 54.17%\n",
      "Fold 3 Epoch [18/200], Train Loss: 1.0815, Train Acc: 43.75%, Val Loss: 1.0774, Val Acc: 54.17%\n",
      "Fold 3 Epoch [19/200], Train Loss: 1.0785, Train Acc: 34.38%, Val Loss: 1.0838, Val Acc: 33.33%\n",
      "Fold 3 Epoch [20/200], Train Loss: 1.1043, Train Acc: 33.33%, Val Loss: 1.0847, Val Acc: 33.33%\n",
      "Fold 3 Epoch [21/200], Train Loss: 1.0554, Train Acc: 50.00%, Val Loss: 1.0428, Val Acc: 54.17%\n",
      "Fold 3 Epoch [22/200], Train Loss: 1.0626, Train Acc: 53.12%, Val Loss: 1.0195, Val Acc: 62.50%\n",
      "Fold 3 Epoch [23/200], Train Loss: 0.9973, Train Acc: 52.08%, Val Loss: 0.9486, Val Acc: 62.50%\n",
      "Fold 3 Epoch [24/200], Train Loss: 0.9872, Train Acc: 54.17%, Val Loss: 0.9592, Val Acc: 62.50%\n",
      "Fold 3 Epoch [25/200], Train Loss: 0.9767, Train Acc: 38.54%, Val Loss: 0.9250, Val Acc: 62.50%\n",
      "Fold 3 Epoch [26/200], Train Loss: 0.9394, Train Acc: 63.54%, Val Loss: 0.8486, Val Acc: 75.00%\n",
      "Fold 3 Epoch [27/200], Train Loss: 1.0166, Train Acc: 48.96%, Val Loss: 0.9301, Val Acc: 58.33%\n",
      "Fold 3 Epoch [28/200], Train Loss: 1.0329, Train Acc: 47.92%, Val Loss: 0.8961, Val Acc: 62.50%\n",
      "Fold 3 Epoch [29/200], Train Loss: 1.0242, Train Acc: 42.71%, Val Loss: 0.9790, Val Acc: 54.17%\n",
      "Fold 3 Epoch [30/200], Train Loss: 1.0372, Train Acc: 39.58%, Val Loss: 0.9619, Val Acc: 54.17%\n",
      "Fold 3 Epoch [31/200], Train Loss: 1.0648, Train Acc: 39.58%, Val Loss: 0.9980, Val Acc: 45.83%\n",
      "Fold 3 Epoch [32/200], Train Loss: 0.9565, Train Acc: 38.54%, Val Loss: 0.8548, Val Acc: 62.50%\n",
      "Fold 3 Epoch [33/200], Train Loss: 0.9213, Train Acc: 59.38%, Val Loss: 0.7739, Val Acc: 70.83%\n",
      "Fold 3 Epoch [34/200], Train Loss: 0.8815, Train Acc: 63.54%, Val Loss: 0.7308, Val Acc: 79.17%\n",
      "Fold 3 Epoch [35/200], Train Loss: 0.8452, Train Acc: 59.38%, Val Loss: 0.6786, Val Acc: 83.33%\n",
      "Fold 3 Epoch [36/200], Train Loss: 0.7893, Train Acc: 59.38%, Val Loss: 0.6547, Val Acc: 75.00%\n",
      "Fold 3 Epoch [37/200], Train Loss: 0.8189, Train Acc: 66.67%, Val Loss: 0.6478, Val Acc: 79.17%\n",
      "Fold 3 Epoch [38/200], Train Loss: 0.7427, Train Acc: 65.62%, Val Loss: 0.6378, Val Acc: 79.17%\n",
      "Fold 3 Epoch [39/200], Train Loss: 0.7699, Train Acc: 67.71%, Val Loss: 0.6594, Val Acc: 70.83%\n",
      "Fold 3 Epoch [40/200], Train Loss: 0.7631, Train Acc: 65.62%, Val Loss: 0.6618, Val Acc: 70.83%\n",
      "Fold 3 Epoch [41/200], Train Loss: 0.7744, Train Acc: 66.67%, Val Loss: 0.6357, Val Acc: 66.67%\n",
      "Fold 3 Epoch [42/200], Train Loss: 0.7113, Train Acc: 67.71%, Val Loss: 0.5765, Val Acc: 70.83%\n",
      "Fold 3 Epoch [43/200], Train Loss: 0.8841, Train Acc: 62.50%, Val Loss: 0.8826, Val Acc: 62.50%\n",
      "Fold 3 Epoch [44/200], Train Loss: 1.2057, Train Acc: 46.88%, Val Loss: 1.0206, Val Acc: 54.17%\n",
      "Fold 3 Epoch [45/200], Train Loss: 0.9126, Train Acc: 60.42%, Val Loss: 0.8174, Val Acc: 58.33%\n",
      "Fold 3 Epoch [46/200], Train Loss: 1.1309, Train Acc: 50.00%, Val Loss: 1.0470, Val Acc: 50.00%\n",
      "Fold 3 Epoch [47/200], Train Loss: 1.0334, Train Acc: 48.96%, Val Loss: 0.9238, Val Acc: 50.00%\n",
      "Fold 3 Epoch [48/200], Train Loss: 0.9042, Train Acc: 59.38%, Val Loss: 0.8329, Val Acc: 54.17%\n",
      "Fold 3 Epoch [49/200], Train Loss: 0.9596, Train Acc: 47.92%, Val Loss: 0.8119, Val Acc: 62.50%\n",
      "Fold 3 Epoch [50/200], Train Loss: 0.8895, Train Acc: 52.08%, Val Loss: 0.7730, Val Acc: 66.67%\n",
      "Fold 3 Epoch [51/200], Train Loss: 0.8668, Train Acc: 63.54%, Val Loss: 0.7386, Val Acc: 62.50%\n",
      "Fold 3 Epoch [52/200], Train Loss: 0.8311, Train Acc: 66.67%, Val Loss: 0.7589, Val Acc: 70.83%\n",
      "Fold 3 Epoch [53/200], Train Loss: 0.8029, Train Acc: 65.62%, Val Loss: 0.7363, Val Acc: 70.83%\n",
      "Fold 3 Epoch [54/200], Train Loss: 0.7924, Train Acc: 60.42%, Val Loss: 0.7033, Val Acc: 62.50%\n",
      "Fold 3 Epoch [55/200], Train Loss: 0.7535, Train Acc: 63.54%, Val Loss: 0.7235, Val Acc: 62.50%\n",
      "Fold 3 Epoch [56/200], Train Loss: 0.7851, Train Acc: 68.75%, Val Loss: 0.7071, Val Acc: 62.50%\n",
      "Fold 3 Epoch [57/200], Train Loss: 0.7564, Train Acc: 67.71%, Val Loss: 0.7017, Val Acc: 62.50%\n",
      "Fold 3 Epoch [58/200], Train Loss: 0.7765, Train Acc: 67.71%, Val Loss: 0.7401, Val Acc: 62.50%\n",
      "Fold 3 Epoch [59/200], Train Loss: 0.8168, Train Acc: 64.58%, Val Loss: 0.6928, Val Acc: 66.67%\n",
      "Fold 3 Epoch [60/200], Train Loss: 0.7316, Train Acc: 63.54%, Val Loss: 0.6546, Val Acc: 66.67%\n",
      "Fold 3 Epoch [61/200], Train Loss: 1.4319, Train Acc: 51.04%, Val Loss: 1.3653, Val Acc: 41.67%\n",
      "Fold 3 Epoch [62/200], Train Loss: 1.0454, Train Acc: 48.96%, Val Loss: 1.0708, Val Acc: 37.50%\n",
      "Fold 3 Epoch [63/200], Train Loss: 1.1525, Train Acc: 58.33%, Val Loss: 1.2919, Val Acc: 54.17%\n",
      "Fold 3 Epoch [64/200], Train Loss: 1.3349, Train Acc: 51.04%, Val Loss: 0.9161, Val Acc: 62.50%\n",
      "Fold 3 Epoch [65/200], Train Loss: 1.0064, Train Acc: 38.54%, Val Loss: 0.7749, Val Acc: 50.00%\n",
      "Fold 3 Epoch [66/200], Train Loss: 0.9968, Train Acc: 54.17%, Val Loss: 0.9175, Val Acc: 66.67%\n",
      "Fold 3 Epoch [67/200], Train Loss: 1.0260, Train Acc: 51.04%, Val Loss: 0.8999, Val Acc: 62.50%\n",
      "Fold 3 Epoch [68/200], Train Loss: 1.0301, Train Acc: 35.42%, Val Loss: 0.9141, Val Acc: 41.67%\n",
      "Fold 3 Epoch [69/200], Train Loss: 1.0127, Train Acc: 42.71%, Val Loss: 0.9436, Val Acc: 50.00%\n",
      "Fold 3 Epoch [70/200], Train Loss: 1.0659, Train Acc: 40.62%, Val Loss: 0.9881, Val Acc: 45.83%\n",
      "Fold 3 Epoch [71/200], Train Loss: 1.0913, Train Acc: 37.50%, Val Loss: 0.9786, Val Acc: 45.83%\n",
      "Fold 3 Epoch [72/200], Train Loss: 1.0676, Train Acc: 36.46%, Val Loss: 0.9686, Val Acc: 45.83%\n",
      "Fold 3 Epoch [73/200], Train Loss: 1.0900, Train Acc: 32.29%, Val Loss: 1.0263, Val Acc: 37.50%\n",
      "Fold 3 Epoch [74/200], Train Loss: 1.0789, Train Acc: 33.33%, Val Loss: 1.0335, Val Acc: 37.50%\n",
      "Fold 3 Epoch [75/200], Train Loss: 1.0760, Train Acc: 36.46%, Val Loss: 1.0332, Val Acc: 41.67%\n",
      "Fold 3 Epoch [76/200], Train Loss: 1.0633, Train Acc: 36.46%, Val Loss: 1.0423, Val Acc: 41.67%\n",
      "Fold 3 Epoch [77/200], Train Loss: 1.0770, Train Acc: 33.33%, Val Loss: 1.0586, Val Acc: 37.50%\n",
      "Fold 3 Epoch [78/200], Train Loss: 1.0723, Train Acc: 33.33%, Val Loss: 1.0710, Val Acc: 29.17%\n",
      "Fold 3 Epoch [79/200], Train Loss: 1.0807, Train Acc: 34.38%, Val Loss: 1.0748, Val Acc: 29.17%\n",
      "Fold 3 Epoch [80/200], Train Loss: 1.0739, Train Acc: 34.38%, Val Loss: 1.0723, Val Acc: 29.17%\n",
      "Fold 3 Epoch [81/200], Train Loss: 1.0815, Train Acc: 48.96%, Val Loss: 1.0610, Val Acc: 45.83%\n",
      "Fold 3 Epoch [82/200], Train Loss: 1.0567, Train Acc: 50.00%, Val Loss: 1.0425, Val Acc: 45.83%\n",
      "Fold 3 Epoch [83/200], Train Loss: 1.0390, Train Acc: 47.92%, Val Loss: 1.0210, Val Acc: 37.50%\n",
      "Fold 3 Epoch [84/200], Train Loss: 1.0304, Train Acc: 46.88%, Val Loss: 1.0045, Val Acc: 50.00%\n",
      "Fold 3 Epoch [85/200], Train Loss: 0.9982, Train Acc: 64.58%, Val Loss: 0.9817, Val Acc: 62.50%\n",
      "Fold 3 Epoch [86/200], Train Loss: 0.9807, Train Acc: 50.00%, Val Loss: 0.9736, Val Acc: 45.83%\n",
      "Fold 3 Epoch [87/200], Train Loss: 0.9690, Train Acc: 57.29%, Val Loss: 0.9651, Val Acc: 58.33%\n",
      "Fold 3 Epoch [88/200], Train Loss: 0.9156, Train Acc: 55.21%, Val Loss: 0.9473, Val Acc: 58.33%\n",
      "Fold 3 Epoch [89/200], Train Loss: 0.9166, Train Acc: 55.21%, Val Loss: 0.9378, Val Acc: 58.33%\n",
      "Fold 3 Epoch [90/200], Train Loss: 0.9570, Train Acc: 54.17%, Val Loss: 0.9535, Val Acc: 58.33%\n",
      "Fold 3 Epoch [91/200], Train Loss: 0.9038, Train Acc: 56.25%, Val Loss: 0.9333, Val Acc: 50.00%\n",
      "Fold 3 Epoch [92/200], Train Loss: 0.8928, Train Acc: 56.25%, Val Loss: 0.9313, Val Acc: 50.00%\n",
      "Fold 3 Epoch [93/200], Train Loss: 0.9483, Train Acc: 60.42%, Val Loss: 1.0657, Val Acc: 54.17%\n",
      "Fold 3 Epoch [94/200], Train Loss: 1.3518, Train Acc: 44.79%, Val Loss: 1.4249, Val Acc: 37.50%\n",
      "Fold 3 Epoch [95/200], Train Loss: 1.1786, Train Acc: 44.79%, Val Loss: 1.1491, Val Acc: 45.83%\n",
      "Fold 3 Epoch [96/200], Train Loss: 1.0616, Train Acc: 40.62%, Val Loss: 1.0407, Val Acc: 41.67%\n",
      "Fold 3 Epoch [97/200], Train Loss: 1.1569, Train Acc: 34.38%, Val Loss: 1.1354, Val Acc: 37.50%\n",
      "Fold 3 Epoch [98/200], Train Loss: 1.1422, Train Acc: 34.38%, Val Loss: 1.1036, Val Acc: 37.50%\n",
      "Fold 3 Epoch [99/200], Train Loss: 1.0341, Train Acc: 44.79%, Val Loss: 1.0013, Val Acc: 50.00%\n",
      "Fold 3 Epoch [100/200], Train Loss: 0.9490, Train Acc: 55.21%, Val Loss: 0.9298, Val Acc: 58.33%\n",
      "Fold 3 Epoch [101/200], Train Loss: 0.8907, Train Acc: 58.33%, Val Loss: 0.9053, Val Acc: 58.33%\n",
      "Fold 3 Epoch [102/200], Train Loss: 0.8903, Train Acc: 61.46%, Val Loss: 0.8957, Val Acc: 62.50%\n",
      "Fold 3 Epoch [103/200], Train Loss: 0.9029, Train Acc: 59.38%, Val Loss: 0.8716, Val Acc: 66.67%\n",
      "Fold 3 Epoch [104/200], Train Loss: 0.8299, Train Acc: 61.46%, Val Loss: 0.8127, Val Acc: 70.83%\n",
      "Fold 3 Epoch [105/200], Train Loss: 0.7967, Train Acc: 64.58%, Val Loss: 0.7449, Val Acc: 75.00%\n",
      "Fold 3 Epoch [106/200], Train Loss: 0.7646, Train Acc: 66.67%, Val Loss: 0.7669, Val Acc: 70.83%\n",
      "Fold 3 Epoch [107/200], Train Loss: 1.0184, Train Acc: 54.17%, Val Loss: 1.0816, Val Acc: 45.83%\n",
      "Fold 3 Epoch [108/200], Train Loss: 0.7384, Train Acc: 66.67%, Val Loss: 0.7491, Val Acc: 66.67%\n",
      "Fold 3 Epoch [109/200], Train Loss: 0.7795, Train Acc: 65.62%, Val Loss: 0.7676, Val Acc: 70.83%\n",
      "Fold 3 Epoch [110/200], Train Loss: 0.7637, Train Acc: 77.08%, Val Loss: 0.7661, Val Acc: 66.67%\n",
      "Fold 3 Epoch [111/200], Train Loss: 0.7545, Train Acc: 73.96%, Val Loss: 0.7741, Val Acc: 66.67%\n",
      "Fold 3 Epoch [112/200], Train Loss: 0.7009, Train Acc: 76.04%, Val Loss: 0.7130, Val Acc: 75.00%\n",
      "Fold 3 Epoch [113/200], Train Loss: 0.6652, Train Acc: 76.04%, Val Loss: 0.7224, Val Acc: 75.00%\n",
      "Fold 3 Epoch [114/200], Train Loss: 0.6626, Train Acc: 75.00%, Val Loss: 0.7028, Val Acc: 75.00%\n",
      "Fold 3 Epoch [115/200], Train Loss: 0.6243, Train Acc: 68.75%, Val Loss: 0.5358, Val Acc: 75.00%\n",
      "Fold 3 Epoch [116/200], Train Loss: 1.5584, Train Acc: 39.58%, Val Loss: 1.3748, Val Acc: 45.83%\n",
      "Fold 3 Epoch [117/200], Train Loss: 1.2095, Train Acc: 45.83%, Val Loss: 1.1276, Val Acc: 54.17%\n",
      "Fold 3 Epoch [118/200], Train Loss: 1.0010, Train Acc: 50.00%, Val Loss: 0.9565, Val Acc: 54.17%\n",
      "Fold 3 Epoch [119/200], Train Loss: 1.0487, Train Acc: 46.88%, Val Loss: 1.0194, Val Acc: 50.00%\n",
      "Fold 3 Epoch [120/200], Train Loss: 0.9673, Train Acc: 51.04%, Val Loss: 0.9211, Val Acc: 54.17%\n",
      "Fold 3 Epoch [121/200], Train Loss: 0.9749, Train Acc: 47.92%, Val Loss: 0.9275, Val Acc: 54.17%\n",
      "Fold 3 Epoch [122/200], Train Loss: 0.9641, Train Acc: 47.92%, Val Loss: 0.9100, Val Acc: 54.17%\n",
      "Fold 3 Epoch [123/200], Train Loss: 0.9502, Train Acc: 64.58%, Val Loss: 0.8828, Val Acc: 62.50%\n",
      "Fold 3 Epoch [124/200], Train Loss: 0.9121, Train Acc: 58.33%, Val Loss: 0.8789, Val Acc: 58.33%\n",
      "Fold 3 Epoch [125/200], Train Loss: 0.8992, Train Acc: 64.58%, Val Loss: 0.8623, Val Acc: 62.50%\n",
      "Fold 3 Epoch [126/200], Train Loss: 0.9092, Train Acc: 70.83%, Val Loss: 0.8523, Val Acc: 66.67%\n",
      "Fold 3 Epoch [127/200], Train Loss: 0.8674, Train Acc: 67.71%, Val Loss: 0.8317, Val Acc: 62.50%\n",
      "Fold 3 Epoch [128/200], Train Loss: 0.8787, Train Acc: 62.50%, Val Loss: 0.8381, Val Acc: 66.67%\n",
      "Fold 3 Epoch [129/200], Train Loss: 0.8024, Train Acc: 69.79%, Val Loss: 0.7926, Val Acc: 62.50%\n",
      "Fold 3 Epoch [130/200], Train Loss: 0.8124, Train Acc: 64.58%, Val Loss: 0.8164, Val Acc: 62.50%\n",
      "Fold 3 Epoch [131/200], Train Loss: 0.8215, Train Acc: 65.62%, Val Loss: 0.7959, Val Acc: 75.00%\n",
      "Fold 3 Epoch [132/200], Train Loss: 0.7009, Train Acc: 72.92%, Val Loss: 0.7237, Val Acc: 66.67%\n",
      "Fold 3 Epoch [133/200], Train Loss: 0.8767, Train Acc: 52.08%, Val Loss: 0.8295, Val Acc: 54.17%\n",
      "Fold 3 Epoch [134/200], Train Loss: 0.8400, Train Acc: 55.21%, Val Loss: 0.8299, Val Acc: 58.33%\n",
      "Fold 3 Epoch [135/200], Train Loss: 0.9701, Train Acc: 48.96%, Val Loss: 0.8787, Val Acc: 62.50%\n",
      "Fold 3 Epoch [136/200], Train Loss: 0.8689, Train Acc: 67.71%, Val Loss: 0.9138, Val Acc: 58.33%\n",
      "Fold 3 Epoch [137/200], Train Loss: 0.8441, Train Acc: 67.71%, Val Loss: 0.8846, Val Acc: 58.33%\n",
      "Fold 3 Epoch [138/200], Train Loss: 0.7396, Train Acc: 73.96%, Val Loss: 0.7022, Val Acc: 79.17%\n",
      "Fold 3 Epoch [139/200], Train Loss: 0.7883, Train Acc: 67.71%, Val Loss: 0.7227, Val Acc: 70.83%\n",
      "Fold 3 Epoch [140/200], Train Loss: 0.8217, Train Acc: 56.25%, Val Loss: 0.8153, Val Acc: 62.50%\n",
      "Fold 3 Epoch [141/200], Train Loss: 0.8662, Train Acc: 47.92%, Val Loss: 0.7841, Val Acc: 62.50%\n",
      "Fold 3 Epoch [142/200], Train Loss: 0.7229, Train Acc: 71.88%, Val Loss: 0.6334, Val Acc: 75.00%\n",
      "Fold 3 Epoch [143/200], Train Loss: 0.8010, Train Acc: 67.71%, Val Loss: 0.9167, Val Acc: 50.00%\n",
      "Fold 3 Epoch [144/200], Train Loss: 0.7183, Train Acc: 72.92%, Val Loss: 0.6876, Val Acc: 75.00%\n",
      "Fold 3 Epoch [145/200], Train Loss: 0.7076, Train Acc: 80.21%, Val Loss: 0.5818, Val Acc: 91.67%\n",
      "Fold 3 Epoch [146/200], Train Loss: 0.6725, Train Acc: 79.17%, Val Loss: 0.5874, Val Acc: 83.33%\n",
      "Fold 3 Epoch [147/200], Train Loss: 1.4289, Train Acc: 48.96%, Val Loss: 1.3794, Val Acc: 58.33%\n",
      "Fold 3 Epoch [148/200], Train Loss: 0.9334, Train Acc: 65.62%, Val Loss: 1.0085, Val Acc: 62.50%\n",
      "Fold 3 Epoch [149/200], Train Loss: 1.1146, Train Acc: 46.88%, Val Loss: 1.0131, Val Acc: 45.83%\n",
      "Fold 3 Epoch [150/200], Train Loss: 0.9083, Train Acc: 77.08%, Val Loss: 0.8567, Val Acc: 83.33%\n",
      "Fold 3 Epoch [151/200], Train Loss: 0.9783, Train Acc: 50.00%, Val Loss: 0.9439, Val Acc: 62.50%\n",
      "Fold 3 Epoch [152/200], Train Loss: 0.9258, Train Acc: 51.04%, Val Loss: 0.8828, Val Acc: 62.50%\n",
      "Fold 3 Epoch [153/200], Train Loss: 0.8106, Train Acc: 51.04%, Val Loss: 0.7091, Val Acc: 66.67%\n",
      "Fold 3 Epoch [154/200], Train Loss: 0.7294, Train Acc: 75.00%, Val Loss: 0.6269, Val Acc: 70.83%\n",
      "Fold 3 Epoch [155/200], Train Loss: 0.6606, Train Acc: 71.88%, Val Loss: 0.5912, Val Acc: 75.00%\n",
      "Fold 3 Epoch [156/200], Train Loss: 0.6789, Train Acc: 79.17%, Val Loss: 0.5267, Val Acc: 83.33%\n",
      "Fold 3 Epoch [157/200], Train Loss: 0.6105, Train Acc: 83.33%, Val Loss: 0.4741, Val Acc: 87.50%\n",
      "Fold 3 Epoch [158/200], Train Loss: 0.5896, Train Acc: 81.25%, Val Loss: 0.4375, Val Acc: 83.33%\n",
      "Fold 3 Epoch [159/200], Train Loss: 0.5519, Train Acc: 82.29%, Val Loss: 0.4337, Val Acc: 91.67%\n",
      "Fold 3 Epoch [160/200], Train Loss: 0.5724, Train Acc: 76.04%, Val Loss: 0.4683, Val Acc: 79.17%\n",
      "Fold 3 Epoch [161/200], Train Loss: 0.5728, Train Acc: 80.21%, Val Loss: 0.4760, Val Acc: 83.33%\n",
      "Fold 3 Epoch [162/200], Train Loss: 0.5820, Train Acc: 82.29%, Val Loss: 0.5082, Val Acc: 87.50%\n",
      "Fold 3 Epoch [163/200], Train Loss: 0.4721, Train Acc: 83.33%, Val Loss: 0.4533, Val Acc: 87.50%\n",
      "Fold 3 Epoch [164/200], Train Loss: 0.4653, Train Acc: 86.46%, Val Loss: 0.3598, Val Acc: 87.50%\n",
      "Fold 3 Epoch [165/200], Train Loss: 0.4870, Train Acc: 84.38%, Val Loss: 0.3650, Val Acc: 91.67%\n",
      "Fold 3 Epoch [166/200], Train Loss: 0.4988, Train Acc: 83.33%, Val Loss: 0.4325, Val Acc: 87.50%\n",
      "Fold 3 Epoch [167/200], Train Loss: 0.4297, Train Acc: 82.29%, Val Loss: 0.3479, Val Acc: 87.50%\n",
      "Fold 3 Epoch [168/200], Train Loss: 0.3885, Train Acc: 88.54%, Val Loss: 0.2637, Val Acc: 91.67%\n",
      "Fold 3 Epoch [169/200], Train Loss: 0.4294, Train Acc: 87.50%, Val Loss: 0.2653, Val Acc: 91.67%\n",
      "Fold 3 Epoch [170/200], Train Loss: 0.3898, Train Acc: 86.46%, Val Loss: 0.3197, Val Acc: 91.67%\n",
      "Fold 3 Epoch [171/200], Train Loss: 0.3807, Train Acc: 86.46%, Val Loss: 0.1232, Val Acc: 100.00%\n",
      "Fold 3 Epoch [172/200], Train Loss: 0.3488, Train Acc: 87.50%, Val Loss: 0.1645, Val Acc: 95.83%\n",
      "Fold 3 Epoch [173/200], Train Loss: 0.4093, Train Acc: 86.46%, Val Loss: 0.1743, Val Acc: 95.83%\n",
      "Fold 3 Epoch [174/200], Train Loss: 0.2861, Train Acc: 91.67%, Val Loss: 0.1808, Val Acc: 91.67%\n",
      "Fold 3 Epoch [175/200], Train Loss: 0.2842, Train Acc: 88.54%, Val Loss: 0.1691, Val Acc: 91.67%\n",
      "Fold 3 Epoch [176/200], Train Loss: 0.5761, Train Acc: 84.38%, Val Loss: 0.4909, Val Acc: 83.33%\n",
      "Fold 3 Epoch [177/200], Train Loss: 0.3555, Train Acc: 85.42%, Val Loss: 0.4876, Val Acc: 87.50%\n",
      "Fold 3 Epoch [178/200], Train Loss: 0.3813, Train Acc: 84.38%, Val Loss: 0.3598, Val Acc: 83.33%\n",
      "Fold 3 Epoch [179/200], Train Loss: 0.3063, Train Acc: 89.58%, Val Loss: 0.2817, Val Acc: 91.67%\n",
      "Fold 3 Epoch [180/200], Train Loss: 0.3518, Train Acc: 88.54%, Val Loss: 0.4437, Val Acc: 91.67%\n",
      "Fold 3 Epoch [181/200], Train Loss: 0.3112, Train Acc: 89.58%, Val Loss: 0.5586, Val Acc: 87.50%\n",
      "Fold 3 Epoch [182/200], Train Loss: 0.2983, Train Acc: 89.58%, Val Loss: 0.2114, Val Acc: 91.67%\n",
      "Fold 3 Epoch [183/200], Train Loss: 0.3003, Train Acc: 89.58%, Val Loss: 0.2084, Val Acc: 91.67%\n",
      "Fold 3 Epoch [184/200], Train Loss: 0.3004, Train Acc: 90.62%, Val Loss: 0.4602, Val Acc: 83.33%\n",
      "Fold 3 Epoch [185/200], Train Loss: 0.2714, Train Acc: 90.62%, Val Loss: 0.6143, Val Acc: 83.33%\n",
      "Fold 3 Epoch [186/200], Train Loss: 0.2851, Train Acc: 89.58%, Val Loss: 0.7930, Val Acc: 83.33%\n",
      "Fold 3 Epoch [187/200], Train Loss: 0.3040, Train Acc: 87.50%, Val Loss: 0.2960, Val Acc: 91.67%\n",
      "Fold 3 Epoch [188/200], Train Loss: 0.3086, Train Acc: 88.54%, Val Loss: 0.2994, Val Acc: 91.67%\n",
      "Fold 3 Epoch [189/200], Train Loss: 0.3105, Train Acc: 89.58%, Val Loss: 0.2824, Val Acc: 95.83%\n",
      "Fold 3 Epoch [190/200], Train Loss: 0.3010, Train Acc: 87.50%, Val Loss: 0.5843, Val Acc: 83.33%\n",
      "Fold 3 Epoch [191/200], Train Loss: 0.3971, Train Acc: 86.46%, Val Loss: 0.8259, Val Acc: 83.33%\n",
      "Fold 3 Epoch [192/200], Train Loss: 0.3216, Train Acc: 89.58%, Val Loss: 0.3233, Val Acc: 83.33%\n",
      "Fold 3 Epoch [193/200], Train Loss: 0.3226, Train Acc: 89.58%, Val Loss: 0.2805, Val Acc: 87.50%\n",
      "Fold 3 Epoch [194/200], Train Loss: 0.2763, Train Acc: 89.58%, Val Loss: 0.7991, Val Acc: 83.33%\n",
      "Fold 3 Epoch [195/200], Train Loss: 0.6799, Train Acc: 80.21%, Val Loss: 0.8383, Val Acc: 79.17%\n",
      "Fold 3 Epoch [196/200], Train Loss: 0.4728, Train Acc: 79.17%, Val Loss: 0.6929, Val Acc: 70.83%\n",
      "Fold 3 Epoch [197/200], Train Loss: 0.5344, Train Acc: 81.25%, Val Loss: 0.7191, Val Acc: 70.83%\n",
      "Fold 3 Epoch [198/200], Train Loss: 0.5884, Train Acc: 77.08%, Val Loss: 0.6717, Val Acc: 70.83%\n",
      "Fold 3 Epoch [199/200], Train Loss: 0.5426, Train Acc: 78.12%, Val Loss: 0.6333, Val Acc: 70.83%\n",
      "Fold 3 Epoch [200/200], Train Loss: 0.4845, Train Acc: 78.12%, Val Loss: 0.5988, Val Acc: 70.83%\n",
      "Fold 3 Best Accuracy: 100.00%\n",
      "Fold 4/5\n",
      "Fold 4 Epoch [1/200], Train Loss: 1.0875, Train Acc: 56.25%, Val Loss: 1.3901, Val Acc: 45.83%\n",
      "Fold 4 Epoch [2/200], Train Loss: 2.0868, Train Acc: 45.83%, Val Loss: 2.0873, Val Acc: 33.33%\n",
      "Fold 4 Epoch [3/200], Train Loss: 1.2884, Train Acc: 32.29%, Val Loss: 1.1848, Val Acc: 41.67%\n",
      "Fold 4 Epoch [4/200], Train Loss: 1.3482, Train Acc: 35.42%, Val Loss: 1.3788, Val Acc: 25.00%\n",
      "Fold 4 Epoch [5/200], Train Loss: 1.1033, Train Acc: 35.42%, Val Loss: 1.1457, Val Acc: 25.00%\n",
      "Fold 4 Epoch [6/200], Train Loss: 1.2572, Train Acc: 33.33%, Val Loss: 1.2506, Val Acc: 29.17%\n",
      "Fold 4 Epoch [7/200], Train Loss: 1.1201, Train Acc: 33.33%, Val Loss: 1.1305, Val Acc: 29.17%\n",
      "Fold 4 Epoch [8/200], Train Loss: 1.1289, Train Acc: 36.46%, Val Loss: 1.1675, Val Acc: 25.00%\n",
      "Fold 4 Epoch [9/200], Train Loss: 1.1350, Train Acc: 39.58%, Val Loss: 1.1986, Val Acc: 33.33%\n",
      "Fold 4 Epoch [10/200], Train Loss: 1.0764, Train Acc: 37.50%, Val Loss: 1.1133, Val Acc: 37.50%\n",
      "Fold 4 Epoch [11/200], Train Loss: 1.1123, Train Acc: 37.50%, Val Loss: 1.0930, Val Acc: 37.50%\n",
      "Fold 4 Epoch [12/200], Train Loss: 1.0942, Train Acc: 37.50%, Val Loss: 1.0303, Val Acc: 37.50%\n",
      "Fold 4 Epoch [13/200], Train Loss: 1.0817, Train Acc: 39.58%, Val Loss: 1.0477, Val Acc: 33.33%\n",
      "Fold 4 Epoch [14/200], Train Loss: 1.1313, Train Acc: 39.58%, Val Loss: 1.1192, Val Acc: 33.33%\n",
      "Fold 4 Epoch [15/200], Train Loss: 1.0693, Train Acc: 39.58%, Val Loss: 1.0833, Val Acc: 33.33%\n",
      "Fold 4 Epoch [16/200], Train Loss: 1.0968, Train Acc: 37.50%, Val Loss: 1.0651, Val Acc: 37.50%\n",
      "Fold 4 Epoch [17/200], Train Loss: 1.0656, Train Acc: 30.21%, Val Loss: 1.0148, Val Acc: 45.83%\n",
      "Fold 4 Epoch [18/200], Train Loss: 1.1188, Train Acc: 39.58%, Val Loss: 1.0480, Val Acc: 33.33%\n",
      "Fold 4 Epoch [19/200], Train Loss: 1.0770, Train Acc: 39.58%, Val Loss: 1.0784, Val Acc: 33.33%\n",
      "Fold 4 Epoch [20/200], Train Loss: 1.0661, Train Acc: 37.50%, Val Loss: 1.1022, Val Acc: 37.50%\n",
      "Fold 4 Epoch [21/200], Train Loss: 1.0743, Train Acc: 37.50%, Val Loss: 1.0973, Val Acc: 37.50%\n",
      "Fold 4 Epoch [22/200], Train Loss: 1.0568, Train Acc: 39.58%, Val Loss: 1.0786, Val Acc: 33.33%\n",
      "Fold 4 Epoch [23/200], Train Loss: 1.0421, Train Acc: 39.58%, Val Loss: 1.0774, Val Acc: 33.33%\n",
      "Fold 4 Epoch [24/200], Train Loss: 1.0632, Train Acc: 36.46%, Val Loss: 1.0664, Val Acc: 37.50%\n",
      "Fold 4 Epoch [25/200], Train Loss: 1.0799, Train Acc: 35.42%, Val Loss: 1.0574, Val Acc: 37.50%\n",
      "Fold 4 Epoch [26/200], Train Loss: 1.0707, Train Acc: 37.50%, Val Loss: 1.0707, Val Acc: 29.17%\n",
      "Fold 4 Epoch [27/200], Train Loss: 1.0819, Train Acc: 37.50%, Val Loss: 1.1261, Val Acc: 25.00%\n",
      "Fold 4 Epoch [28/200], Train Loss: 1.0928, Train Acc: 34.38%, Val Loss: 1.1182, Val Acc: 29.17%\n",
      "Fold 4 Epoch [29/200], Train Loss: 1.1102, Train Acc: 34.38%, Val Loss: 1.1263, Val Acc: 29.17%\n",
      "Fold 4 Epoch [30/200], Train Loss: 1.1033, Train Acc: 34.38%, Val Loss: 1.1168, Val Acc: 29.17%\n",
      "Fold 4 Epoch [31/200], Train Loss: 1.0931, Train Acc: 35.42%, Val Loss: 1.1657, Val Acc: 25.00%\n",
      "Fold 4 Epoch [32/200], Train Loss: 1.1155, Train Acc: 35.42%, Val Loss: 1.1673, Val Acc: 25.00%\n",
      "Fold 4 Epoch [33/200], Train Loss: 1.0962, Train Acc: 35.42%, Val Loss: 1.1223, Val Acc: 25.00%\n",
      "Fold 4 Epoch [34/200], Train Loss: 1.1029, Train Acc: 30.21%, Val Loss: 1.0835, Val Acc: 45.83%\n",
      "Fold 4 Epoch [35/200], Train Loss: 1.1024, Train Acc: 35.42%, Val Loss: 1.1589, Val Acc: 25.00%\n",
      "Fold 4 Epoch [36/200], Train Loss: 1.1026, Train Acc: 34.38%, Val Loss: 1.1195, Val Acc: 29.17%\n",
      "Fold 4 Epoch [37/200], Train Loss: 1.1174, Train Acc: 34.38%, Val Loss: 1.1066, Val Acc: 29.17%\n",
      "Fold 4 Epoch [38/200], Train Loss: 1.1031, Train Acc: 34.38%, Val Loss: 1.0941, Val Acc: 29.17%\n",
      "Fold 4 Epoch [39/200], Train Loss: 1.1010, Train Acc: 35.42%, Val Loss: 1.1227, Val Acc: 25.00%\n",
      "Fold 4 Epoch [40/200], Train Loss: 1.0974, Train Acc: 35.42%, Val Loss: 1.1063, Val Acc: 25.00%\n",
      "Fold 4 Epoch [41/200], Train Loss: 1.1230, Train Acc: 34.38%, Val Loss: 1.1687, Val Acc: 29.17%\n",
      "Fold 4 Epoch [42/200], Train Loss: 1.1038, Train Acc: 34.38%, Val Loss: 1.1547, Val Acc: 29.17%\n",
      "Fold 4 Epoch [43/200], Train Loss: 1.1120, Train Acc: 35.42%, Val Loss: 1.1272, Val Acc: 25.00%\n",
      "Fold 4 Epoch [44/200], Train Loss: 1.1101, Train Acc: 35.42%, Val Loss: 1.1051, Val Acc: 25.00%\n",
      "Fold 4 Epoch [45/200], Train Loss: 1.0872, Train Acc: 36.46%, Val Loss: 1.1312, Val Acc: 25.00%\n",
      "Fold 4 Epoch [46/200], Train Loss: 1.0906, Train Acc: 35.42%, Val Loss: 1.1405, Val Acc: 29.17%\n",
      "Fold 4 Epoch [47/200], Train Loss: 1.0940, Train Acc: 34.38%, Val Loss: 1.1073, Val Acc: 29.17%\n",
      "Fold 4 Epoch [48/200], Train Loss: 1.1016, Train Acc: 30.21%, Val Loss: 1.0889, Val Acc: 45.83%\n",
      "Fold 4 Epoch [49/200], Train Loss: 1.1039, Train Acc: 34.38%, Val Loss: 1.1493, Val Acc: 29.17%\n",
      "Fold 4 Epoch [50/200], Train Loss: 1.1039, Train Acc: 35.42%, Val Loss: 1.1683, Val Acc: 25.00%\n",
      "Fold 4 Epoch [51/200], Train Loss: 1.1170, Train Acc: 35.42%, Val Loss: 1.1001, Val Acc: 25.00%\n",
      "Fold 4 Epoch [52/200], Train Loss: 1.1061, Train Acc: 30.21%, Val Loss: 1.0736, Val Acc: 45.83%\n",
      "Fold 4 Epoch [53/200], Train Loss: 1.0983, Train Acc: 34.38%, Val Loss: 1.1214, Val Acc: 29.17%\n",
      "Fold 4 Epoch [54/200], Train Loss: 1.0944, Train Acc: 35.42%, Val Loss: 1.1399, Val Acc: 25.00%\n",
      "Fold 4 Epoch [55/200], Train Loss: 1.0979, Train Acc: 34.38%, Val Loss: 1.1163, Val Acc: 29.17%\n",
      "Fold 4 Epoch [56/200], Train Loss: 1.1013, Train Acc: 34.38%, Val Loss: 1.1096, Val Acc: 29.17%\n",
      "Fold 4 Epoch [57/200], Train Loss: 1.0968, Train Acc: 35.42%, Val Loss: 1.1457, Val Acc: 25.00%\n",
      "Fold 4 Epoch [58/200], Train Loss: 1.1055, Train Acc: 35.42%, Val Loss: 1.1605, Val Acc: 25.00%\n",
      "Fold 4 Epoch [59/200], Train Loss: 1.0965, Train Acc: 35.42%, Val Loss: 1.1117, Val Acc: 25.00%\n",
      "Fold 4 Epoch [60/200], Train Loss: 1.1035, Train Acc: 30.21%, Val Loss: 1.0907, Val Acc: 45.83%\n",
      "Fold 4 Epoch [61/200], Train Loss: 1.1192, Train Acc: 35.42%, Val Loss: 1.1543, Val Acc: 25.00%\n",
      "Fold 4 Epoch [62/200], Train Loss: 1.1251, Train Acc: 35.42%, Val Loss: 1.1862, Val Acc: 25.00%\n",
      "Fold 4 Epoch [63/200], Train Loss: 1.1057, Train Acc: 34.38%, Val Loss: 1.1379, Val Acc: 29.17%\n",
      "Fold 4 Epoch [64/200], Train Loss: 1.1215, Train Acc: 34.38%, Val Loss: 1.0982, Val Acc: 29.17%\n",
      "Fold 4 Epoch [65/200], Train Loss: 1.1010, Train Acc: 35.42%, Val Loss: 1.1062, Val Acc: 25.00%\n",
      "Fold 4 Epoch [66/200], Train Loss: 1.1154, Train Acc: 35.42%, Val Loss: 1.1709, Val Acc: 25.00%\n",
      "Fold 4 Epoch [67/200], Train Loss: 1.1013, Train Acc: 35.42%, Val Loss: 1.1361, Val Acc: 25.00%\n",
      "Fold 4 Epoch [68/200], Train Loss: 1.1080, Train Acc: 34.38%, Val Loss: 1.0962, Val Acc: 29.17%\n",
      "Fold 4 Epoch [69/200], Train Loss: 1.0968, Train Acc: 35.42%, Val Loss: 1.1139, Val Acc: 25.00%\n",
      "Fold 4 Epoch [70/200], Train Loss: 1.1104, Train Acc: 35.42%, Val Loss: 1.2003, Val Acc: 25.00%\n",
      "Fold 4 Epoch [71/200], Train Loss: 1.0965, Train Acc: 35.42%, Val Loss: 1.1204, Val Acc: 25.00%\n",
      "Fold 4 Epoch [72/200], Train Loss: 1.1016, Train Acc: 34.38%, Val Loss: 1.0938, Val Acc: 29.17%\n",
      "Fold 4 Epoch [73/200], Train Loss: 1.1023, Train Acc: 34.38%, Val Loss: 1.1245, Val Acc: 29.17%\n",
      "Fold 4 Epoch [74/200], Train Loss: 1.1014, Train Acc: 33.33%, Val Loss: 1.1587, Val Acc: 29.17%\n",
      "Fold 4 Epoch [75/200], Train Loss: 1.1109, Train Acc: 35.42%, Val Loss: 1.1688, Val Acc: 25.00%\n",
      "Fold 4 Epoch [76/200], Train Loss: 1.1018, Train Acc: 35.42%, Val Loss: 1.1367, Val Acc: 25.00%\n",
      "Fold 4 Epoch [77/200], Train Loss: 1.0934, Train Acc: 35.42%, Val Loss: 1.1013, Val Acc: 25.00%\n",
      "Fold 4 Epoch [78/200], Train Loss: 1.1243, Train Acc: 30.21%, Val Loss: 1.0772, Val Acc: 45.83%\n",
      "Fold 4 Epoch [79/200], Train Loss: 1.0947, Train Acc: 30.21%, Val Loss: 1.0826, Val Acc: 45.83%\n",
      "Fold 4 Epoch [80/200], Train Loss: 1.1160, Train Acc: 34.38%, Val Loss: 1.1800, Val Acc: 29.17%\n",
      "Fold 4 Epoch [81/200], Train Loss: 1.1096, Train Acc: 35.42%, Val Loss: 1.1705, Val Acc: 25.00%\n",
      "Fold 4 Epoch [82/200], Train Loss: 1.0870, Train Acc: 30.21%, Val Loss: 1.0437, Val Acc: 45.83%\n",
      "Fold 4 Epoch [83/200], Train Loss: 1.0296, Train Acc: 45.83%, Val Loss: 1.0191, Val Acc: 41.67%\n",
      "Fold 4 Epoch [84/200], Train Loss: 1.1162, Train Acc: 35.42%, Val Loss: 1.1490, Val Acc: 29.17%\n",
      "Fold 4 Epoch [85/200], Train Loss: 1.1044, Train Acc: 37.50%, Val Loss: 1.1534, Val Acc: 29.17%\n",
      "Fold 4 Epoch [86/200], Train Loss: 1.0637, Train Acc: 39.58%, Val Loss: 1.0747, Val Acc: 33.33%\n",
      "Fold 4 Epoch [87/200], Train Loss: 1.0649, Train Acc: 39.58%, Val Loss: 1.0344, Val Acc: 33.33%\n",
      "Fold 4 Epoch [88/200], Train Loss: 1.0678, Train Acc: 44.79%, Val Loss: 1.0215, Val Acc: 37.50%\n",
      "Fold 4 Epoch [89/200], Train Loss: 1.0631, Train Acc: 39.58%, Val Loss: 1.0418, Val Acc: 33.33%\n",
      "Fold 4 Epoch [90/200], Train Loss: 1.0611, Train Acc: 39.58%, Val Loss: 1.0488, Val Acc: 33.33%\n",
      "Fold 4 Epoch [91/200], Train Loss: 1.0671, Train Acc: 39.58%, Val Loss: 1.0819, Val Acc: 33.33%\n",
      "Fold 4 Epoch [92/200], Train Loss: 1.0729, Train Acc: 39.58%, Val Loss: 1.1040, Val Acc: 33.33%\n",
      "Fold 4 Epoch [93/200], Train Loss: 1.0698, Train Acc: 39.58%, Val Loss: 1.0665, Val Acc: 33.33%\n",
      "Fold 4 Epoch [94/200], Train Loss: 1.0723, Train Acc: 30.21%, Val Loss: 1.0434, Val Acc: 45.83%\n",
      "Fold 4 Epoch [95/200], Train Loss: 1.0647, Train Acc: 39.58%, Val Loss: 1.0626, Val Acc: 33.33%\n",
      "Fold 4 Epoch [96/200], Train Loss: 1.0457, Train Acc: 39.58%, Val Loss: 1.1033, Val Acc: 33.33%\n",
      "Fold 4 Epoch [97/200], Train Loss: 1.0619, Train Acc: 37.50%, Val Loss: 1.0858, Val Acc: 37.50%\n",
      "Fold 4 Epoch [98/200], Train Loss: 1.0663, Train Acc: 37.50%, Val Loss: 1.0623, Val Acc: 37.50%\n",
      "Fold 4 Epoch [99/200], Train Loss: 1.0604, Train Acc: 37.50%, Val Loss: 1.0400, Val Acc: 37.50%\n",
      "Fold 4 Epoch [100/200], Train Loss: 1.0572, Train Acc: 53.12%, Val Loss: 1.0100, Val Acc: 54.17%\n",
      "Fold 4 Epoch [101/200], Train Loss: 1.0647, Train Acc: 37.50%, Val Loss: 1.0368, Val Acc: 37.50%\n",
      "Fold 4 Epoch [102/200], Train Loss: 1.0482, Train Acc: 37.50%, Val Loss: 1.0622, Val Acc: 37.50%\n",
      "Fold 4 Epoch [103/200], Train Loss: 1.0707, Train Acc: 39.58%, Val Loss: 1.0747, Val Acc: 33.33%\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "fold_losses = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_all)):\n",
    "    print(f'Fold {fold + 1}/{n_folds}')\n",
    "    \n",
    "    # Split data\n",
    "    X_train_fold = X_all[train_idx]\n",
    "    y_train_fold = y_all[train_idx]\n",
    "    X_val_fold = X_all[val_idx]\n",
    "    y_val_fold = y_all[val_idx]\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_train_fold, y_train_fold)\n",
    "    val_dataset = TensorDataset(X_val_fold, y_val_fold)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    # Initialize model for each fold\n",
    "    model = OptimizedLSTMModel(\n",
    "        num_classes=NUM_CLASSES,\n",
    "        input_channels=3,\n",
    "        seq_length=int(CHUNK_SIZE * SAMPLING_RATE)\n",
    "    )\n",
    "    optimizer = Adam(model.parameters(), lr=0.02, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-4)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_accuracy = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(200): \n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.transpose(1, 2)  # Transpose for LSTM [batch, seq_len, features]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        # Note: Need to transpose inputs for evaluation too\n",
    "        train_loss, train_accuracy = evaluate_model(train_loader, model, criterion, transpose=True)\n",
    "        val_loss, val_accuracy = evaluate_model(val_loader, model, criterion, transpose=True)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Log and print\n",
    "        log_message = (f\"Fold {fold+1} Epoch [{epoch+1}/200], \"\n",
    "                      f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
    "                      f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "        logging.info(log_message)\n",
    "        print(log_message)\n",
    "        \n",
    "        # Save best model based on validation accuracy\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()  # Save the model state\n",
    "            # Optional: Uncomment if you have a save_best_model function\n",
    "            # save_best_model(epoch, model, optimizer, val_loss, train_losses, val_losses)\n",
    "    \n",
    "    # After training, store the best results for this fold\n",
    "    fold_accuracies.append(best_val_accuracy)\n",
    "    fold_losses.append(best_val_loss)\n",
    "    print(f'Fold {fold + 1} Best Accuracy: {best_val_accuracy:.2f}%')\n",
    "    \n",
    "\n",
    "# Print summary\n",
    "mean_accuracy = np.mean(fold_accuracies)\n",
    "std_accuracy = np.std(fold_accuracies)\n",
    "print(f'\\n5-Fold CV Results:')\n",
    "print(f'Average Accuracy: {mean_accuracy:.2f}% (±{std_accuracy:.2f})')\n",
    "print(f'Fold Accuracies: {[f\"{acc:.2f}%\" for acc in fold_accuracies]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37f5a0a4-374a-40dd-9d27-81279b433394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSTM Results Summary:\n",
      " Chunk Size (s)  Accuracy (%)\n",
      "          0.250     56.060606\n",
      "          0.500     86.666667\n",
      "          1.000     66.000000\n",
      "          1.375     74.166667\n",
      "          2.000     76.666667\n",
      "          2.250     55.000000\n",
      "          2.500     73.333333\n",
      "          2.750     48.333333\n",
      "          3.000     33.333333\n",
      "          3.250     30.000000\n",
      "          3.500     70.000000\n",
      "          3.750     30.000000\n",
      "          4.000     43.333333\n",
      "          5.000     40.000000\n"
     ]
    }
   ],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(chunk_sizes, accuracies, 'bo-')\n",
    "plt.xlabel('Chunk Size (seconds)')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.title('LSTM Model Accuracy vs Chunk Size')\n",
    "plt.grid(True)\n",
    "\n",
    "# Add accuracy values as text\n",
    "for i, (x, y) in enumerate(zip(chunk_sizes, accuracies)):\n",
    "    plt.text(x, y, f'{y:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.savefig('lstm_chunk_size_vs_accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "# Print results table\n",
    "results = pd.DataFrame({\n",
    "    'Chunk Size (s)': chunk_sizes,\n",
    "    'Accuracy (%)': accuracies\n",
    "})\n",
    "print(\"\\nLSTM Results Summary:\")\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c79cb82-164f-4ed0-ba0b-63dcc6f6bbc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 3, got 237",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m---> 16\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m         _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     18\u001b[0m         all_predictions\u001b[38;5;241m.\u001b[39mextend(predicted\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[15], line 23\u001b[0m, in \u001b[0;36mOptimizedLSTMModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Pass through LSTM\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Use the output of the last timestep for classification\u001b[39;00m\n\u001b[0;32m     26\u001b[0m out \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Take the last hidden state\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:1119\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1116\u001b[0m             hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m   1117\u001b[0m         \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m         \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m-> 1119\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:1000\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_forward_args\u001b[39m(\n\u001b[0;32m    995\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    997\u001b[0m     hidden: Tuple[Tensor, Tensor],  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    998\u001b[0m     batch_sizes: Optional[Tensor],\n\u001b[0;32m    999\u001b[0m ):\n\u001b[1;32m-> 1000\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1001\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(\n\u001b[0;32m   1002\u001b[0m         hidden[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1003\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m   1004\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1005\u001b[0m     )\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(\n\u001b[0;32m   1007\u001b[0m         hidden[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   1008\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m   1009\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1010\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:312\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    310\u001b[0m     )\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 3, got 237"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Activities for the Confusion matrix\n",
    "ACTIVITIES = ['sit', 'walk','upstair']\n",
    "\n",
    "# Model evaluation (confusion matrix)\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Loop through the test loader to collect predictions and true labels\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Compute the confusion matrix, explicitly specifying the labels\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions, labels=[0, 1, 2]) \n",
    "\n",
    "# Assuming conf_matrix and ACTIVITIES are already defined\n",
    "class_accuracies = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)  # Compute per-class accuracy\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(conf_matrix, display_labels=ACTIVITIES)\n",
    "fig, ax = plt.subplots()\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "\n",
    "# Add per-class accuracy text\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "    acc_text = f\"{class_accuracies[i] * 100:.2f}%\"\n",
    "    ax.text(\n",
    "        len(ACTIVITIES) + 0.3, i, acc_text, \n",
    "        fontsize=12, verticalalignment='center', color='black'\n",
    "    )\n",
    "\n",
    "plt.title(\"Confusion Matrix for Activities with Per-Class Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f841cd-23c7-425c-a8c0-6e363b2c94f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label: 1, Predicted: 2, Confidence: 0.5379\n",
      "Full Probabilities: [0.04630952328443527, 0.40478968620300293, 0.5378606915473938, 0.0032725243363529444, 0.0037186944391578436, 0.0018561289180070162, 0.0015050942311063409, 0.0006875882390886545]\n",
      "True Label: 1, Predicted: 2, Confidence: 0.6603\n",
      "Full Probabilities: [0.012942157685756683, 0.3238711655139923, 0.6603409051895142, 0.0006900696316733956, 0.0012600854970514774, 0.00047482800437137485, 0.0003185397945344448, 0.00010225101141259074]\n",
      "True Label: 2, Predicted: 1, Confidence: 0.5808\n",
      "Full Probabilities: [0.002849593525752425, 0.5807549357414246, 0.4157315492630005, 0.00019912587595172226, 0.000295086792903021, 0.00010057010513264686, 5.421382957138121e-05, 1.4962002751417458e-05]\n",
      "True Label: 2, Predicted: 1, Confidence: 0.5663\n",
      "Full Probabilities: [0.004601947031915188, 0.5663013458251953, 0.42831793427467346, 0.0002489405160304159, 0.0003278449294157326, 0.00011758197069866583, 6.75682895234786e-05, 1.6763236999395303e-05]\n",
      "True Label: 2, Predicted: 1, Confidence: 0.5180\n",
      "Full Probabilities: [0.020252525806427002, 0.5179758667945862, 0.4608774185180664, 0.0002581731241662055, 0.00039337240741588175, 0.0001394125574734062, 8.091720519587398e-05, 2.237147418782115e-05]\n",
      "True Label: 0, Predicted: 1, Confidence: 0.7273\n",
      "Full Probabilities: [0.24202822148799896, 0.7272588014602661, 0.01800217106938362, 0.002684463746845722, 0.003459376050159335, 0.0026517054066061974, 0.0024689871352165937, 0.0014463680563494563]\n",
      "True Label: 1, Predicted: 2, Confidence: 0.5364\n",
      "Full Probabilities: [0.0010767356725409627, 0.461995929479599, 0.5364068150520325, 0.0001329303195234388, 0.00027212704299017787, 7.19023373676464e-05, 3.5536650102585554e-05, 8.04939008958172e-06]\n",
      "True Label: 1, Predicted: 2, Confidence: 0.6007\n",
      "Full Probabilities: [0.0015494574327021837, 0.39651644229888916, 0.6007441282272339, 0.000301126652630046, 0.0005974904634058475, 0.00017378314805682749, 9.326271538157016e-05, 2.4287490305141546e-05]\n",
      "True Label: 1, Predicted: 2, Confidence: 0.8370\n",
      "Full Probabilities: [0.0041362508200109005, 0.15798449516296387, 0.8370068073272705, 0.0001938036730280146, 0.00045545579632744193, 0.00012768110900651664, 7.822032057447359e-05, 1.728711140458472e-05]\n",
      "True Label: 1, Predicted: 2, Confidence: 0.7846\n",
      "Full Probabilities: [0.0027064322493970394, 0.2118576020002365, 0.7845824360847473, 0.00020613192464224994, 0.0004299695719964802, 0.0001244906452484429, 7.59389586164616e-05, 1.701954533928074e-05]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "incorrect_samples = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)  # Get raw logits\n",
    "        probabilities = F.softmax(outputs, dim=1)  # Convert to probabilities\n",
    "        predicted_labels = torch.argmax(probabilities, dim=1)  # Get predicted class\n",
    "        confidence_scores = torch.max(probabilities, dim=1).values  # Get max confidence\n",
    "\n",
    "        # Find incorrect predictions\n",
    "        incorrect_indices = (predicted_labels != batch_y)\n",
    "        for i in range(len(batch_y)):\n",
    "            if incorrect_indices[i]:\n",
    "                incorrect_samples.append({\n",
    "                    \"True Label\": batch_y[i].item(),\n",
    "                    \"Predicted Label\": predicted_labels[i].item(),\n",
    "                    \"Confidence\": confidence_scores[i].item(),\n",
    "                    \"Probabilities\": probabilities[i].tolist()\n",
    "                })\n",
    "\n",
    "# Print results\n",
    "for sample in incorrect_samples:\n",
    "    print(f\"True Label: {sample['True Label']}, Predicted: {sample['Predicted Label']}, Confidence: {sample['Confidence']:.4f}\")\n",
    "    print(f\"Full Probabilities: {sample['Probabilities']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
