{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "30e998e0-f6cd-4d3d-82f0-08637032af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from firebase_admin import credentials, firestore\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Firebase Initialization\n",
    "cred = credentials.Certificate(\"adminkey.json\")\n",
    "#firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename='training_log.txt', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "INCLUDE_ONLY = ['Stephen', 'Lillian', 'Ren', 'Yuanheng', 'Ethan Shao']\n",
    "ACTIVITIES = ['sit','walk','upstair']\n",
    "CHUNK_SIZE = 2.375  # in seconds (can be a decimal)\n",
    "SAMPLING_RATE = 100  # Hz\n",
    "NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f933bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "OVERLAP = 0.25  # Fix for previous NameError\n",
    "\n",
    "# Define feature extraction function\n",
    "def extract_features(chunk):\n",
    "    \"\"\"Extract features from a chunked acceleration segment with selected statistics.\"\"\"\n",
    "    feature_vector = []\n",
    "    \n",
    "    for axis in ['x', 'y', 'z']:\n",
    "        data_series = pd.Series(chunk[axis])\n",
    "        # Apply smoothing\n",
    "        smoothed_data = data_series.rolling(window=5, min_periods=1).mean()\n",
    "        feature_vector.extend([\n",
    "            smoothed_data.mean(),                  # Mean\n",
    "            smoothed_data.median(),                # Median\n",
    "            smoothed_data.std(),                   # Standard deviation\n",
    "            smoothed_data.var(),                   # Variance\n",
    "            smoothed_data.min(),                   # Minimum\n",
    "            smoothed_data.max(),                   # Maximum\n",
    "        ])\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287ad27b-8007-4ccd-b113-f8016c2aaec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data \n",
    "def fetch_data_for_stephen(collection_name, activities, time_start=500, time_end=6000):\n",
    "    data, docs = [], []\n",
    "    person_name = \"Stephen\"\n",
    "    \n",
    "    for activity in activities:\n",
    "        for recording in db.collection(collection_name).document(person_name).collection(activity).stream():\n",
    "            record = recording.to_dict()\n",
    "            if 'acceleration' not in record:\n",
    "                continue\n",
    "\n",
    "            docs.append(record)\n",
    "            df = pd.DataFrame(record['acceleration'])\n",
    "            \n",
    "            if 'time' in df.columns:\n",
    "                filtered_df = df[(df['time'] >= time_start) & (df['time'] <= time_end)]\n",
    "                data.append(filtered_df)\n",
    "            else:\n",
    "                raise ValueError(\"The 'acceleration' field must include a 'time' column.\")\n",
    "    return data, docs\n",
    "\n",
    "# Fetch data\n",
    "training_data_raw, training_docs = fetch_data_for_stephen(\"training\", ACTIVITIES)\n",
    "testing_data_raw, testing_docs = fetch_data_for_stephen(\"testing\", ACTIVITIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1284a61-2522-4348-abbf-1d4e31e36bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chunking with overlap\n",
    "def chunk_data_with_overlap(data_raw, docs, chunk_size, activities, sampling_rate, overlap=0.5):\n",
    "    \"\"\"Chunk raw acceleration data into smaller labeled segments using overlapping windows.\"\"\"\n",
    "    data, labels = [], []\n",
    "    chunk_samples = int(chunk_size * sampling_rate)\n",
    "    step = int(chunk_samples * (1 - overlap))  # Compute step size based on overlap\n",
    "\n",
    "    for i, df in enumerate(data_raw):\n",
    "        # Slide over the data with the defined step\n",
    "        for start in range(0, len(df) - chunk_samples + 1, step):\n",
    "            end = start + chunk_samples     \n",
    "            chunk = df.iloc[start:end]\n",
    "            activity = docs[i]['activity']\n",
    "            label = activities.index(activity)\n",
    "            data.append(extract_features(chunk))\n",
    "            labels.append(label)\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "43de7965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset summary:\n",
      "+----------+------------------+\n",
      "| Dataset  | number of chunks |\n",
      "+----------+------------------+\n",
      "| training |       150        |\n",
      "| testing  |        60        |\n",
      "+----------+------------------+\n",
      "Training Activities Count\n",
      "sit: 20 chunks\n",
      "walk: 20 chunks\n",
      "upstair: 20 chunks\n",
      "\n",
      "Testing Activity Count\n",
      "sit:20 chunks\n",
      "walk:20 chunks\n",
      "upstair:20 chunks\n",
      "150\n",
      "60\n",
      "(150, 3, 100)\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate #for table formatting\n",
    "\n",
    "#Calculate the number of training and testing samples\n",
    "num_training_samples = len(training_data)\n",
    "num_testing_samples = len(testing_data)\n",
    "\n",
    "#table\n",
    "summary_table = [[\"training\", num_training_samples], [\"testing\", num_testing_samples]]\n",
    "\n",
    "#print\n",
    "print(\"dataset summary:\")\n",
    "print(tabulate(summary_table, headers = [\"Dataset\", \"number of chunks\"], tablefmt=\"pretty\"))\n",
    "\n",
    "print(\"Training Activities Count\")\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "    print(f\"{activity}: {int(training_distribution[i])} chunks\")\n",
    "\n",
    "print(\"\\nTesting Activity Count\")\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "    print(f\"{activity}:{int(testing_distribution[i])} chunks\")\n",
    "print(len(training_data))\n",
    "print(len(testing_data))\n",
    "print(np.array(training_data).shape)\n",
    "print(len(training_data_raw))\n",
    "print(len(testing_data_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae1da3e-3e2c-40ad-82cb-6d090f2cc7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features using chunk_data_with_overlap\n",
    "X_train, y_train = chunk_data_with_overlap(training_data_raw, training_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE, OVERLAP)\n",
    "X_test, y_test = chunk_data_with_overlap(testing_data_raw, testing_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE, OVERLAP)\n",
    "\n",
    "# Combine train and test for cross-validation (Stephen's data only)\n",
    "X_all = np.concatenate([X_train, X_test], axis=0)\n",
    "y_all = np.concatenate([y_train, y_test], axis=0)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_all = scaler.fit_transform(X_all)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_all = torch.tensor(X_all, dtype=torch.float32)  # Shape: (n_samples, 18)\n",
    "y_all = torch.tensor(y_all, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec68f8d-85ff-48e1-b219-a547e9393c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for extracted features (MLP)\n",
    "class FeatureModel(nn.Module):\n",
    "    def __init__(self, num_classes, input_features=18):\n",
    "        super(FeatureModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_features, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fea2d82-2721-4226-87c8-d0386cb60e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save best model and metadata\n",
    "BEST_MODEL_PATH = \"best_model.pth\"\n",
    "BEST_METADATA_PATH = \"best_model.json\"\n",
    "\n",
    "# Corrected save_best_model function\n",
    "def save_best_model(epoch, model, optimizer, loss, accuracy, train_losses, val_losses):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "    }\n",
    "    torch.save(checkpoint, BEST_MODEL_PATH)\n",
    "\n",
    "    # Save metadata with correct variable names\n",
    "    with open(BEST_METADATA_PATH, \"w\") as f:\n",
    "        json.dump({\"epoch\": epoch, \"val_loss\": loss, \"val_accuracy\": accuracy}, f)\n",
    "\n",
    "# Function to load best model if exists\n",
    "def load_best_model(model, optimizer, best_model_path=BEST_MODEL_PATH):\n",
    "    if os.path.exists(best_model_path):\n",
    "        checkpoint = torch.load(best_model_path)\n",
    "        \n",
    "        # Print checkpoint keys to understand its structure\n",
    "        print(\"Checkpoint keys:\", checkpoint.keys())\n",
    "        \n",
    "        # Load model weights\n",
    "        model_state_dict = model.state_dict()\n",
    "        checkpoint_state_dict = checkpoint['model_state_dict']\n",
    "        \n",
    "        # Filter out incompatible keys\n",
    "        filtered_checkpoint_state_dict = {k: v for k, v in checkpoint_state_dict.items() if k in model_state_dict}\n",
    "        model_state_dict.update(filtered_checkpoint_state_dict)\n",
    "        model.load_state_dict(model_state_dict)\n",
    "        \n",
    "        # Load other metadata if available\n",
    "        start_epoch = checkpoint.get('epoch', 0)\n",
    "        best_loss = checkpoint.get('loss', float('inf'))  # Default to infinity if loss is missing\n",
    "        best_avg_accuracy = checkpoint.get('accuracy', 0)  # Default to 0 if accuracy is missing\n",
    "        \n",
    "        # If train_losses and test_losses aren't saved, return empty lists or placeholders\n",
    "        train_losses = checkpoint.get('train_losses', [])\n",
    "        test_losses = checkpoint.get('test_losses', [])\n",
    "        \n",
    "        return start_epoch, best_loss, best_avg_accuracy, train_losses, test_losses\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting from scratch.\")\n",
    "        # Return default values\n",
    "        return 0, float('inf'), 0, [], []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "afb6cc02-f6d5-441e-8ea1-736efc276305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model (no transpose for MLP)\n",
    "def evaluate_model(loader, model, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "01254ebf-d89d-4173-9f04-ba08155090cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Data Augmentation Functions\n",
    "def add_jitter(batch_X, noise_level=0.01):\n",
    "    noise = torch.randn_like(batch_X) * noise_level\n",
    "    return batch_X + noise\n",
    "\n",
    "def scale_signal(batch_X, scale_range=(0.9, 1.1)):\n",
    "    scale_factor = torch.FloatTensor(batch_X.shape[0], 1, 1).uniform_(*scale_range)\n",
    "    return batch_X * scale_factor\n",
    "\n",
    "def time_warp(batch_X, sigma=0.2):\n",
    "    batch_size, num_channels, seq_length = batch_X.shape\n",
    "    time_steps = np.arange(seq_length)\n",
    "    warping_curve = np.cumsum(np.random.normal(0, sigma, size=(batch_size, seq_length)), axis=1)\n",
    "    warping_curve = (warping_curve - warping_curve.min(axis=1, keepdims=True)) / \\\n",
    "                    (warping_curve.max(axis=1, keepdims=True) - warping_curve.min(axis=1, keepdims=True)) * seq_length\n",
    "    warped_X = torch.zeros_like(batch_X)\n",
    "    for i in range(batch_size):\n",
    "        for j in range(num_channels):\n",
    "            f = interp1d(time_steps, batch_X[i, j, :].cpu().numpy(), kind='linear', fill_value='extrapolate')\n",
    "            warped_X[i, j, :] = torch.tensor(f(warping_curve[i]), dtype=batch_X.dtype)\n",
    "    return warped_X\n",
    "\n",
    "def random_crop(batch_X, crop_size=0.9, target_length=None):\n",
    "    batch_size, num_channels, seq_length = batch_X.shape\n",
    "    new_length = int(seq_length * crop_size)\n",
    "    start_idx = torch.randint(0, seq_length - new_length + 1, (batch_size,))\n",
    "    cropped_X = torch.zeros(batch_size, num_channels, new_length)\n",
    "    for i in range(batch_size):\n",
    "        cropped_X[i] = batch_X[i, :, start_idx[i]:start_idx[i] + new_length]\n",
    "    \n",
    "    if target_length is not None:\n",
    "        if new_length < target_length:\n",
    "            padding = torch.zeros(batch_size, num_channels, target_length - new_length)\n",
    "            cropped_X = torch.cat([cropped_X, padding], dim=2)\n",
    "        elif new_length > target_length:\n",
    "            cropped_X = cropped_X[:, :, :target_length]\n",
    "    return cropped_X\n",
    "\n",
    "def permute_segments(batch_X, num_segments=5):\n",
    "    batch_size, num_channels, seq_length = batch_X.shape\n",
    "    segment_length = seq_length // num_segments\n",
    "    permuted_X = batch_X.clone()\n",
    "    for i in range(batch_size):\n",
    "        perm = torch.randperm(num_segments)\n",
    "        for j in range(num_segments):\n",
    "            permuted_X[i, :, j * segment_length:(j + 1) * segment_length] = \\\n",
    "                batch_X[i, :, perm[j] * segment_length:(perm[j] + 1) * segment_length]\n",
    "    return permuted_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fc68f1f4-225f-455c-b6d0-80a76a3ce598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint keys: dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'test_loss', 'train_losses', 'test_losses'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minsu\\AppData\\Local\\Temp\\ipykernel_21472\\2561067316.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(best_model_path)\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, optimizer, and loss function\n",
    "model = OptimizedLSTMModel(num_classes=NUM_CLASSES, input_channels=3, seq_length=int(CHUNK_SIZE * 100))\n",
    "optimizer = Adam(model.parameters(), lr=0.02, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50, min_lr=1e-5)\n",
    "\n",
    "# Load best model if exists\n",
    "start_epoch, best_loss, best_avg_accuracy, train_losses, test_losses = load_best_model(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fd628c08-376b-468a-ab5f-6c1a88c7587c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Fold 1 Epoch [1/100], Train Loss: 1.0559, Train Acc: 39.29%, Val Loss: 1.1288, Val Acc: 12.50%\n",
      "Fold 1 Epoch [2/100], Train Loss: 1.0357, Train Acc: 39.29%, Val Loss: 1.1207, Val Acc: 12.50%\n",
      "Fold 1 Epoch [3/100], Train Loss: 1.0156, Train Acc: 71.43%, Val Loss: 1.1121, Val Acc: 12.50%\n",
      "Fold 1 Epoch [4/100], Train Loss: 0.9951, Train Acc: 78.57%, Val Loss: 1.1029, Val Acc: 25.00%\n",
      "Fold 1 Epoch [5/100], Train Loss: 0.9745, Train Acc: 78.57%, Val Loss: 1.0942, Val Acc: 25.00%\n",
      "Fold 1 Epoch [6/100], Train Loss: 0.9536, Train Acc: 78.57%, Val Loss: 1.0854, Val Acc: 25.00%\n",
      "Fold 1 Epoch [7/100], Train Loss: 0.9323, Train Acc: 78.57%, Val Loss: 1.0766, Val Acc: 25.00%\n",
      "Fold 1 Epoch [8/100], Train Loss: 0.9106, Train Acc: 82.14%, Val Loss: 1.0679, Val Acc: 25.00%\n",
      "Fold 1 Epoch [9/100], Train Loss: 0.8888, Train Acc: 82.14%, Val Loss: 1.0590, Val Acc: 25.00%\n",
      "Fold 1 Epoch [10/100], Train Loss: 0.8664, Train Acc: 85.71%, Val Loss: 1.0498, Val Acc: 37.50%\n",
      "Fold 1 Epoch [11/100], Train Loss: 0.8436, Train Acc: 89.29%, Val Loss: 1.0408, Val Acc: 37.50%\n",
      "Fold 1 Epoch [12/100], Train Loss: 0.8206, Train Acc: 89.29%, Val Loss: 1.0319, Val Acc: 37.50%\n",
      "Fold 1 Epoch [13/100], Train Loss: 0.7974, Train Acc: 89.29%, Val Loss: 1.0231, Val Acc: 37.50%\n",
      "Fold 1 Epoch [14/100], Train Loss: 0.7743, Train Acc: 89.29%, Val Loss: 1.0150, Val Acc: 37.50%\n",
      "Fold 1 Epoch [15/100], Train Loss: 0.7513, Train Acc: 89.29%, Val Loss: 1.0070, Val Acc: 37.50%\n",
      "Fold 1 Epoch [16/100], Train Loss: 0.7283, Train Acc: 89.29%, Val Loss: 0.9989, Val Acc: 37.50%\n",
      "Fold 1 Epoch [17/100], Train Loss: 0.7046, Train Acc: 92.86%, Val Loss: 0.9906, Val Acc: 37.50%\n",
      "Fold 1 Epoch [18/100], Train Loss: 0.6805, Train Acc: 92.86%, Val Loss: 0.9826, Val Acc: 37.50%\n",
      "Fold 1 Epoch [19/100], Train Loss: 0.6561, Train Acc: 92.86%, Val Loss: 0.9749, Val Acc: 37.50%\n",
      "Fold 1 Epoch [20/100], Train Loss: 0.6312, Train Acc: 92.86%, Val Loss: 0.9671, Val Acc: 37.50%\n",
      "Fold 1 Epoch [21/100], Train Loss: 0.6062, Train Acc: 96.43%, Val Loss: 0.9594, Val Acc: 37.50%\n",
      "Fold 1 Epoch [22/100], Train Loss: 0.5813, Train Acc: 96.43%, Val Loss: 0.9520, Val Acc: 37.50%\n",
      "Fold 1 Epoch [23/100], Train Loss: 0.5563, Train Acc: 96.43%, Val Loss: 0.9446, Val Acc: 37.50%\n",
      "Fold 1 Epoch [24/100], Train Loss: 0.5317, Train Acc: 96.43%, Val Loss: 0.9380, Val Acc: 37.50%\n",
      "Fold 1 Epoch [25/100], Train Loss: 0.5075, Train Acc: 96.43%, Val Loss: 0.9309, Val Acc: 37.50%\n",
      "Fold 1 Epoch [26/100], Train Loss: 0.4838, Train Acc: 96.43%, Val Loss: 0.9238, Val Acc: 37.50%\n",
      "Fold 1 Epoch [27/100], Train Loss: 0.4609, Train Acc: 96.43%, Val Loss: 0.9170, Val Acc: 50.00%\n",
      "Fold 1 Epoch [28/100], Train Loss: 0.4387, Train Acc: 96.43%, Val Loss: 0.9108, Val Acc: 50.00%\n",
      "Fold 1 Epoch [29/100], Train Loss: 0.4173, Train Acc: 96.43%, Val Loss: 0.9047, Val Acc: 50.00%\n",
      "Fold 1 Epoch [30/100], Train Loss: 0.3967, Train Acc: 96.43%, Val Loss: 0.8986, Val Acc: 50.00%\n",
      "Fold 1 Epoch [31/100], Train Loss: 0.3771, Train Acc: 96.43%, Val Loss: 0.8933, Val Acc: 50.00%\n",
      "Fold 1 Epoch [32/100], Train Loss: 0.3583, Train Acc: 96.43%, Val Loss: 0.8885, Val Acc: 50.00%\n",
      "Fold 1 Epoch [33/100], Train Loss: 0.3404, Train Acc: 96.43%, Val Loss: 0.8840, Val Acc: 50.00%\n",
      "Fold 1 Epoch [34/100], Train Loss: 0.3235, Train Acc: 96.43%, Val Loss: 0.8798, Val Acc: 50.00%\n",
      "Fold 1 Epoch [35/100], Train Loss: 0.3076, Train Acc: 96.43%, Val Loss: 0.8752, Val Acc: 50.00%\n",
      "Fold 1 Epoch [36/100], Train Loss: 0.2926, Train Acc: 96.43%, Val Loss: 0.8697, Val Acc: 62.50%\n",
      "Fold 1 Epoch [37/100], Train Loss: 0.2786, Train Acc: 96.43%, Val Loss: 0.8641, Val Acc: 62.50%\n",
      "Fold 1 Epoch [38/100], Train Loss: 0.2653, Train Acc: 96.43%, Val Loss: 0.8582, Val Acc: 62.50%\n",
      "Fold 1 Epoch [39/100], Train Loss: 0.2529, Train Acc: 96.43%, Val Loss: 0.8527, Val Acc: 62.50%\n",
      "Fold 1 Epoch [40/100], Train Loss: 0.2412, Train Acc: 96.43%, Val Loss: 0.8471, Val Acc: 62.50%\n",
      "Fold 1 Epoch [41/100], Train Loss: 0.2302, Train Acc: 96.43%, Val Loss: 0.8411, Val Acc: 62.50%\n",
      "Fold 1 Epoch [42/100], Train Loss: 0.2197, Train Acc: 96.43%, Val Loss: 0.8344, Val Acc: 62.50%\n",
      "Fold 1 Epoch [43/100], Train Loss: 0.2098, Train Acc: 96.43%, Val Loss: 0.8269, Val Acc: 62.50%\n",
      "Fold 1 Epoch [44/100], Train Loss: 0.2005, Train Acc: 96.43%, Val Loss: 0.8192, Val Acc: 62.50%\n",
      "Fold 1 Epoch [45/100], Train Loss: 0.1916, Train Acc: 100.00%, Val Loss: 0.8103, Val Acc: 62.50%\n",
      "Fold 1 Epoch [46/100], Train Loss: 0.1830, Train Acc: 100.00%, Val Loss: 0.8003, Val Acc: 62.50%\n",
      "Fold 1 Epoch [47/100], Train Loss: 0.1749, Train Acc: 100.00%, Val Loss: 0.7907, Val Acc: 62.50%\n",
      "Fold 1 Epoch [48/100], Train Loss: 0.1671, Train Acc: 100.00%, Val Loss: 0.7795, Val Acc: 62.50%\n",
      "Fold 1 Epoch [49/100], Train Loss: 0.1596, Train Acc: 100.00%, Val Loss: 0.7661, Val Acc: 62.50%\n",
      "Fold 1 Epoch [50/100], Train Loss: 0.1524, Train Acc: 100.00%, Val Loss: 0.7515, Val Acc: 62.50%\n",
      "Fold 1 Epoch [51/100], Train Loss: 0.1454, Train Acc: 100.00%, Val Loss: 0.7360, Val Acc: 62.50%\n",
      "Fold 1 Epoch [52/100], Train Loss: 0.1386, Train Acc: 100.00%, Val Loss: 0.7203, Val Acc: 62.50%\n",
      "Fold 1 Epoch [53/100], Train Loss: 0.1320, Train Acc: 100.00%, Val Loss: 0.7041, Val Acc: 62.50%\n",
      "Fold 1 Epoch [54/100], Train Loss: 0.1256, Train Acc: 100.00%, Val Loss: 0.6878, Val Acc: 75.00%\n",
      "Fold 1 Epoch [55/100], Train Loss: 0.1195, Train Acc: 100.00%, Val Loss: 0.6714, Val Acc: 75.00%\n",
      "Fold 1 Epoch [56/100], Train Loss: 0.1135, Train Acc: 100.00%, Val Loss: 0.6540, Val Acc: 75.00%\n",
      "Fold 1 Epoch [57/100], Train Loss: 0.1078, Train Acc: 100.00%, Val Loss: 0.6366, Val Acc: 75.00%\n",
      "Fold 1 Epoch [58/100], Train Loss: 0.1023, Train Acc: 100.00%, Val Loss: 0.6203, Val Acc: 75.00%\n",
      "Fold 1 Epoch [59/100], Train Loss: 0.0969, Train Acc: 100.00%, Val Loss: 0.6039, Val Acc: 75.00%\n",
      "Fold 1 Epoch [60/100], Train Loss: 0.0918, Train Acc: 100.00%, Val Loss: 0.5880, Val Acc: 75.00%\n",
      "Fold 1 Epoch [61/100], Train Loss: 0.0869, Train Acc: 100.00%, Val Loss: 0.5723, Val Acc: 75.00%\n",
      "Fold 1 Epoch [62/100], Train Loss: 0.0822, Train Acc: 100.00%, Val Loss: 0.5571, Val Acc: 75.00%\n",
      "Fold 1 Epoch [63/100], Train Loss: 0.0777, Train Acc: 100.00%, Val Loss: 0.5422, Val Acc: 75.00%\n",
      "Fold 1 Epoch [64/100], Train Loss: 0.0734, Train Acc: 100.00%, Val Loss: 0.5271, Val Acc: 75.00%\n",
      "Fold 1 Epoch [65/100], Train Loss: 0.0693, Train Acc: 100.00%, Val Loss: 0.5137, Val Acc: 75.00%\n",
      "Fold 1 Epoch [66/100], Train Loss: 0.0655, Train Acc: 100.00%, Val Loss: 0.5013, Val Acc: 87.50%\n",
      "Fold 1 Epoch [67/100], Train Loss: 0.0619, Train Acc: 100.00%, Val Loss: 0.4892, Val Acc: 87.50%\n",
      "Fold 1 Epoch [68/100], Train Loss: 0.0585, Train Acc: 100.00%, Val Loss: 0.4788, Val Acc: 87.50%\n",
      "Fold 1 Epoch [69/100], Train Loss: 0.0553, Train Acc: 100.00%, Val Loss: 0.4685, Val Acc: 87.50%\n",
      "Fold 1 Epoch [70/100], Train Loss: 0.0522, Train Acc: 100.00%, Val Loss: 0.4585, Val Acc: 87.50%\n",
      "Fold 1 Epoch [71/100], Train Loss: 0.0493, Train Acc: 100.00%, Val Loss: 0.4490, Val Acc: 87.50%\n",
      "Fold 1 Epoch [72/100], Train Loss: 0.0465, Train Acc: 100.00%, Val Loss: 0.4400, Val Acc: 87.50%\n",
      "Fold 1 Epoch [73/100], Train Loss: 0.0439, Train Acc: 100.00%, Val Loss: 0.4316, Val Acc: 87.50%\n",
      "Fold 1 Epoch [74/100], Train Loss: 0.0414, Train Acc: 100.00%, Val Loss: 0.4241, Val Acc: 87.50%\n",
      "Fold 1 Epoch [75/100], Train Loss: 0.0390, Train Acc: 100.00%, Val Loss: 0.4169, Val Acc: 87.50%\n",
      "Fold 1 Epoch [76/100], Train Loss: 0.0368, Train Acc: 100.00%, Val Loss: 0.4099, Val Acc: 87.50%\n",
      "Fold 1 Epoch [77/100], Train Loss: 0.0347, Train Acc: 100.00%, Val Loss: 0.4024, Val Acc: 87.50%\n",
      "Fold 1 Epoch [78/100], Train Loss: 0.0327, Train Acc: 100.00%, Val Loss: 0.3945, Val Acc: 87.50%\n",
      "Fold 1 Epoch [79/100], Train Loss: 0.0308, Train Acc: 100.00%, Val Loss: 0.3862, Val Acc: 87.50%\n",
      "Fold 1 Epoch [80/100], Train Loss: 0.0290, Train Acc: 100.00%, Val Loss: 0.3782, Val Acc: 87.50%\n",
      "Fold 1 Epoch [81/100], Train Loss: 0.0273, Train Acc: 100.00%, Val Loss: 0.3702, Val Acc: 87.50%\n",
      "Fold 1 Epoch [82/100], Train Loss: 0.0258, Train Acc: 100.00%, Val Loss: 0.3629, Val Acc: 87.50%\n",
      "Fold 1 Epoch [83/100], Train Loss: 0.0244, Train Acc: 100.00%, Val Loss: 0.3566, Val Acc: 87.50%\n",
      "Fold 1 Epoch [84/100], Train Loss: 0.0230, Train Acc: 100.00%, Val Loss: 0.3501, Val Acc: 87.50%\n",
      "Fold 1 Epoch [85/100], Train Loss: 0.0218, Train Acc: 100.00%, Val Loss: 0.3447, Val Acc: 87.50%\n",
      "Fold 1 Epoch [86/100], Train Loss: 0.0206, Train Acc: 100.00%, Val Loss: 0.3388, Val Acc: 87.50%\n",
      "Fold 1 Epoch [87/100], Train Loss: 0.0195, Train Acc: 100.00%, Val Loss: 0.3334, Val Acc: 87.50%\n",
      "Fold 1 Epoch [88/100], Train Loss: 0.0184, Train Acc: 100.00%, Val Loss: 0.3284, Val Acc: 87.50%\n",
      "Fold 1 Epoch [89/100], Train Loss: 0.0175, Train Acc: 100.00%, Val Loss: 0.3237, Val Acc: 87.50%\n",
      "Fold 1 Epoch [90/100], Train Loss: 0.0166, Train Acc: 100.00%, Val Loss: 0.3197, Val Acc: 87.50%\n",
      "Fold 1 Epoch [91/100], Train Loss: 0.0158, Train Acc: 100.00%, Val Loss: 0.3162, Val Acc: 87.50%\n",
      "Fold 1 Epoch [92/100], Train Loss: 0.0150, Train Acc: 100.00%, Val Loss: 0.3129, Val Acc: 87.50%\n",
      "Fold 1 Epoch [93/100], Train Loss: 0.0143, Train Acc: 100.00%, Val Loss: 0.3093, Val Acc: 87.50%\n",
      "Fold 1 Epoch [94/100], Train Loss: 0.0136, Train Acc: 100.00%, Val Loss: 0.3058, Val Acc: 87.50%\n",
      "Fold 1 Epoch [95/100], Train Loss: 0.0129, Train Acc: 100.00%, Val Loss: 0.3028, Val Acc: 87.50%\n",
      "Fold 1 Epoch [96/100], Train Loss: 0.0123, Train Acc: 100.00%, Val Loss: 0.3003, Val Acc: 87.50%\n",
      "Fold 1 Epoch [97/100], Train Loss: 0.0118, Train Acc: 100.00%, Val Loss: 0.2979, Val Acc: 87.50%\n",
      "Fold 1 Epoch [98/100], Train Loss: 0.0113, Train Acc: 100.00%, Val Loss: 0.2946, Val Acc: 87.50%\n",
      "Fold 1 Epoch [99/100], Train Loss: 0.0108, Train Acc: 100.00%, Val Loss: 0.2913, Val Acc: 87.50%\n",
      "Fold 1 Epoch [100/100], Train Loss: 0.0103, Train Acc: 100.00%, Val Loss: 0.2877, Val Acc: 87.50%\n",
      "Fold 1 Best Accuracy: 87.50%\n",
      "Fold 2/5\n",
      "Fold 2 Epoch [1/100], Train Loss: 1.0904, Train Acc: 55.17%, Val Loss: 1.0842, Val Acc: 42.86%\n",
      "Fold 2 Epoch [2/100], Train Loss: 1.0675, Train Acc: 65.52%, Val Loss: 1.0617, Val Acc: 57.14%\n",
      "Fold 2 Epoch [3/100], Train Loss: 1.0450, Train Acc: 65.52%, Val Loss: 1.0396, Val Acc: 71.43%\n",
      "Fold 2 Epoch [4/100], Train Loss: 1.0234, Train Acc: 65.52%, Val Loss: 1.0180, Val Acc: 71.43%\n",
      "Fold 2 Epoch [5/100], Train Loss: 1.0018, Train Acc: 65.52%, Val Loss: 0.9963, Val Acc: 71.43%\n",
      "Fold 2 Epoch [6/100], Train Loss: 0.9792, Train Acc: 65.52%, Val Loss: 0.9741, Val Acc: 71.43%\n",
      "Fold 2 Epoch [7/100], Train Loss: 0.9571, Train Acc: 65.52%, Val Loss: 0.9523, Val Acc: 71.43%\n",
      "Fold 2 Epoch [8/100], Train Loss: 0.9358, Train Acc: 75.86%, Val Loss: 0.9311, Val Acc: 71.43%\n",
      "Fold 2 Epoch [9/100], Train Loss: 0.9148, Train Acc: 89.66%, Val Loss: 0.9097, Val Acc: 85.71%\n",
      "Fold 2 Epoch [10/100], Train Loss: 0.8943, Train Acc: 100.00%, Val Loss: 0.8889, Val Acc: 100.00%\n",
      "Fold 2 Epoch [11/100], Train Loss: 0.8745, Train Acc: 100.00%, Val Loss: 0.8686, Val Acc: 100.00%\n",
      "Fold 2 Epoch [12/100], Train Loss: 0.8547, Train Acc: 100.00%, Val Loss: 0.8485, Val Acc: 100.00%\n",
      "Fold 2 Epoch [13/100], Train Loss: 0.8345, Train Acc: 100.00%, Val Loss: 0.8281, Val Acc: 100.00%\n",
      "Fold 2 Epoch [14/100], Train Loss: 0.8142, Train Acc: 100.00%, Val Loss: 0.8075, Val Acc: 100.00%\n",
      "Fold 2 Epoch [15/100], Train Loss: 0.7937, Train Acc: 100.00%, Val Loss: 0.7866, Val Acc: 100.00%\n",
      "Fold 2 Epoch [16/100], Train Loss: 0.7730, Train Acc: 100.00%, Val Loss: 0.7653, Val Acc: 100.00%\n",
      "Fold 2 Epoch [17/100], Train Loss: 0.7519, Train Acc: 100.00%, Val Loss: 0.7435, Val Acc: 100.00%\n",
      "Fold 2 Epoch [18/100], Train Loss: 0.7305, Train Acc: 100.00%, Val Loss: 0.7216, Val Acc: 100.00%\n",
      "Fold 2 Epoch [19/100], Train Loss: 0.7089, Train Acc: 100.00%, Val Loss: 0.6997, Val Acc: 100.00%\n",
      "Fold 2 Epoch [20/100], Train Loss: 0.6871, Train Acc: 100.00%, Val Loss: 0.6778, Val Acc: 100.00%\n",
      "Fold 2 Epoch [21/100], Train Loss: 0.6653, Train Acc: 100.00%, Val Loss: 0.6557, Val Acc: 100.00%\n",
      "Fold 2 Epoch [22/100], Train Loss: 0.6433, Train Acc: 100.00%, Val Loss: 0.6336, Val Acc: 100.00%\n",
      "Fold 2 Epoch [23/100], Train Loss: 0.6214, Train Acc: 100.00%, Val Loss: 0.6114, Val Acc: 100.00%\n",
      "Fold 2 Epoch [24/100], Train Loss: 0.5993, Train Acc: 100.00%, Val Loss: 0.5893, Val Acc: 100.00%\n",
      "Fold 2 Epoch [25/100], Train Loss: 0.5772, Train Acc: 100.00%, Val Loss: 0.5672, Val Acc: 100.00%\n",
      "Fold 2 Epoch [26/100], Train Loss: 0.5554, Train Acc: 100.00%, Val Loss: 0.5455, Val Acc: 100.00%\n",
      "Fold 2 Epoch [27/100], Train Loss: 0.5338, Train Acc: 100.00%, Val Loss: 0.5239, Val Acc: 100.00%\n",
      "Fold 2 Epoch [28/100], Train Loss: 0.5124, Train Acc: 100.00%, Val Loss: 0.5026, Val Acc: 100.00%\n",
      "Fold 2 Epoch [29/100], Train Loss: 0.4911, Train Acc: 100.00%, Val Loss: 0.4813, Val Acc: 100.00%\n",
      "Fold 2 Epoch [30/100], Train Loss: 0.4698, Train Acc: 100.00%, Val Loss: 0.4601, Val Acc: 100.00%\n",
      "Fold 2 Epoch [31/100], Train Loss: 0.4486, Train Acc: 100.00%, Val Loss: 0.4393, Val Acc: 100.00%\n",
      "Fold 2 Epoch [32/100], Train Loss: 0.4278, Train Acc: 100.00%, Val Loss: 0.4190, Val Acc: 100.00%\n",
      "Fold 2 Epoch [33/100], Train Loss: 0.4074, Train Acc: 100.00%, Val Loss: 0.3990, Val Acc: 100.00%\n",
      "Fold 2 Epoch [34/100], Train Loss: 0.3874, Train Acc: 100.00%, Val Loss: 0.3795, Val Acc: 100.00%\n",
      "Fold 2 Epoch [35/100], Train Loss: 0.3678, Train Acc: 100.00%, Val Loss: 0.3604, Val Acc: 100.00%\n",
      "Fold 2 Epoch [36/100], Train Loss: 0.3486, Train Acc: 100.00%, Val Loss: 0.3416, Val Acc: 100.00%\n",
      "Fold 2 Epoch [37/100], Train Loss: 0.3299, Train Acc: 100.00%, Val Loss: 0.3235, Val Acc: 100.00%\n",
      "Fold 2 Epoch [38/100], Train Loss: 0.3117, Train Acc: 100.00%, Val Loss: 0.3059, Val Acc: 100.00%\n",
      "Fold 2 Epoch [39/100], Train Loss: 0.2941, Train Acc: 100.00%, Val Loss: 0.2889, Val Acc: 100.00%\n",
      "Fold 2 Epoch [40/100], Train Loss: 0.2770, Train Acc: 100.00%, Val Loss: 0.2725, Val Acc: 100.00%\n",
      "Fold 2 Epoch [41/100], Train Loss: 0.2605, Train Acc: 100.00%, Val Loss: 0.2565, Val Acc: 100.00%\n",
      "Fold 2 Epoch [42/100], Train Loss: 0.2445, Train Acc: 100.00%, Val Loss: 0.2412, Val Acc: 100.00%\n",
      "Fold 2 Epoch [43/100], Train Loss: 0.2294, Train Acc: 100.00%, Val Loss: 0.2267, Val Acc: 100.00%\n",
      "Fold 2 Epoch [44/100], Train Loss: 0.2150, Train Acc: 100.00%, Val Loss: 0.2131, Val Acc: 100.00%\n",
      "Fold 2 Epoch [45/100], Train Loss: 0.2013, Train Acc: 100.00%, Val Loss: 0.2000, Val Acc: 100.00%\n",
      "Fold 2 Epoch [46/100], Train Loss: 0.1883, Train Acc: 100.00%, Val Loss: 0.1875, Val Acc: 100.00%\n",
      "Fold 2 Epoch [47/100], Train Loss: 0.1760, Train Acc: 100.00%, Val Loss: 0.1757, Val Acc: 100.00%\n",
      "Fold 2 Epoch [48/100], Train Loss: 0.1644, Train Acc: 100.00%, Val Loss: 0.1644, Val Acc: 100.00%\n",
      "Fold 2 Epoch [49/100], Train Loss: 0.1534, Train Acc: 100.00%, Val Loss: 0.1537, Val Acc: 100.00%\n",
      "Fold 2 Epoch [50/100], Train Loss: 0.1431, Train Acc: 100.00%, Val Loss: 0.1436, Val Acc: 100.00%\n",
      "Fold 2 Epoch [51/100], Train Loss: 0.1334, Train Acc: 100.00%, Val Loss: 0.1342, Val Acc: 100.00%\n",
      "Fold 2 Epoch [52/100], Train Loss: 0.1243, Train Acc: 100.00%, Val Loss: 0.1254, Val Acc: 100.00%\n",
      "Fold 2 Epoch [53/100], Train Loss: 0.1158, Train Acc: 100.00%, Val Loss: 0.1170, Val Acc: 100.00%\n",
      "Fold 2 Epoch [54/100], Train Loss: 0.1079, Train Acc: 100.00%, Val Loss: 0.1093, Val Acc: 100.00%\n",
      "Fold 2 Epoch [55/100], Train Loss: 0.1005, Train Acc: 100.00%, Val Loss: 0.1020, Val Acc: 100.00%\n",
      "Fold 2 Epoch [56/100], Train Loss: 0.0936, Train Acc: 100.00%, Val Loss: 0.0951, Val Acc: 100.00%\n",
      "Fold 2 Epoch [57/100], Train Loss: 0.0872, Train Acc: 100.00%, Val Loss: 0.0887, Val Acc: 100.00%\n",
      "Fold 2 Epoch [58/100], Train Loss: 0.0812, Train Acc: 100.00%, Val Loss: 0.0828, Val Acc: 100.00%\n",
      "Fold 2 Epoch [59/100], Train Loss: 0.0757, Train Acc: 100.00%, Val Loss: 0.0773, Val Acc: 100.00%\n",
      "Fold 2 Epoch [60/100], Train Loss: 0.0706, Train Acc: 100.00%, Val Loss: 0.0723, Val Acc: 100.00%\n",
      "Fold 2 Epoch [61/100], Train Loss: 0.0658, Train Acc: 100.00%, Val Loss: 0.0676, Val Acc: 100.00%\n",
      "Fold 2 Epoch [62/100], Train Loss: 0.0614, Train Acc: 100.00%, Val Loss: 0.0633, Val Acc: 100.00%\n",
      "Fold 2 Epoch [63/100], Train Loss: 0.0573, Train Acc: 100.00%, Val Loss: 0.0592, Val Acc: 100.00%\n",
      "Fold 2 Epoch [64/100], Train Loss: 0.0535, Train Acc: 100.00%, Val Loss: 0.0555, Val Acc: 100.00%\n",
      "Fold 2 Epoch [65/100], Train Loss: 0.0501, Train Acc: 100.00%, Val Loss: 0.0521, Val Acc: 100.00%\n",
      "Fold 2 Epoch [66/100], Train Loss: 0.0469, Train Acc: 100.00%, Val Loss: 0.0489, Val Acc: 100.00%\n",
      "Fold 2 Epoch [67/100], Train Loss: 0.0439, Train Acc: 100.00%, Val Loss: 0.0460, Val Acc: 100.00%\n",
      "Fold 2 Epoch [68/100], Train Loss: 0.0412, Train Acc: 100.00%, Val Loss: 0.0432, Val Acc: 100.00%\n",
      "Fold 2 Epoch [69/100], Train Loss: 0.0387, Train Acc: 100.00%, Val Loss: 0.0406, Val Acc: 100.00%\n",
      "Fold 2 Epoch [70/100], Train Loss: 0.0363, Train Acc: 100.00%, Val Loss: 0.0381, Val Acc: 100.00%\n",
      "Fold 2 Epoch [71/100], Train Loss: 0.0341, Train Acc: 100.00%, Val Loss: 0.0358, Val Acc: 100.00%\n",
      "Fold 2 Epoch [72/100], Train Loss: 0.0321, Train Acc: 100.00%, Val Loss: 0.0336, Val Acc: 100.00%\n",
      "Fold 2 Epoch [73/100], Train Loss: 0.0302, Train Acc: 100.00%, Val Loss: 0.0316, Val Acc: 100.00%\n",
      "Fold 2 Epoch [74/100], Train Loss: 0.0285, Train Acc: 100.00%, Val Loss: 0.0298, Val Acc: 100.00%\n",
      "Fold 2 Epoch [75/100], Train Loss: 0.0269, Train Acc: 100.00%, Val Loss: 0.0282, Val Acc: 100.00%\n",
      "Fold 2 Epoch [76/100], Train Loss: 0.0254, Train Acc: 100.00%, Val Loss: 0.0267, Val Acc: 100.00%\n",
      "Fold 2 Epoch [77/100], Train Loss: 0.0240, Train Acc: 100.00%, Val Loss: 0.0253, Val Acc: 100.00%\n",
      "Fold 2 Epoch [78/100], Train Loss: 0.0227, Train Acc: 100.00%, Val Loss: 0.0239, Val Acc: 100.00%\n",
      "Fold 2 Epoch [79/100], Train Loss: 0.0215, Train Acc: 100.00%, Val Loss: 0.0226, Val Acc: 100.00%\n",
      "Fold 2 Epoch [80/100], Train Loss: 0.0204, Train Acc: 100.00%, Val Loss: 0.0214, Val Acc: 100.00%\n",
      "Fold 2 Epoch [81/100], Train Loss: 0.0193, Train Acc: 100.00%, Val Loss: 0.0203, Val Acc: 100.00%\n",
      "Fold 2 Epoch [82/100], Train Loss: 0.0182, Train Acc: 100.00%, Val Loss: 0.0193, Val Acc: 100.00%\n",
      "Fold 2 Epoch [83/100], Train Loss: 0.0172, Train Acc: 100.00%, Val Loss: 0.0185, Val Acc: 100.00%\n",
      "Fold 2 Epoch [84/100], Train Loss: 0.0163, Train Acc: 100.00%, Val Loss: 0.0176, Val Acc: 100.00%\n",
      "Fold 2 Epoch [85/100], Train Loss: 0.0155, Train Acc: 100.00%, Val Loss: 0.0169, Val Acc: 100.00%\n",
      "Fold 2 Epoch [86/100], Train Loss: 0.0147, Train Acc: 100.00%, Val Loss: 0.0162, Val Acc: 100.00%\n",
      "Fold 2 Epoch [87/100], Train Loss: 0.0140, Train Acc: 100.00%, Val Loss: 0.0155, Val Acc: 100.00%\n",
      "Fold 2 Epoch [88/100], Train Loss: 0.0134, Train Acc: 100.00%, Val Loss: 0.0149, Val Acc: 100.00%\n",
      "Fold 2 Epoch [89/100], Train Loss: 0.0127, Train Acc: 100.00%, Val Loss: 0.0142, Val Acc: 100.00%\n",
      "Fold 2 Epoch [90/100], Train Loss: 0.0121, Train Acc: 100.00%, Val Loss: 0.0136, Val Acc: 100.00%\n",
      "Fold 2 Epoch [91/100], Train Loss: 0.0116, Train Acc: 100.00%, Val Loss: 0.0130, Val Acc: 100.00%\n",
      "Fold 2 Epoch [92/100], Train Loss: 0.0111, Train Acc: 100.00%, Val Loss: 0.0125, Val Acc: 100.00%\n",
      "Fold 2 Epoch [93/100], Train Loss: 0.0106, Train Acc: 100.00%, Val Loss: 0.0120, Val Acc: 100.00%\n",
      "Fold 2 Epoch [94/100], Train Loss: 0.0102, Train Acc: 100.00%, Val Loss: 0.0115, Val Acc: 100.00%\n",
      "Fold 2 Epoch [95/100], Train Loss: 0.0098, Train Acc: 100.00%, Val Loss: 0.0111, Val Acc: 100.00%\n",
      "Fold 2 Epoch [96/100], Train Loss: 0.0094, Train Acc: 100.00%, Val Loss: 0.0106, Val Acc: 100.00%\n",
      "Fold 2 Epoch [97/100], Train Loss: 0.0090, Train Acc: 100.00%, Val Loss: 0.0102, Val Acc: 100.00%\n",
      "Fold 2 Epoch [98/100], Train Loss: 0.0087, Train Acc: 100.00%, Val Loss: 0.0098, Val Acc: 100.00%\n",
      "Fold 2 Epoch [99/100], Train Loss: 0.0083, Train Acc: 100.00%, Val Loss: 0.0095, Val Acc: 100.00%\n",
      "Fold 2 Epoch [100/100], Train Loss: 0.0080, Train Acc: 100.00%, Val Loss: 0.0091, Val Acc: 100.00%\n",
      "Fold 2 Best Accuracy: 100.00%\n",
      "Fold 3/5\n",
      "Fold 3 Epoch [1/100], Train Loss: 1.0356, Train Acc: 44.83%, Val Loss: 1.0547, Val Acc: 28.57%\n",
      "Fold 3 Epoch [2/100], Train Loss: 1.0210, Train Acc: 58.62%, Val Loss: 1.0389, Val Acc: 57.14%\n",
      "Fold 3 Epoch [3/100], Train Loss: 1.0067, Train Acc: 58.62%, Val Loss: 1.0234, Val Acc: 85.71%\n",
      "Fold 3 Epoch [4/100], Train Loss: 0.9924, Train Acc: 62.07%, Val Loss: 1.0079, Val Acc: 85.71%\n",
      "Fold 3 Epoch [5/100], Train Loss: 0.9779, Train Acc: 62.07%, Val Loss: 0.9922, Val Acc: 85.71%\n",
      "Fold 3 Epoch [6/100], Train Loss: 0.9631, Train Acc: 68.97%, Val Loss: 0.9761, Val Acc: 85.71%\n",
      "Fold 3 Epoch [7/100], Train Loss: 0.9482, Train Acc: 75.86%, Val Loss: 0.9597, Val Acc: 85.71%\n",
      "Fold 3 Epoch [8/100], Train Loss: 0.9330, Train Acc: 89.66%, Val Loss: 0.9431, Val Acc: 85.71%\n",
      "Fold 3 Epoch [9/100], Train Loss: 0.9177, Train Acc: 96.55%, Val Loss: 0.9266, Val Acc: 100.00%\n",
      "Fold 3 Epoch [10/100], Train Loss: 0.9022, Train Acc: 96.55%, Val Loss: 0.9098, Val Acc: 100.00%\n",
      "Fold 3 Epoch [11/100], Train Loss: 0.8863, Train Acc: 96.55%, Val Loss: 0.8927, Val Acc: 100.00%\n",
      "Fold 3 Epoch [12/100], Train Loss: 0.8702, Train Acc: 96.55%, Val Loss: 0.8749, Val Acc: 100.00%\n",
      "Fold 3 Epoch [13/100], Train Loss: 0.8538, Train Acc: 96.55%, Val Loss: 0.8569, Val Acc: 100.00%\n",
      "Fold 3 Epoch [14/100], Train Loss: 0.8371, Train Acc: 96.55%, Val Loss: 0.8387, Val Acc: 100.00%\n",
      "Fold 3 Epoch [15/100], Train Loss: 0.8198, Train Acc: 96.55%, Val Loss: 0.8199, Val Acc: 100.00%\n",
      "Fold 3 Epoch [16/100], Train Loss: 0.8021, Train Acc: 96.55%, Val Loss: 0.8005, Val Acc: 100.00%\n",
      "Fold 3 Epoch [17/100], Train Loss: 0.7840, Train Acc: 96.55%, Val Loss: 0.7806, Val Acc: 100.00%\n",
      "Fold 3 Epoch [18/100], Train Loss: 0.7655, Train Acc: 100.00%, Val Loss: 0.7601, Val Acc: 100.00%\n",
      "Fold 3 Epoch [19/100], Train Loss: 0.7467, Train Acc: 100.00%, Val Loss: 0.7393, Val Acc: 100.00%\n",
      "Fold 3 Epoch [20/100], Train Loss: 0.7275, Train Acc: 100.00%, Val Loss: 0.7180, Val Acc: 100.00%\n",
      "Fold 3 Epoch [21/100], Train Loss: 0.7080, Train Acc: 100.00%, Val Loss: 0.6962, Val Acc: 100.00%\n",
      "Fold 3 Epoch [22/100], Train Loss: 0.6882, Train Acc: 100.00%, Val Loss: 0.6742, Val Acc: 100.00%\n",
      "Fold 3 Epoch [23/100], Train Loss: 0.6679, Train Acc: 100.00%, Val Loss: 0.6521, Val Acc: 100.00%\n",
      "Fold 3 Epoch [24/100], Train Loss: 0.6473, Train Acc: 100.00%, Val Loss: 0.6297, Val Acc: 100.00%\n",
      "Fold 3 Epoch [25/100], Train Loss: 0.6266, Train Acc: 100.00%, Val Loss: 0.6072, Val Acc: 100.00%\n",
      "Fold 3 Epoch [26/100], Train Loss: 0.6056, Train Acc: 100.00%, Val Loss: 0.5844, Val Acc: 100.00%\n",
      "Fold 3 Epoch [27/100], Train Loss: 0.5845, Train Acc: 100.00%, Val Loss: 0.5616, Val Acc: 100.00%\n",
      "Fold 3 Epoch [28/100], Train Loss: 0.5630, Train Acc: 100.00%, Val Loss: 0.5388, Val Acc: 100.00%\n",
      "Fold 3 Epoch [29/100], Train Loss: 0.5414, Train Acc: 100.00%, Val Loss: 0.5159, Val Acc: 100.00%\n",
      "Fold 3 Epoch [30/100], Train Loss: 0.5198, Train Acc: 100.00%, Val Loss: 0.4932, Val Acc: 100.00%\n",
      "Fold 3 Epoch [31/100], Train Loss: 0.4982, Train Acc: 100.00%, Val Loss: 0.4707, Val Acc: 100.00%\n",
      "Fold 3 Epoch [32/100], Train Loss: 0.4766, Train Acc: 100.00%, Val Loss: 0.4487, Val Acc: 100.00%\n",
      "Fold 3 Epoch [33/100], Train Loss: 0.4550, Train Acc: 100.00%, Val Loss: 0.4273, Val Acc: 100.00%\n",
      "Fold 3 Epoch [34/100], Train Loss: 0.4335, Train Acc: 100.00%, Val Loss: 0.4064, Val Acc: 100.00%\n",
      "Fold 3 Epoch [35/100], Train Loss: 0.4122, Train Acc: 100.00%, Val Loss: 0.3859, Val Acc: 100.00%\n",
      "Fold 3 Epoch [36/100], Train Loss: 0.3913, Train Acc: 100.00%, Val Loss: 0.3657, Val Acc: 100.00%\n",
      "Fold 3 Epoch [37/100], Train Loss: 0.3707, Train Acc: 100.00%, Val Loss: 0.3460, Val Acc: 100.00%\n",
      "Fold 3 Epoch [38/100], Train Loss: 0.3506, Train Acc: 100.00%, Val Loss: 0.3269, Val Acc: 100.00%\n",
      "Fold 3 Epoch [39/100], Train Loss: 0.3311, Train Acc: 100.00%, Val Loss: 0.3085, Val Acc: 100.00%\n",
      "Fold 3 Epoch [40/100], Train Loss: 0.3119, Train Acc: 100.00%, Val Loss: 0.2908, Val Acc: 100.00%\n",
      "Fold 3 Epoch [41/100], Train Loss: 0.2933, Train Acc: 100.00%, Val Loss: 0.2737, Val Acc: 100.00%\n",
      "Fold 3 Epoch [42/100], Train Loss: 0.2753, Train Acc: 100.00%, Val Loss: 0.2575, Val Acc: 100.00%\n",
      "Fold 3 Epoch [43/100], Train Loss: 0.2579, Train Acc: 100.00%, Val Loss: 0.2421, Val Acc: 100.00%\n",
      "Fold 3 Epoch [44/100], Train Loss: 0.2411, Train Acc: 100.00%, Val Loss: 0.2275, Val Acc: 100.00%\n",
      "Fold 3 Epoch [45/100], Train Loss: 0.2250, Train Acc: 100.00%, Val Loss: 0.2137, Val Acc: 100.00%\n",
      "Fold 3 Epoch [46/100], Train Loss: 0.2094, Train Acc: 100.00%, Val Loss: 0.2006, Val Acc: 100.00%\n",
      "Fold 3 Epoch [47/100], Train Loss: 0.1946, Train Acc: 100.00%, Val Loss: 0.1881, Val Acc: 100.00%\n",
      "Fold 3 Epoch [48/100], Train Loss: 0.1805, Train Acc: 100.00%, Val Loss: 0.1762, Val Acc: 100.00%\n",
      "Fold 3 Epoch [49/100], Train Loss: 0.1671, Train Acc: 100.00%, Val Loss: 0.1650, Val Acc: 100.00%\n",
      "Fold 3 Epoch [50/100], Train Loss: 0.1544, Train Acc: 100.00%, Val Loss: 0.1545, Val Acc: 100.00%\n",
      "Fold 3 Epoch [51/100], Train Loss: 0.1426, Train Acc: 100.00%, Val Loss: 0.1447, Val Acc: 100.00%\n",
      "Fold 3 Epoch [52/100], Train Loss: 0.1315, Train Acc: 100.00%, Val Loss: 0.1353, Val Acc: 100.00%\n",
      "Fold 3 Epoch [53/100], Train Loss: 0.1212, Train Acc: 100.00%, Val Loss: 0.1262, Val Acc: 100.00%\n",
      "Fold 3 Epoch [54/100], Train Loss: 0.1115, Train Acc: 100.00%, Val Loss: 0.1178, Val Acc: 100.00%\n",
      "Fold 3 Epoch [55/100], Train Loss: 0.1025, Train Acc: 100.00%, Val Loss: 0.1098, Val Acc: 100.00%\n",
      "Fold 3 Epoch [56/100], Train Loss: 0.0941, Train Acc: 100.00%, Val Loss: 0.1022, Val Acc: 100.00%\n",
      "Fold 3 Epoch [57/100], Train Loss: 0.0865, Train Acc: 100.00%, Val Loss: 0.0954, Val Acc: 100.00%\n",
      "Fold 3 Epoch [58/100], Train Loss: 0.0794, Train Acc: 100.00%, Val Loss: 0.0892, Val Acc: 100.00%\n",
      "Fold 3 Epoch [59/100], Train Loss: 0.0729, Train Acc: 100.00%, Val Loss: 0.0833, Val Acc: 100.00%\n",
      "Fold 3 Epoch [60/100], Train Loss: 0.0670, Train Acc: 100.00%, Val Loss: 0.0777, Val Acc: 100.00%\n",
      "Fold 3 Epoch [61/100], Train Loss: 0.0616, Train Acc: 100.00%, Val Loss: 0.0725, Val Acc: 100.00%\n",
      "Fold 3 Epoch [62/100], Train Loss: 0.0567, Train Acc: 100.00%, Val Loss: 0.0677, Val Acc: 100.00%\n",
      "Fold 3 Epoch [63/100], Train Loss: 0.0521, Train Acc: 100.00%, Val Loss: 0.0633, Val Acc: 100.00%\n",
      "Fold 3 Epoch [64/100], Train Loss: 0.0480, Train Acc: 100.00%, Val Loss: 0.0593, Val Acc: 100.00%\n",
      "Fold 3 Epoch [65/100], Train Loss: 0.0443, Train Acc: 100.00%, Val Loss: 0.0555, Val Acc: 100.00%\n",
      "Fold 3 Epoch [66/100], Train Loss: 0.0409, Train Acc: 100.00%, Val Loss: 0.0521, Val Acc: 100.00%\n",
      "Fold 3 Epoch [67/100], Train Loss: 0.0378, Train Acc: 100.00%, Val Loss: 0.0489, Val Acc: 100.00%\n",
      "Fold 3 Epoch [68/100], Train Loss: 0.0350, Train Acc: 100.00%, Val Loss: 0.0460, Val Acc: 100.00%\n",
      "Fold 3 Epoch [69/100], Train Loss: 0.0324, Train Acc: 100.00%, Val Loss: 0.0433, Val Acc: 100.00%\n",
      "Fold 3 Epoch [70/100], Train Loss: 0.0300, Train Acc: 100.00%, Val Loss: 0.0407, Val Acc: 100.00%\n",
      "Fold 3 Epoch [71/100], Train Loss: 0.0278, Train Acc: 100.00%, Val Loss: 0.0383, Val Acc: 100.00%\n",
      "Fold 3 Epoch [72/100], Train Loss: 0.0259, Train Acc: 100.00%, Val Loss: 0.0362, Val Acc: 100.00%\n",
      "Fold 3 Epoch [73/100], Train Loss: 0.0241, Train Acc: 100.00%, Val Loss: 0.0341, Val Acc: 100.00%\n",
      "Fold 3 Epoch [74/100], Train Loss: 0.0224, Train Acc: 100.00%, Val Loss: 0.0322, Val Acc: 100.00%\n",
      "Fold 3 Epoch [75/100], Train Loss: 0.0209, Train Acc: 100.00%, Val Loss: 0.0304, Val Acc: 100.00%\n",
      "Fold 3 Epoch [76/100], Train Loss: 0.0195, Train Acc: 100.00%, Val Loss: 0.0288, Val Acc: 100.00%\n",
      "Fold 3 Epoch [77/100], Train Loss: 0.0182, Train Acc: 100.00%, Val Loss: 0.0273, Val Acc: 100.00%\n",
      "Fold 3 Epoch [78/100], Train Loss: 0.0171, Train Acc: 100.00%, Val Loss: 0.0260, Val Acc: 100.00%\n",
      "Fold 3 Epoch [79/100], Train Loss: 0.0161, Train Acc: 100.00%, Val Loss: 0.0247, Val Acc: 100.00%\n",
      "Fold 3 Epoch [80/100], Train Loss: 0.0151, Train Acc: 100.00%, Val Loss: 0.0235, Val Acc: 100.00%\n",
      "Fold 3 Epoch [81/100], Train Loss: 0.0143, Train Acc: 100.00%, Val Loss: 0.0223, Val Acc: 100.00%\n",
      "Fold 3 Epoch [82/100], Train Loss: 0.0135, Train Acc: 100.00%, Val Loss: 0.0213, Val Acc: 100.00%\n",
      "Fold 3 Epoch [83/100], Train Loss: 0.0127, Train Acc: 100.00%, Val Loss: 0.0203, Val Acc: 100.00%\n",
      "Fold 3 Epoch [84/100], Train Loss: 0.0120, Train Acc: 100.00%, Val Loss: 0.0193, Val Acc: 100.00%\n",
      "Fold 3 Epoch [85/100], Train Loss: 0.0114, Train Acc: 100.00%, Val Loss: 0.0184, Val Acc: 100.00%\n",
      "Fold 3 Epoch [86/100], Train Loss: 0.0108, Train Acc: 100.00%, Val Loss: 0.0176, Val Acc: 100.00%\n",
      "Fold 3 Epoch [87/100], Train Loss: 0.0102, Train Acc: 100.00%, Val Loss: 0.0168, Val Acc: 100.00%\n",
      "Fold 3 Epoch [88/100], Train Loss: 0.0097, Train Acc: 100.00%, Val Loss: 0.0161, Val Acc: 100.00%\n",
      "Fold 3 Epoch [89/100], Train Loss: 0.0092, Train Acc: 100.00%, Val Loss: 0.0154, Val Acc: 100.00%\n",
      "Fold 3 Epoch [90/100], Train Loss: 0.0088, Train Acc: 100.00%, Val Loss: 0.0148, Val Acc: 100.00%\n",
      "Fold 3 Epoch [91/100], Train Loss: 0.0084, Train Acc: 100.00%, Val Loss: 0.0141, Val Acc: 100.00%\n",
      "Fold 3 Epoch [92/100], Train Loss: 0.0080, Train Acc: 100.00%, Val Loss: 0.0136, Val Acc: 100.00%\n",
      "Fold 3 Epoch [93/100], Train Loss: 0.0077, Train Acc: 100.00%, Val Loss: 0.0131, Val Acc: 100.00%\n",
      "Fold 3 Epoch [94/100], Train Loss: 0.0073, Train Acc: 100.00%, Val Loss: 0.0126, Val Acc: 100.00%\n",
      "Fold 3 Epoch [95/100], Train Loss: 0.0071, Train Acc: 100.00%, Val Loss: 0.0121, Val Acc: 100.00%\n",
      "Fold 3 Epoch [96/100], Train Loss: 0.0068, Train Acc: 100.00%, Val Loss: 0.0117, Val Acc: 100.00%\n",
      "Fold 3 Epoch [97/100], Train Loss: 0.0065, Train Acc: 100.00%, Val Loss: 0.0113, Val Acc: 100.00%\n",
      "Fold 3 Epoch [98/100], Train Loss: 0.0062, Train Acc: 100.00%, Val Loss: 0.0109, Val Acc: 100.00%\n",
      "Fold 3 Epoch [99/100], Train Loss: 0.0060, Train Acc: 100.00%, Val Loss: 0.0105, Val Acc: 100.00%\n",
      "Fold 3 Epoch [100/100], Train Loss: 0.0058, Train Acc: 100.00%, Val Loss: 0.0101, Val Acc: 100.00%\n",
      "Fold 3 Best Accuracy: 100.00%\n",
      "Fold 4/5\n",
      "Fold 4 Epoch [1/100], Train Loss: 1.0634, Train Acc: 34.48%, Val Loss: 1.0577, Val Acc: 28.57%\n",
      "Fold 4 Epoch [2/100], Train Loss: 1.0482, Train Acc: 37.93%, Val Loss: 1.0403, Val Acc: 57.14%\n",
      "Fold 4 Epoch [3/100], Train Loss: 1.0330, Train Acc: 37.93%, Val Loss: 1.0236, Val Acc: 57.14%\n",
      "Fold 4 Epoch [4/100], Train Loss: 1.0170, Train Acc: 58.62%, Val Loss: 1.0064, Val Acc: 71.43%\n",
      "Fold 4 Epoch [5/100], Train Loss: 1.0003, Train Acc: 68.97%, Val Loss: 0.9895, Val Acc: 71.43%\n",
      "Fold 4 Epoch [6/100], Train Loss: 0.9833, Train Acc: 68.97%, Val Loss: 0.9720, Val Acc: 85.71%\n",
      "Fold 4 Epoch [7/100], Train Loss: 0.9662, Train Acc: 82.76%, Val Loss: 0.9546, Val Acc: 85.71%\n",
      "Fold 4 Epoch [8/100], Train Loss: 0.9487, Train Acc: 82.76%, Val Loss: 0.9366, Val Acc: 85.71%\n",
      "Fold 4 Epoch [9/100], Train Loss: 0.9308, Train Acc: 96.55%, Val Loss: 0.9180, Val Acc: 85.71%\n",
      "Fold 4 Epoch [10/100], Train Loss: 0.9127, Train Acc: 96.55%, Val Loss: 0.8992, Val Acc: 100.00%\n",
      "Fold 4 Epoch [11/100], Train Loss: 0.8942, Train Acc: 96.55%, Val Loss: 0.8798, Val Acc: 100.00%\n",
      "Fold 4 Epoch [12/100], Train Loss: 0.8753, Train Acc: 96.55%, Val Loss: 0.8599, Val Acc: 100.00%\n",
      "Fold 4 Epoch [13/100], Train Loss: 0.8559, Train Acc: 96.55%, Val Loss: 0.8395, Val Acc: 100.00%\n",
      "Fold 4 Epoch [14/100], Train Loss: 0.8364, Train Acc: 96.55%, Val Loss: 0.8187, Val Acc: 100.00%\n",
      "Fold 4 Epoch [15/100], Train Loss: 0.8166, Train Acc: 100.00%, Val Loss: 0.7974, Val Acc: 100.00%\n",
      "Fold 4 Epoch [16/100], Train Loss: 0.7965, Train Acc: 100.00%, Val Loss: 0.7753, Val Acc: 100.00%\n",
      "Fold 4 Epoch [17/100], Train Loss: 0.7760, Train Acc: 100.00%, Val Loss: 0.7528, Val Acc: 100.00%\n",
      "Fold 4 Epoch [18/100], Train Loss: 0.7551, Train Acc: 100.00%, Val Loss: 0.7301, Val Acc: 100.00%\n",
      "Fold 4 Epoch [19/100], Train Loss: 0.7339, Train Acc: 100.00%, Val Loss: 0.7072, Val Acc: 100.00%\n",
      "Fold 4 Epoch [20/100], Train Loss: 0.7124, Train Acc: 100.00%, Val Loss: 0.6839, Val Acc: 100.00%\n",
      "Fold 4 Epoch [21/100], Train Loss: 0.6906, Train Acc: 100.00%, Val Loss: 0.6605, Val Acc: 100.00%\n",
      "Fold 4 Epoch [22/100], Train Loss: 0.6686, Train Acc: 100.00%, Val Loss: 0.6370, Val Acc: 100.00%\n",
      "Fold 4 Epoch [23/100], Train Loss: 0.6463, Train Acc: 100.00%, Val Loss: 0.6134, Val Acc: 100.00%\n",
      "Fold 4 Epoch [24/100], Train Loss: 0.6237, Train Acc: 100.00%, Val Loss: 0.5898, Val Acc: 100.00%\n",
      "Fold 4 Epoch [25/100], Train Loss: 0.6012, Train Acc: 100.00%, Val Loss: 0.5667, Val Acc: 100.00%\n",
      "Fold 4 Epoch [26/100], Train Loss: 0.5788, Train Acc: 100.00%, Val Loss: 0.5437, Val Acc: 100.00%\n",
      "Fold 4 Epoch [27/100], Train Loss: 0.5564, Train Acc: 100.00%, Val Loss: 0.5211, Val Acc: 100.00%\n",
      "Fold 4 Epoch [28/100], Train Loss: 0.5343, Train Acc: 100.00%, Val Loss: 0.4989, Val Acc: 100.00%\n",
      "Fold 4 Epoch [29/100], Train Loss: 0.5123, Train Acc: 100.00%, Val Loss: 0.4770, Val Acc: 100.00%\n",
      "Fold 4 Epoch [30/100], Train Loss: 0.4906, Train Acc: 100.00%, Val Loss: 0.4556, Val Acc: 100.00%\n",
      "Fold 4 Epoch [31/100], Train Loss: 0.4691, Train Acc: 100.00%, Val Loss: 0.4347, Val Acc: 100.00%\n",
      "Fold 4 Epoch [32/100], Train Loss: 0.4481, Train Acc: 100.00%, Val Loss: 0.4144, Val Acc: 100.00%\n",
      "Fold 4 Epoch [33/100], Train Loss: 0.4274, Train Acc: 100.00%, Val Loss: 0.3947, Val Acc: 100.00%\n",
      "Fold 4 Epoch [34/100], Train Loss: 0.4072, Train Acc: 100.00%, Val Loss: 0.3756, Val Acc: 100.00%\n",
      "Fold 4 Epoch [35/100], Train Loss: 0.3875, Train Acc: 100.00%, Val Loss: 0.3572, Val Acc: 100.00%\n",
      "Fold 4 Epoch [36/100], Train Loss: 0.3683, Train Acc: 100.00%, Val Loss: 0.3394, Val Acc: 100.00%\n",
      "Fold 4 Epoch [37/100], Train Loss: 0.3496, Train Acc: 100.00%, Val Loss: 0.3223, Val Acc: 100.00%\n",
      "Fold 4 Epoch [38/100], Train Loss: 0.3315, Train Acc: 100.00%, Val Loss: 0.3059, Val Acc: 100.00%\n",
      "Fold 4 Epoch [39/100], Train Loss: 0.3140, Train Acc: 100.00%, Val Loss: 0.2901, Val Acc: 100.00%\n",
      "Fold 4 Epoch [40/100], Train Loss: 0.2971, Train Acc: 100.00%, Val Loss: 0.2751, Val Acc: 100.00%\n",
      "Fold 4 Epoch [41/100], Train Loss: 0.2809, Train Acc: 100.00%, Val Loss: 0.2606, Val Acc: 100.00%\n",
      "Fold 4 Epoch [42/100], Train Loss: 0.2652, Train Acc: 100.00%, Val Loss: 0.2466, Val Acc: 100.00%\n",
      "Fold 4 Epoch [43/100], Train Loss: 0.2501, Train Acc: 100.00%, Val Loss: 0.2332, Val Acc: 100.00%\n",
      "Fold 4 Epoch [44/100], Train Loss: 0.2356, Train Acc: 100.00%, Val Loss: 0.2204, Val Acc: 100.00%\n",
      "Fold 4 Epoch [45/100], Train Loss: 0.2216, Train Acc: 100.00%, Val Loss: 0.2082, Val Acc: 100.00%\n",
      "Fold 4 Epoch [46/100], Train Loss: 0.2083, Train Acc: 100.00%, Val Loss: 0.1965, Val Acc: 100.00%\n",
      "Fold 4 Epoch [47/100], Train Loss: 0.1957, Train Acc: 100.00%, Val Loss: 0.1855, Val Acc: 100.00%\n",
      "Fold 4 Epoch [48/100], Train Loss: 0.1836, Train Acc: 100.00%, Val Loss: 0.1750, Val Acc: 100.00%\n",
      "Fold 4 Epoch [49/100], Train Loss: 0.1722, Train Acc: 100.00%, Val Loss: 0.1651, Val Acc: 100.00%\n",
      "Fold 4 Epoch [50/100], Train Loss: 0.1612, Train Acc: 100.00%, Val Loss: 0.1556, Val Acc: 100.00%\n",
      "Fold 4 Epoch [51/100], Train Loss: 0.1509, Train Acc: 100.00%, Val Loss: 0.1467, Val Acc: 100.00%\n",
      "Fold 4 Epoch [52/100], Train Loss: 0.1412, Train Acc: 100.00%, Val Loss: 0.1382, Val Acc: 100.00%\n",
      "Fold 4 Epoch [53/100], Train Loss: 0.1321, Train Acc: 100.00%, Val Loss: 0.1301, Val Acc: 100.00%\n",
      "Fold 4 Epoch [54/100], Train Loss: 0.1235, Train Acc: 100.00%, Val Loss: 0.1224, Val Acc: 100.00%\n",
      "Fold 4 Epoch [55/100], Train Loss: 0.1154, Train Acc: 100.00%, Val Loss: 0.1152, Val Acc: 100.00%\n",
      "Fold 4 Epoch [56/100], Train Loss: 0.1077, Train Acc: 100.00%, Val Loss: 0.1082, Val Acc: 100.00%\n",
      "Fold 4 Epoch [57/100], Train Loss: 0.1006, Train Acc: 100.00%, Val Loss: 0.1018, Val Acc: 100.00%\n",
      "Fold 4 Epoch [58/100], Train Loss: 0.0939, Train Acc: 100.00%, Val Loss: 0.0958, Val Acc: 100.00%\n",
      "Fold 4 Epoch [59/100], Train Loss: 0.0877, Train Acc: 100.00%, Val Loss: 0.0902, Val Acc: 100.00%\n",
      "Fold 4 Epoch [60/100], Train Loss: 0.0819, Train Acc: 100.00%, Val Loss: 0.0849, Val Acc: 100.00%\n",
      "Fold 4 Epoch [61/100], Train Loss: 0.0765, Train Acc: 100.00%, Val Loss: 0.0801, Val Acc: 100.00%\n",
      "Fold 4 Epoch [62/100], Train Loss: 0.0716, Train Acc: 100.00%, Val Loss: 0.0757, Val Acc: 100.00%\n",
      "Fold 4 Epoch [63/100], Train Loss: 0.0669, Train Acc: 100.00%, Val Loss: 0.0715, Val Acc: 100.00%\n",
      "Fold 4 Epoch [64/100], Train Loss: 0.0626, Train Acc: 100.00%, Val Loss: 0.0675, Val Acc: 100.00%\n",
      "Fold 4 Epoch [65/100], Train Loss: 0.0585, Train Acc: 100.00%, Val Loss: 0.0637, Val Acc: 100.00%\n",
      "Fold 4 Epoch [66/100], Train Loss: 0.0547, Train Acc: 100.00%, Val Loss: 0.0602, Val Acc: 100.00%\n",
      "Fold 4 Epoch [67/100], Train Loss: 0.0513, Train Acc: 100.00%, Val Loss: 0.0568, Val Acc: 100.00%\n",
      "Fold 4 Epoch [68/100], Train Loss: 0.0481, Train Acc: 100.00%, Val Loss: 0.0537, Val Acc: 100.00%\n",
      "Fold 4 Epoch [69/100], Train Loss: 0.0452, Train Acc: 100.00%, Val Loss: 0.0508, Val Acc: 100.00%\n",
      "Fold 4 Epoch [70/100], Train Loss: 0.0424, Train Acc: 100.00%, Val Loss: 0.0481, Val Acc: 100.00%\n",
      "Fold 4 Epoch [71/100], Train Loss: 0.0399, Train Acc: 100.00%, Val Loss: 0.0456, Val Acc: 100.00%\n",
      "Fold 4 Epoch [72/100], Train Loss: 0.0375, Train Acc: 100.00%, Val Loss: 0.0432, Val Acc: 100.00%\n",
      "Fold 4 Epoch [73/100], Train Loss: 0.0353, Train Acc: 100.00%, Val Loss: 0.0410, Val Acc: 100.00%\n",
      "Fold 4 Epoch [74/100], Train Loss: 0.0332, Train Acc: 100.00%, Val Loss: 0.0389, Val Acc: 100.00%\n",
      "Fold 4 Epoch [75/100], Train Loss: 0.0313, Train Acc: 100.00%, Val Loss: 0.0369, Val Acc: 100.00%\n",
      "Fold 4 Epoch [76/100], Train Loss: 0.0296, Train Acc: 100.00%, Val Loss: 0.0350, Val Acc: 100.00%\n",
      "Fold 4 Epoch [77/100], Train Loss: 0.0279, Train Acc: 100.00%, Val Loss: 0.0332, Val Acc: 100.00%\n",
      "Fold 4 Epoch [78/100], Train Loss: 0.0264, Train Acc: 100.00%, Val Loss: 0.0316, Val Acc: 100.00%\n",
      "Fold 4 Epoch [79/100], Train Loss: 0.0250, Train Acc: 100.00%, Val Loss: 0.0302, Val Acc: 100.00%\n",
      "Fold 4 Epoch [80/100], Train Loss: 0.0238, Train Acc: 100.00%, Val Loss: 0.0289, Val Acc: 100.00%\n",
      "Fold 4 Epoch [81/100], Train Loss: 0.0226, Train Acc: 100.00%, Val Loss: 0.0277, Val Acc: 100.00%\n",
      "Fold 4 Epoch [82/100], Train Loss: 0.0215, Train Acc: 100.00%, Val Loss: 0.0266, Val Acc: 100.00%\n",
      "Fold 4 Epoch [83/100], Train Loss: 0.0205, Train Acc: 100.00%, Val Loss: 0.0256, Val Acc: 100.00%\n",
      "Fold 4 Epoch [84/100], Train Loss: 0.0195, Train Acc: 100.00%, Val Loss: 0.0246, Val Acc: 100.00%\n",
      "Fold 4 Epoch [85/100], Train Loss: 0.0186, Train Acc: 100.00%, Val Loss: 0.0235, Val Acc: 100.00%\n",
      "Fold 4 Epoch [86/100], Train Loss: 0.0177, Train Acc: 100.00%, Val Loss: 0.0224, Val Acc: 100.00%\n",
      "Fold 4 Epoch [87/100], Train Loss: 0.0168, Train Acc: 100.00%, Val Loss: 0.0214, Val Acc: 100.00%\n",
      "Fold 4 Epoch [88/100], Train Loss: 0.0160, Train Acc: 100.00%, Val Loss: 0.0205, Val Acc: 100.00%\n",
      "Fold 4 Epoch [89/100], Train Loss: 0.0153, Train Acc: 100.00%, Val Loss: 0.0196, Val Acc: 100.00%\n",
      "Fold 4 Epoch [90/100], Train Loss: 0.0145, Train Acc: 100.00%, Val Loss: 0.0186, Val Acc: 100.00%\n",
      "Fold 4 Epoch [91/100], Train Loss: 0.0138, Train Acc: 100.00%, Val Loss: 0.0177, Val Acc: 100.00%\n",
      "Fold 4 Epoch [92/100], Train Loss: 0.0131, Train Acc: 100.00%, Val Loss: 0.0168, Val Acc: 100.00%\n",
      "Fold 4 Epoch [93/100], Train Loss: 0.0125, Train Acc: 100.00%, Val Loss: 0.0160, Val Acc: 100.00%\n",
      "Fold 4 Epoch [94/100], Train Loss: 0.0119, Train Acc: 100.00%, Val Loss: 0.0151, Val Acc: 100.00%\n",
      "Fold 4 Epoch [95/100], Train Loss: 0.0113, Train Acc: 100.00%, Val Loss: 0.0143, Val Acc: 100.00%\n",
      "Fold 4 Epoch [96/100], Train Loss: 0.0108, Train Acc: 100.00%, Val Loss: 0.0136, Val Acc: 100.00%\n",
      "Fold 4 Epoch [97/100], Train Loss: 0.0103, Train Acc: 100.00%, Val Loss: 0.0129, Val Acc: 100.00%\n",
      "Fold 4 Epoch [98/100], Train Loss: 0.0098, Train Acc: 100.00%, Val Loss: 0.0123, Val Acc: 100.00%\n",
      "Fold 4 Epoch [99/100], Train Loss: 0.0093, Train Acc: 100.00%, Val Loss: 0.0116, Val Acc: 100.00%\n",
      "Fold 4 Epoch [100/100], Train Loss: 0.0089, Train Acc: 100.00%, Val Loss: 0.0110, Val Acc: 100.00%\n",
      "Fold 4 Best Accuracy: 100.00%\n",
      "Fold 5/5\n",
      "Fold 5 Epoch [1/100], Train Loss: 1.0935, Train Acc: 27.59%, Val Loss: 1.0401, Val Acc: 57.14%\n",
      "Fold 5 Epoch [2/100], Train Loss: 1.0745, Train Acc: 27.59%, Val Loss: 1.0259, Val Acc: 57.14%\n",
      "Fold 5 Epoch [3/100], Train Loss: 1.0559, Train Acc: 37.93%, Val Loss: 1.0119, Val Acc: 57.14%\n",
      "Fold 5 Epoch [4/100], Train Loss: 1.0375, Train Acc: 44.83%, Val Loss: 0.9983, Val Acc: 57.14%\n",
      "Fold 5 Epoch [5/100], Train Loss: 1.0188, Train Acc: 75.86%, Val Loss: 0.9855, Val Acc: 85.71%\n",
      "Fold 5 Epoch [6/100], Train Loss: 1.0000, Train Acc: 86.21%, Val Loss: 0.9725, Val Acc: 85.71%\n",
      "Fold 5 Epoch [7/100], Train Loss: 0.9813, Train Acc: 89.66%, Val Loss: 0.9599, Val Acc: 85.71%\n",
      "Fold 5 Epoch [8/100], Train Loss: 0.9629, Train Acc: 93.10%, Val Loss: 0.9470, Val Acc: 85.71%\n",
      "Fold 5 Epoch [9/100], Train Loss: 0.9445, Train Acc: 93.10%, Val Loss: 0.9337, Val Acc: 100.00%\n",
      "Fold 5 Epoch [10/100], Train Loss: 0.9261, Train Acc: 96.55%, Val Loss: 0.9201, Val Acc: 100.00%\n",
      "Fold 5 Epoch [11/100], Train Loss: 0.9073, Train Acc: 96.55%, Val Loss: 0.9063, Val Acc: 100.00%\n",
      "Fold 5 Epoch [12/100], Train Loss: 0.8882, Train Acc: 96.55%, Val Loss: 0.8922, Val Acc: 100.00%\n",
      "Fold 5 Epoch [13/100], Train Loss: 0.8687, Train Acc: 96.55%, Val Loss: 0.8775, Val Acc: 100.00%\n",
      "Fold 5 Epoch [14/100], Train Loss: 0.8492, Train Acc: 96.55%, Val Loss: 0.8627, Val Acc: 100.00%\n",
      "Fold 5 Epoch [15/100], Train Loss: 0.8292, Train Acc: 100.00%, Val Loss: 0.8469, Val Acc: 100.00%\n",
      "Fold 5 Epoch [16/100], Train Loss: 0.8088, Train Acc: 100.00%, Val Loss: 0.8306, Val Acc: 100.00%\n",
      "Fold 5 Epoch [17/100], Train Loss: 0.7877, Train Acc: 100.00%, Val Loss: 0.8137, Val Acc: 100.00%\n",
      "Fold 5 Epoch [18/100], Train Loss: 0.7664, Train Acc: 100.00%, Val Loss: 0.7962, Val Acc: 100.00%\n",
      "Fold 5 Epoch [19/100], Train Loss: 0.7445, Train Acc: 100.00%, Val Loss: 0.7783, Val Acc: 100.00%\n",
      "Fold 5 Epoch [20/100], Train Loss: 0.7222, Train Acc: 100.00%, Val Loss: 0.7602, Val Acc: 100.00%\n",
      "Fold 5 Epoch [21/100], Train Loss: 0.6998, Train Acc: 100.00%, Val Loss: 0.7418, Val Acc: 100.00%\n",
      "Fold 5 Epoch [22/100], Train Loss: 0.6775, Train Acc: 100.00%, Val Loss: 0.7234, Val Acc: 100.00%\n",
      "Fold 5 Epoch [23/100], Train Loss: 0.6549, Train Acc: 100.00%, Val Loss: 0.7050, Val Acc: 100.00%\n",
      "Fold 5 Epoch [24/100], Train Loss: 0.6323, Train Acc: 100.00%, Val Loss: 0.6865, Val Acc: 100.00%\n",
      "Fold 5 Epoch [25/100], Train Loss: 0.6097, Train Acc: 100.00%, Val Loss: 0.6678, Val Acc: 100.00%\n",
      "Fold 5 Epoch [26/100], Train Loss: 0.5871, Train Acc: 100.00%, Val Loss: 0.6490, Val Acc: 100.00%\n",
      "Fold 5 Epoch [27/100], Train Loss: 0.5645, Train Acc: 100.00%, Val Loss: 0.6304, Val Acc: 100.00%\n",
      "Fold 5 Epoch [28/100], Train Loss: 0.5421, Train Acc: 100.00%, Val Loss: 0.6118, Val Acc: 100.00%\n",
      "Fold 5 Epoch [29/100], Train Loss: 0.5199, Train Acc: 100.00%, Val Loss: 0.5931, Val Acc: 100.00%\n",
      "Fold 5 Epoch [30/100], Train Loss: 0.4978, Train Acc: 100.00%, Val Loss: 0.5743, Val Acc: 100.00%\n",
      "Fold 5 Epoch [31/100], Train Loss: 0.4760, Train Acc: 100.00%, Val Loss: 0.5553, Val Acc: 100.00%\n",
      "Fold 5 Epoch [32/100], Train Loss: 0.4546, Train Acc: 100.00%, Val Loss: 0.5358, Val Acc: 100.00%\n",
      "Fold 5 Epoch [33/100], Train Loss: 0.4335, Train Acc: 100.00%, Val Loss: 0.5163, Val Acc: 100.00%\n",
      "Fold 5 Epoch [34/100], Train Loss: 0.4128, Train Acc: 100.00%, Val Loss: 0.4970, Val Acc: 100.00%\n",
      "Fold 5 Epoch [35/100], Train Loss: 0.3926, Train Acc: 100.00%, Val Loss: 0.4777, Val Acc: 100.00%\n",
      "Fold 5 Epoch [36/100], Train Loss: 0.3729, Train Acc: 100.00%, Val Loss: 0.4584, Val Acc: 100.00%\n",
      "Fold 5 Epoch [37/100], Train Loss: 0.3540, Train Acc: 100.00%, Val Loss: 0.4398, Val Acc: 100.00%\n",
      "Fold 5 Epoch [38/100], Train Loss: 0.3356, Train Acc: 100.00%, Val Loss: 0.4212, Val Acc: 100.00%\n",
      "Fold 5 Epoch [39/100], Train Loss: 0.3177, Train Acc: 100.00%, Val Loss: 0.4029, Val Acc: 100.00%\n",
      "Fold 5 Epoch [40/100], Train Loss: 0.3005, Train Acc: 100.00%, Val Loss: 0.3848, Val Acc: 100.00%\n",
      "Fold 5 Epoch [41/100], Train Loss: 0.2838, Train Acc: 100.00%, Val Loss: 0.3672, Val Acc: 100.00%\n",
      "Fold 5 Epoch [42/100], Train Loss: 0.2679, Train Acc: 100.00%, Val Loss: 0.3498, Val Acc: 100.00%\n",
      "Fold 5 Epoch [43/100], Train Loss: 0.2526, Train Acc: 100.00%, Val Loss: 0.3328, Val Acc: 100.00%\n",
      "Fold 5 Epoch [44/100], Train Loss: 0.2381, Train Acc: 100.00%, Val Loss: 0.3162, Val Acc: 100.00%\n",
      "Fold 5 Epoch [45/100], Train Loss: 0.2242, Train Acc: 100.00%, Val Loss: 0.3001, Val Acc: 100.00%\n",
      "Fold 5 Epoch [46/100], Train Loss: 0.2109, Train Acc: 100.00%, Val Loss: 0.2847, Val Acc: 100.00%\n",
      "Fold 5 Epoch [47/100], Train Loss: 0.1983, Train Acc: 100.00%, Val Loss: 0.2693, Val Acc: 100.00%\n",
      "Fold 5 Epoch [48/100], Train Loss: 0.1863, Train Acc: 100.00%, Val Loss: 0.2544, Val Acc: 100.00%\n",
      "Fold 5 Epoch [49/100], Train Loss: 0.1750, Train Acc: 100.00%, Val Loss: 0.2400, Val Acc: 100.00%\n",
      "Fold 5 Epoch [50/100], Train Loss: 0.1643, Train Acc: 100.00%, Val Loss: 0.2261, Val Acc: 100.00%\n",
      "Fold 5 Epoch [51/100], Train Loss: 0.1541, Train Acc: 100.00%, Val Loss: 0.2129, Val Acc: 100.00%\n",
      "Fold 5 Epoch [52/100], Train Loss: 0.1445, Train Acc: 100.00%, Val Loss: 0.2002, Val Acc: 100.00%\n",
      "Fold 5 Epoch [53/100], Train Loss: 0.1355, Train Acc: 100.00%, Val Loss: 0.1880, Val Acc: 100.00%\n",
      "Fold 5 Epoch [54/100], Train Loss: 0.1270, Train Acc: 100.00%, Val Loss: 0.1768, Val Acc: 100.00%\n",
      "Fold 5 Epoch [55/100], Train Loss: 0.1189, Train Acc: 100.00%, Val Loss: 0.1662, Val Acc: 100.00%\n",
      "Fold 5 Epoch [56/100], Train Loss: 0.1114, Train Acc: 100.00%, Val Loss: 0.1562, Val Acc: 100.00%\n",
      "Fold 5 Epoch [57/100], Train Loss: 0.1042, Train Acc: 100.00%, Val Loss: 0.1463, Val Acc: 100.00%\n",
      "Fold 5 Epoch [58/100], Train Loss: 0.0975, Train Acc: 100.00%, Val Loss: 0.1373, Val Acc: 100.00%\n",
      "Fold 5 Epoch [59/100], Train Loss: 0.0911, Train Acc: 100.00%, Val Loss: 0.1288, Val Acc: 100.00%\n",
      "Fold 5 Epoch [60/100], Train Loss: 0.0851, Train Acc: 100.00%, Val Loss: 0.1208, Val Acc: 100.00%\n",
      "Fold 5 Epoch [61/100], Train Loss: 0.0795, Train Acc: 100.00%, Val Loss: 0.1131, Val Acc: 100.00%\n",
      "Fold 5 Epoch [62/100], Train Loss: 0.0742, Train Acc: 100.00%, Val Loss: 0.1058, Val Acc: 100.00%\n",
      "Fold 5 Epoch [63/100], Train Loss: 0.0692, Train Acc: 100.00%, Val Loss: 0.0991, Val Acc: 100.00%\n",
      "Fold 5 Epoch [64/100], Train Loss: 0.0647, Train Acc: 100.00%, Val Loss: 0.0930, Val Acc: 100.00%\n",
      "Fold 5 Epoch [65/100], Train Loss: 0.0605, Train Acc: 100.00%, Val Loss: 0.0871, Val Acc: 100.00%\n",
      "Fold 5 Epoch [66/100], Train Loss: 0.0565, Train Acc: 100.00%, Val Loss: 0.0818, Val Acc: 100.00%\n",
      "Fold 5 Epoch [67/100], Train Loss: 0.0529, Train Acc: 100.00%, Val Loss: 0.0768, Val Acc: 100.00%\n",
      "Fold 5 Epoch [68/100], Train Loss: 0.0496, Train Acc: 100.00%, Val Loss: 0.0722, Val Acc: 100.00%\n",
      "Fold 5 Epoch [69/100], Train Loss: 0.0465, Train Acc: 100.00%, Val Loss: 0.0678, Val Acc: 100.00%\n",
      "Fold 5 Epoch [70/100], Train Loss: 0.0436, Train Acc: 100.00%, Val Loss: 0.0639, Val Acc: 100.00%\n",
      "Fold 5 Epoch [71/100], Train Loss: 0.0409, Train Acc: 100.00%, Val Loss: 0.0599, Val Acc: 100.00%\n",
      "Fold 5 Epoch [72/100], Train Loss: 0.0384, Train Acc: 100.00%, Val Loss: 0.0562, Val Acc: 100.00%\n",
      "Fold 5 Epoch [73/100], Train Loss: 0.0360, Train Acc: 100.00%, Val Loss: 0.0525, Val Acc: 100.00%\n",
      "Fold 5 Epoch [74/100], Train Loss: 0.0338, Train Acc: 100.00%, Val Loss: 0.0489, Val Acc: 100.00%\n",
      "Fold 5 Epoch [75/100], Train Loss: 0.0318, Train Acc: 100.00%, Val Loss: 0.0457, Val Acc: 100.00%\n",
      "Fold 5 Epoch [76/100], Train Loss: 0.0299, Train Acc: 100.00%, Val Loss: 0.0427, Val Acc: 100.00%\n",
      "Fold 5 Epoch [77/100], Train Loss: 0.0281, Train Acc: 100.00%, Val Loss: 0.0401, Val Acc: 100.00%\n",
      "Fold 5 Epoch [78/100], Train Loss: 0.0265, Train Acc: 100.00%, Val Loss: 0.0377, Val Acc: 100.00%\n",
      "Fold 5 Epoch [79/100], Train Loss: 0.0250, Train Acc: 100.00%, Val Loss: 0.0353, Val Acc: 100.00%\n",
      "Fold 5 Epoch [80/100], Train Loss: 0.0237, Train Acc: 100.00%, Val Loss: 0.0332, Val Acc: 100.00%\n",
      "Fold 5 Epoch [81/100], Train Loss: 0.0224, Train Acc: 100.00%, Val Loss: 0.0313, Val Acc: 100.00%\n",
      "Fold 5 Epoch [82/100], Train Loss: 0.0212, Train Acc: 100.00%, Val Loss: 0.0294, Val Acc: 100.00%\n",
      "Fold 5 Epoch [83/100], Train Loss: 0.0201, Train Acc: 100.00%, Val Loss: 0.0277, Val Acc: 100.00%\n",
      "Fold 5 Epoch [84/100], Train Loss: 0.0190, Train Acc: 100.00%, Val Loss: 0.0261, Val Acc: 100.00%\n",
      "Fold 5 Epoch [85/100], Train Loss: 0.0180, Train Acc: 100.00%, Val Loss: 0.0246, Val Acc: 100.00%\n",
      "Fold 5 Epoch [86/100], Train Loss: 0.0170, Train Acc: 100.00%, Val Loss: 0.0232, Val Acc: 100.00%\n",
      "Fold 5 Epoch [87/100], Train Loss: 0.0161, Train Acc: 100.00%, Val Loss: 0.0219, Val Acc: 100.00%\n",
      "Fold 5 Epoch [88/100], Train Loss: 0.0153, Train Acc: 100.00%, Val Loss: 0.0206, Val Acc: 100.00%\n",
      "Fold 5 Epoch [89/100], Train Loss: 0.0145, Train Acc: 100.00%, Val Loss: 0.0194, Val Acc: 100.00%\n",
      "Fold 5 Epoch [90/100], Train Loss: 0.0138, Train Acc: 100.00%, Val Loss: 0.0183, Val Acc: 100.00%\n",
      "Fold 5 Epoch [91/100], Train Loss: 0.0131, Train Acc: 100.00%, Val Loss: 0.0173, Val Acc: 100.00%\n",
      "Fold 5 Epoch [92/100], Train Loss: 0.0125, Train Acc: 100.00%, Val Loss: 0.0164, Val Acc: 100.00%\n",
      "Fold 5 Epoch [93/100], Train Loss: 0.0119, Train Acc: 100.00%, Val Loss: 0.0156, Val Acc: 100.00%\n",
      "Fold 5 Epoch [94/100], Train Loss: 0.0113, Train Acc: 100.00%, Val Loss: 0.0148, Val Acc: 100.00%\n",
      "Fold 5 Epoch [95/100], Train Loss: 0.0108, Train Acc: 100.00%, Val Loss: 0.0141, Val Acc: 100.00%\n",
      "Fold 5 Epoch [96/100], Train Loss: 0.0103, Train Acc: 100.00%, Val Loss: 0.0136, Val Acc: 100.00%\n",
      "Fold 5 Epoch [97/100], Train Loss: 0.0098, Train Acc: 100.00%, Val Loss: 0.0131, Val Acc: 100.00%\n",
      "Fold 5 Epoch [98/100], Train Loss: 0.0094, Train Acc: 100.00%, Val Loss: 0.0125, Val Acc: 100.00%\n",
      "Fold 5 Epoch [99/100], Train Loss: 0.0090, Train Acc: 100.00%, Val Loss: 0.0120, Val Acc: 100.00%\n",
      "Fold 5 Epoch [100/100], Train Loss: 0.0086, Train Acc: 100.00%, Val Loss: 0.0114, Val Acc: 100.00%\n",
      "Fold 5 Best Accuracy: 100.00%\n",
      "\n",
      "5-Fold CV Results (Stephen Only):\n",
      "Average Accuracy: 97.50% (5.00)\n",
      "Fold Accuracies: ['87.50%', '100.00%', '100.00%', '100.00%', '100.00%']\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation setup\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "fold_losses = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_all)):\n",
    "    print(f'Fold {fold + 1}/{n_folds}')\n",
    "    \n",
    "    # Split data\n",
    "    X_train_fold = X_all[train_idx]\n",
    "    y_train_fold = y_all[train_idx]\n",
    "    X_val_fold = X_all[val_idx]\n",
    "    y_val_fold = y_all[val_idx]\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_train_fold, y_train_fold)\n",
    "    val_dataset = TensorDataset(X_val_fold, y_val_fold)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = FeatureModel(num_classes=NUM_CLASSES, input_features=18)\n",
    "    optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-4)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_accuracy = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        train_loss, train_accuracy = evaluate_model(train_loader, model, criterion)\n",
    "        val_loss, val_accuracy = evaluate_model(val_loader, model, criterion)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Log and print\n",
    "        log_message = (f\"Fold {fold+1} Epoch [{epoch+1}/100], \"\n",
    "                      f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
    "                      f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "        logging.info(log_message)\n",
    "        print(log_message)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_val_loss = val_loss\n",
    "            save_best_model(epoch, model, optimizer, val_loss, val_accuracy, train_losses, val_losses)\n",
    "    \n",
    "    # Store fold results\n",
    "    fold_accuracies.append(best_val_accuracy)\n",
    "    fold_losses.append(best_val_loss)\n",
    "    print(f'Fold {fold + 1} Best Accuracy: {best_val_accuracy:.2f}%')\n",
    "    \n",
    "# Print summary\n",
    "mean_accuracy = np.mean(fold_accuracies)\n",
    "std_accuracy = np.std(fold_accuracies)\n",
    "print(f'\\n5-Fold CV Results (Stephen Only):')\n",
    "print(f'Average Accuracy: {mean_accuracy:.2f}% ({std_accuracy:.2f})')\n",
    "print(f'Fold Accuracies: {[f\"{acc:.2f}%\" for acc in fold_accuracies]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37f5a0a4-374a-40dd-9d27-81279b433394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSTM Results Summary:\n",
      " Chunk Size (s)  Accuracy (%)\n",
      "          0.250     56.060606\n",
      "          0.500     86.666667\n",
      "          1.000     66.000000\n",
      "          1.375     74.166667\n",
      "          2.000     76.666667\n",
      "          2.250     55.000000\n",
      "          2.500     73.333333\n",
      "          2.750     48.333333\n",
      "          3.000     33.333333\n",
      "          3.250     30.000000\n",
      "          3.500     70.000000\n",
      "          3.750     30.000000\n",
      "          4.000     43.333333\n",
      "          5.000     40.000000\n"
     ]
    }
   ],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(chunk_sizes, accuracies, 'bo-')\n",
    "plt.xlabel('Chunk Size (seconds)')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.title('LSTM Model Accuracy vs Chunk Size')\n",
    "plt.grid(True)\n",
    "\n",
    "# Add accuracy values as text\n",
    "for i, (x, y) in enumerate(zip(chunk_sizes, accuracies)):\n",
    "    plt.text(x, y, f'{y:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.savefig('lstm_chunk_size_vs_accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "# Print results table\n",
    "results = pd.DataFrame({\n",
    "    'Chunk Size (s)': chunk_sizes,\n",
    "    'Accuracy (%)': accuracies\n",
    "})\n",
    "print(\"\\nLSTM Results Summary:\")\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c79cb82-164f-4ed0-ba0b-63dcc6f6bbc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 3, got 237",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m---> 16\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m         _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     18\u001b[0m         all_predictions\u001b[38;5;241m.\u001b[39mextend(predicted\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[15], line 23\u001b[0m, in \u001b[0;36mOptimizedLSTMModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Pass through LSTM\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Use the output of the last timestep for classification\u001b[39;00m\n\u001b[0;32m     26\u001b[0m out \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Take the last hidden state\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:1119\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1116\u001b[0m             hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m   1117\u001b[0m         \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m         \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m-> 1119\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:1000\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_forward_args\u001b[39m(\n\u001b[0;32m    995\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    997\u001b[0m     hidden: Tuple[Tensor, Tensor],  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    998\u001b[0m     batch_sizes: Optional[Tensor],\n\u001b[0;32m    999\u001b[0m ):\n\u001b[1;32m-> 1000\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1001\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(\n\u001b[0;32m   1002\u001b[0m         hidden[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1003\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m   1004\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1005\u001b[0m     )\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(\n\u001b[0;32m   1007\u001b[0m         hidden[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   1008\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m   1009\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1010\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:312\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    310\u001b[0m     )\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 3, got 237"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Activities for the Confusion matrix\n",
    "ACTIVITIES = ['sit', 'walk','upstair']\n",
    "\n",
    "# Model evaluation (confusion matrix)\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Loop through the test loader to collect predictions and true labels\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Compute the confusion matrix, explicitly specifying the labels\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions, labels=[0, 1, 2]) \n",
    "\n",
    "# Assuming conf_matrix and ACTIVITIES are already defined\n",
    "class_accuracies = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)  # Compute per-class accuracy\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(conf_matrix, display_labels=ACTIVITIES)\n",
    "fig, ax = plt.subplots()\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "\n",
    "# Add per-class accuracy text\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "    acc_text = f\"{class_accuracies[i] * 100:.2f}%\"\n",
    "    ax.text(\n",
    "        len(ACTIVITIES) + 0.3, i, acc_text, \n",
    "        fontsize=12, verticalalignment='center', color='black'\n",
    "    )\n",
    "\n",
    "plt.title(\"Confusion Matrix for Activities with Per-Class Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f841cd-23c7-425c-a8c0-6e363b2c94f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label: 1, Predicted: 2, Confidence: 0.5379\n",
      "Full Probabilities: [0.04630952328443527, 0.40478968620300293, 0.5378606915473938, 0.0032725243363529444, 0.0037186944391578436, 0.0018561289180070162, 0.0015050942311063409, 0.0006875882390886545]\n",
      "True Label: 1, Predicted: 2, Confidence: 0.6603\n",
      "Full Probabilities: [0.012942157685756683, 0.3238711655139923, 0.6603409051895142, 0.0006900696316733956, 0.0012600854970514774, 0.00047482800437137485, 0.0003185397945344448, 0.00010225101141259074]\n",
      "True Label: 2, Predicted: 1, Confidence: 0.5808\n",
      "Full Probabilities: [0.002849593525752425, 0.5807549357414246, 0.4157315492630005, 0.00019912587595172226, 0.000295086792903021, 0.00010057010513264686, 5.421382957138121e-05, 1.4962002751417458e-05]\n",
      "True Label: 2, Predicted: 1, Confidence: 0.5663\n",
      "Full Probabilities: [0.004601947031915188, 0.5663013458251953, 0.42831793427467346, 0.0002489405160304159, 0.0003278449294157326, 0.00011758197069866583, 6.75682895234786e-05, 1.6763236999395303e-05]\n",
      "True Label: 2, Predicted: 1, Confidence: 0.5180\n",
      "Full Probabilities: [0.020252525806427002, 0.5179758667945862, 0.4608774185180664, 0.0002581731241662055, 0.00039337240741588175, 0.0001394125574734062, 8.091720519587398e-05, 2.237147418782115e-05]\n",
      "True Label: 0, Predicted: 1, Confidence: 0.7273\n",
      "Full Probabilities: [0.24202822148799896, 0.7272588014602661, 0.01800217106938362, 0.002684463746845722, 0.003459376050159335, 0.0026517054066061974, 0.0024689871352165937, 0.0014463680563494563]\n",
      "True Label: 1, Predicted: 2, Confidence: 0.5364\n",
      "Full Probabilities: [0.0010767356725409627, 0.461995929479599, 0.5364068150520325, 0.0001329303195234388, 0.00027212704299017787, 7.19023373676464e-05, 3.5536650102585554e-05, 8.04939008958172e-06]\n",
      "True Label: 1, Predicted: 2, Confidence: 0.6007\n",
      "Full Probabilities: [0.0015494574327021837, 0.39651644229888916, 0.6007441282272339, 0.000301126652630046, 0.0005974904634058475, 0.00017378314805682749, 9.326271538157016e-05, 2.4287490305141546e-05]\n",
      "True Label: 1, Predicted: 2, Confidence: 0.8370\n",
      "Full Probabilities: [0.0041362508200109005, 0.15798449516296387, 0.8370068073272705, 0.0001938036730280146, 0.00045545579632744193, 0.00012768110900651664, 7.822032057447359e-05, 1.728711140458472e-05]\n",
      "True Label: 1, Predicted: 2, Confidence: 0.7846\n",
      "Full Probabilities: [0.0027064322493970394, 0.2118576020002365, 0.7845824360847473, 0.00020613192464224994, 0.0004299695719964802, 0.0001244906452484429, 7.59389586164616e-05, 1.701954533928074e-05]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "incorrect_samples = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)  # Get raw logits\n",
    "        probabilities = F.softmax(outputs, dim=1)  # Convert to probabilities\n",
    "        predicted_labels = torch.argmax(probabilities, dim=1)  # Get predicted class\n",
    "        confidence_scores = torch.max(probabilities, dim=1).values  # Get max confidence\n",
    "\n",
    "        # Find incorrect predictions\n",
    "        incorrect_indices = (predicted_labels != batch_y)\n",
    "        for i in range(len(batch_y)):\n",
    "            if incorrect_indices[i]:\n",
    "                incorrect_samples.append({\n",
    "                    \"True Label\": batch_y[i].item(),\n",
    "                    \"Predicted Label\": predicted_labels[i].item(),\n",
    "                    \"Confidence\": confidence_scores[i].item(),\n",
    "                    \"Probabilities\": probabilities[i].tolist()\n",
    "                })\n",
    "\n",
    "# Print results\n",
    "for sample in incorrect_samples:\n",
    "    print(f\"True Label: {sample['True Label']}, Predicted: {sample['Predicted Label']}, Confidence: {sample['Confidence']:.4f}\")\n",
    "    print(f\"Full Probabilities: {sample['Probabilities']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
