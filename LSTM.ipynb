{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "30e998e0-f6cd-4d3d-82f0-08637032af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from firebase_admin import credentials, firestore\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Firebase Initialization\n",
    "cred = credentials.Certificate(\"adminkey.json\")\n",
    "#firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename='training_log.txt', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "INCLUDE_ONLY = ['Stephen', 'Lillian', 'Ren', 'Yuanheng', 'Ethan Shao']\n",
    "ACTIVITIES = ['sit','walk','upstair']\n",
    "CHUNK_SIZE = 2.375  # in seconds (can be a decimal)\n",
    "SAMPLING_RATE = 100  # Hz\n",
    "NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f933bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "OVERLAP = 0.25  # Fix for previous NameError\n",
    "\n",
    "# Define feature extraction function\n",
    "def extract_features(chunk):\n",
    "    \"\"\"Extract features from a chunked acceleration segment with selected statistics.\"\"\"\n",
    "    feature_vector = []\n",
    "    \n",
    "    for axis in ['x', 'y', 'z']:\n",
    "        data_series = pd.Series(chunk[axis])\n",
    "        # Apply smoothing\n",
    "        smoothed_data = data_series.rolling(window=5, min_periods=1).mean()\n",
    "        feature_vector.extend([\n",
    "            smoothed_data.mean(),                  # Mean\n",
    "            smoothed_data.median(),                # Median\n",
    "            smoothed_data.std(),                   # Standard deviation\n",
    "            smoothed_data.var(),                   # Variance\n",
    "            smoothed_data.min(),                   # Minimum\n",
    "            smoothed_data.max(),                   # Maximum\n",
    "        ])\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "287ad27b-8007-4ccd-b113-f8016c2aaec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data \n",
    "def fetch_data_for_stephen(collection_name, activities, time_start=500, time_end=6000):\n",
    "    data, docs = [], []\n",
    "    person_name = \"Stephen\"\n",
    "    \n",
    "    for activity in activities:\n",
    "        for recording in db.collection(collection_name).document(person_name).collection(activity).stream():\n",
    "            record = recording.to_dict()\n",
    "            if 'acceleration' not in record:\n",
    "                continue\n",
    "\n",
    "            docs.append(record)\n",
    "            df = pd.DataFrame(record['acceleration'])\n",
    "            \n",
    "            if 'time' in df.columns:\n",
    "                filtered_df = df[(df['time'] >= time_start) & (df['time'] <= time_end)]\n",
    "                data.append(filtered_df)\n",
    "            else:\n",
    "                raise ValueError(\"The 'acceleration' field must include a 'time' column.\")\n",
    "    return data, docs\n",
    "\n",
    "# Fetch data\n",
    "training_data_raw, training_docs = fetch_data_for_stephen(\"training\", ACTIVITIES)\n",
    "testing_data_raw, testing_docs = fetch_data_for_stephen(\"testing\", ACTIVITIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a1284a61-2522-4348-abbf-1d4e31e36bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chunking with overlap\n",
    "def chunk_data_with_overlap(data_raw, docs, chunk_size, activities, sampling_rate, overlap=0.5):\n",
    "    \"\"\"Chunk raw acceleration data into smaller labeled segments using overlapping windows.\"\"\"\n",
    "    data, labels = [], []\n",
    "    chunk_samples = int(chunk_size * sampling_rate)\n",
    "    step = int(chunk_samples * (1 - overlap))  # Compute step size based on overlap\n",
    "\n",
    "    for i, df in enumerate(data_raw):\n",
    "        # Slide over the data with the defined step\n",
    "        for start in range(0, len(df) - chunk_samples + 1, step):\n",
    "            end = start + chunk_samples     \n",
    "            chunk = df.iloc[start:end]\n",
    "            activity = docs[i]['activity']\n",
    "            label = activities.index(activity)\n",
    "            data.append(extract_features(chunk))\n",
    "            labels.append(label)\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "43de7965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset summary:\n",
      "+----------+------------------+\n",
      "| Dataset  | number of chunks |\n",
      "+----------+------------------+\n",
      "| training |        6         |\n",
      "| testing  |        6         |\n",
      "+----------+------------------+\n",
      "Training Activities Count\n",
      "sit: 20 chunks\n",
      "walk: 20 chunks\n",
      "upstair: 20 chunks\n",
      "\n",
      "Testing Activity Count\n",
      "sit:20 chunks\n",
      "walk:20 chunks\n",
      "upstair:20 chunks\n",
      "6\n",
      "6\n",
      "(150, 3, 100)\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate #for table formatting\n",
    "\n",
    "#Calculate the number of training and testing samples\n",
    "num_training_samples = len(training_data_raw)\n",
    "num_testing_samples = len(testing_data_raw)\n",
    "\n",
    "\n",
    "#table\n",
    "summary_table = [[\"training\", num_training_samples], [\"testing\", num_testing_samples]]\n",
    "\n",
    "#print\n",
    "print(\"dataset summary:\")\n",
    "print(tabulate(summary_table, headers = [\"Dataset\", \"number of chunks\"], tablefmt=\"pretty\"))\n",
    "\n",
    "print(\"Training Activities Count\")\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "    print(f\"{activity}: {int(training_distribution[i])} chunks\")\n",
    "\n",
    "print(\"\\nTesting Activity Count\")\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "    print(f\"{activity}:{int(testing_distribution[i])} chunks\")\n",
    "print(len(training_data_raw))\n",
    "print(len(testing_data_raw))\n",
    "print(np.array(training_data).shape)\n",
    "print(len(training_data_raw))\n",
    "print(len(testing_data_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fae1da3e-3e2c-40ad-82cb-6d090f2cc7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features using chunk_data_with_overlap\n",
    "X_train, y_train = chunk_data_with_overlap(training_data_raw, training_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE, OVERLAP)\n",
    "X_test, y_test = chunk_data_with_overlap(testing_data_raw, testing_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE, OVERLAP)\n",
    "\n",
    "# Combine train and test for cross-validation (Stephen's data only)\n",
    "X_all = np.concatenate([X_train, X_test], axis=0)\n",
    "y_all = np.concatenate([y_train, y_test], axis=0)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_all = scaler.fit_transform(X_all)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_all = torch.tensor(X_all, dtype=torch.float32)  # Shape: (n_samples, 18)\n",
    "y_all = torch.tensor(y_all, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7ecb31f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' class OptimizedLSTMModel(nn.Module):\\n    def __init__(self, num_classes, input_channels, seq_length, hidden_size=128, num_layers=2):\\n        super(OptimizedLSTMModel, self).__init__()\\n        self.seq_length = seq_length\\n        self.hidden_size = hidden_size\\n        self.num_layers = num_layers\\n        \\n        # LSTM layer expects input of shape (batch_size, seq_length, input_size)\\n        self.lstm = nn.LSTM(input_channels, hidden_size, num_layers, batch_first=True)\\n        \\n        # Fully connected layer to output predictions\\n        self.fc = nn.Linear(hidden_size, num_classes)\\n        \\n        # Dropout layer for regularization\\n        self.dropout = nn.Dropout(0.3)\\n\\n    def forward(self, x):\\n        # LSTM requires a hidden state and cell state to be initialized\\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\\n\\n        # Pass through LSTM\\n        out, _ = self.lstm(x, (h0, c0))\\n\\n        # Use the output of the last timestep for classification\\n        out = out[:, -1, :]  # Take the last hidden state\\n        out = self.dropout(out)\\n        \\n        # Pass through the fully connected layer\\n        out = self.fc(out)\\n        return out\\n'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' class OptimizedLSTMModel(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels, seq_length, hidden_size=128, num_layers=2):\n",
    "        super(OptimizedLSTMModel, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer expects input of shape (batch_size, seq_length, input_size)\n",
    "        self.lstm = nn.LSTM(input_channels, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to output predictions\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM requires a hidden state and cell state to be initialized\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Use the output of the last timestep for classification\n",
    "        out = out[:, -1, :]  # Take the last hidden state\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3ec68f8d-85ff-48e1-b219-a547e9393c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for extracted features (MLP)\n",
    "class FeatureModel(nn.Module):\n",
    "    def __init__(self, num_classes, input_features=18):\n",
    "        super(FeatureModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_features, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2fea2d82-2721-4226-87c8-d0386cb60e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save best model and metadata\n",
    "BEST_MODEL_PATH = \"best_model.pth\"\n",
    "BEST_METADATA_PATH = \"best_model.json\"\n",
    "\n",
    "# Corrected save_best_model function\n",
    "def save_best_model(epoch, model, optimizer, loss, accuracy, train_losses, val_losses):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "    }\n",
    "    torch.save(checkpoint, BEST_MODEL_PATH)\n",
    "\n",
    "    # Save metadata with correct variable names\n",
    "    with open(BEST_METADATA_PATH, \"w\") as f:\n",
    "        json.dump({\"epoch\": epoch, \"val_loss\": loss, \"val_accuracy\": accuracy}, f)\n",
    "\n",
    "# Function to load best model if exists\n",
    "def load_best_model(model, optimizer, best_model_path=BEST_MODEL_PATH):\n",
    "    if os.path.exists(best_model_path):\n",
    "        checkpoint = torch.load(best_model_path)\n",
    "        \n",
    "        # Print checkpoint keys to understand its structure\n",
    "        print(\"Checkpoint keys:\", checkpoint.keys())\n",
    "        \n",
    "        # Load model weights\n",
    "        model_state_dict = model.state_dict()\n",
    "        checkpoint_state_dict = checkpoint['model_state_dict']\n",
    "        \n",
    "        # Filter out incompatible keys\n",
    "        filtered_checkpoint_state_dict = {k: v for k, v in checkpoint_state_dict.items() if k in model_state_dict}\n",
    "        model_state_dict.update(filtered_checkpoint_state_dict)\n",
    "        model.load_state_dict(model_state_dict)\n",
    "        \n",
    "        # Load other metadata if available\n",
    "        start_epoch = checkpoint.get('epoch', 0)\n",
    "        best_loss = checkpoint.get('loss', float('inf'))  # Default to infinity if loss is missing\n",
    "        best_avg_accuracy = checkpoint.get('accuracy', 0)  # Default to 0 if accuracy is missing\n",
    "        \n",
    "        # If train_losses and test_losses aren't saved, return empty lists or placeholders\n",
    "        train_losses = checkpoint.get('train_losses', [])\n",
    "        test_losses = checkpoint.get('test_losses', [])\n",
    "        \n",
    "        return start_epoch, best_loss, best_avg_accuracy, train_losses, test_losses\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting from scratch.\")\n",
    "        # Return default values\n",
    "        return 0, float('inf'), 0, [], []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "afb6cc02-f6d5-441e-8ea1-736efc276305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model (no transpose for MLP)\n",
    "def evaluate_model(loader, model, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "01254ebf-d89d-4173-9f04-ba08155090cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Data Augmentation Functions\n",
    "def add_jitter(batch_X, noise_level=0.01):\n",
    "    noise = torch.randn_like(batch_X) * noise_level\n",
    "    return batch_X + noise\n",
    "\n",
    "def scale_signal(batch_X, scale_range=(0.9, 1.1)):\n",
    "    scale_factor = torch.FloatTensor(batch_X.shape[0], 1, 1).uniform_(*scale_range)\n",
    "    return batch_X * scale_factor\n",
    "\n",
    "def time_warp(batch_X, sigma=0.2):\n",
    "    batch_size, num_channels, seq_length = batch_X.shape\n",
    "    time_steps = np.arange(seq_length)\n",
    "    warping_curve = np.cumsum(np.random.normal(0, sigma, size=(batch_size, seq_length)), axis=1)\n",
    "    warping_curve = (warping_curve - warping_curve.min(axis=1, keepdims=True)) / \\\n",
    "                    (warping_curve.max(axis=1, keepdims=True) - warping_curve.min(axis=1, keepdims=True)) * seq_length\n",
    "    warped_X = torch.zeros_like(batch_X)\n",
    "    for i in range(batch_size):\n",
    "        for j in range(num_channels):\n",
    "            f = interp1d(time_steps, batch_X[i, j, :].cpu().numpy(), kind='linear', fill_value='extrapolate')\n",
    "            warped_X[i, j, :] = torch.tensor(f(warping_curve[i]), dtype=batch_X.dtype)\n",
    "    return warped_X\n",
    "\n",
    "def random_crop(batch_X, crop_size=0.9, target_length=None):\n",
    "    batch_size, num_channels, seq_length = batch_X.shape\n",
    "    new_length = int(seq_length * crop_size)\n",
    "    start_idx = torch.randint(0, seq_length - new_length + 1, (batch_size,))\n",
    "    cropped_X = torch.zeros(batch_size, num_channels, new_length)\n",
    "    for i in range(batch_size):\n",
    "        cropped_X[i] = batch_X[i, :, start_idx[i]:start_idx[i] + new_length]\n",
    "    \n",
    "    if target_length is not None:\n",
    "        if new_length < target_length:\n",
    "            padding = torch.zeros(batch_size, num_channels, target_length - new_length)\n",
    "            cropped_X = torch.cat([cropped_X, padding], dim=2)\n",
    "        elif new_length > target_length:\n",
    "            cropped_X = cropped_X[:, :, :target_length]\n",
    "    return cropped_X\n",
    "\n",
    "def permute_segments(batch_X, num_segments=5):\n",
    "    batch_size, num_channels, seq_length = batch_X.shape\n",
    "    segment_length = seq_length // num_segments\n",
    "    permuted_X = batch_X.clone()\n",
    "    for i in range(batch_size):\n",
    "        perm = torch.randperm(num_segments)\n",
    "        for j in range(num_segments):\n",
    "            permuted_X[i, :, j * segment_length:(j + 1) * segment_length] = \\\n",
    "                batch_X[i, :, perm[j] * segment_length:(perm[j] + 1) * segment_length]\n",
    "    return permuted_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "fc68f1f4-225f-455c-b6d0-80a76a3ce598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint keys: dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss', 'accuracy', 'train_losses', 'val_losses'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minsu\\AppData\\Local\\Temp\\ipykernel_21472\\2522633474.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(best_model_path)\n"
     ]
    }
   ],
   "source": [
    "model = FeatureModel(num_classes=NUM_CLASSES, input_features=18)\n",
    "optimizer = Adam(model.parameters(), lr=0.02, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50, min_lr=1e-5)\n",
    "\n",
    "start_epoch, best_loss, best_avg_accuracy, train_losses, test_losses = load_best_model(model, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "fd628c08-376b-468a-ab5f-6c1a88c7587c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Fold 1 Epoch [1/100], Train Loss: 1.0754, Train Acc: 36.84%, Val Loss: 1.1183, Val Acc: 20.00%\n",
      "Fold 1 Epoch [2/100], Train Loss: 1.0575, Train Acc: 36.84%, Val Loss: 1.1015, Val Acc: 20.00%\n",
      "Fold 1 Epoch [3/100], Train Loss: 1.0395, Train Acc: 36.84%, Val Loss: 1.0846, Val Acc: 20.00%\n",
      "Fold 1 Epoch [4/100], Train Loss: 1.0218, Train Acc: 36.84%, Val Loss: 1.0679, Val Acc: 20.00%\n",
      "Fold 1 Epoch [5/100], Train Loss: 1.0044, Train Acc: 36.84%, Val Loss: 1.0515, Val Acc: 20.00%\n",
      "Fold 1 Epoch [6/100], Train Loss: 0.9872, Train Acc: 36.84%, Val Loss: 1.0355, Val Acc: 20.00%\n",
      "Fold 1 Epoch [7/100], Train Loss: 0.9699, Train Acc: 36.84%, Val Loss: 1.0194, Val Acc: 20.00%\n",
      "Fold 1 Epoch [8/100], Train Loss: 0.9527, Train Acc: 36.84%, Val Loss: 1.0034, Val Acc: 20.00%\n",
      "Fold 1 Epoch [9/100], Train Loss: 0.9353, Train Acc: 36.84%, Val Loss: 0.9870, Val Acc: 40.00%\n",
      "Fold 1 Epoch [10/100], Train Loss: 0.9179, Train Acc: 36.84%, Val Loss: 0.9703, Val Acc: 40.00%\n",
      "Fold 1 Epoch [11/100], Train Loss: 0.9002, Train Acc: 47.37%, Val Loss: 0.9532, Val Acc: 40.00%\n",
      "Fold 1 Epoch [12/100], Train Loss: 0.8826, Train Acc: 68.42%, Val Loss: 0.9360, Val Acc: 80.00%\n",
      "Fold 1 Epoch [13/100], Train Loss: 0.8645, Train Acc: 78.95%, Val Loss: 0.9184, Val Acc: 80.00%\n",
      "Fold 1 Epoch [14/100], Train Loss: 0.8460, Train Acc: 94.74%, Val Loss: 0.9005, Val Acc: 100.00%\n",
      "Fold 1 Epoch [15/100], Train Loss: 0.8271, Train Acc: 94.74%, Val Loss: 0.8824, Val Acc: 100.00%\n",
      "Fold 1 Epoch [16/100], Train Loss: 0.8079, Train Acc: 94.74%, Val Loss: 0.8645, Val Acc: 100.00%\n",
      "Fold 1 Epoch [17/100], Train Loss: 0.7883, Train Acc: 94.74%, Val Loss: 0.8464, Val Acc: 100.00%\n",
      "Fold 1 Epoch [18/100], Train Loss: 0.7688, Train Acc: 100.00%, Val Loss: 0.8280, Val Acc: 100.00%\n",
      "Fold 1 Epoch [19/100], Train Loss: 0.7493, Train Acc: 100.00%, Val Loss: 0.8093, Val Acc: 100.00%\n",
      "Fold 1 Epoch [20/100], Train Loss: 0.7293, Train Acc: 100.00%, Val Loss: 0.7902, Val Acc: 100.00%\n",
      "Fold 1 Epoch [21/100], Train Loss: 0.7092, Train Acc: 100.00%, Val Loss: 0.7707, Val Acc: 100.00%\n",
      "Fold 1 Epoch [22/100], Train Loss: 0.6889, Train Acc: 100.00%, Val Loss: 0.7511, Val Acc: 100.00%\n",
      "Fold 1 Epoch [23/100], Train Loss: 0.6682, Train Acc: 100.00%, Val Loss: 0.7310, Val Acc: 100.00%\n",
      "Fold 1 Epoch [24/100], Train Loss: 0.6472, Train Acc: 100.00%, Val Loss: 0.7106, Val Acc: 100.00%\n",
      "Fold 1 Epoch [25/100], Train Loss: 0.6264, Train Acc: 100.00%, Val Loss: 0.6904, Val Acc: 100.00%\n",
      "Fold 1 Epoch [26/100], Train Loss: 0.6054, Train Acc: 100.00%, Val Loss: 0.6699, Val Acc: 100.00%\n",
      "Fold 1 Epoch [27/100], Train Loss: 0.5843, Train Acc: 100.00%, Val Loss: 0.6492, Val Acc: 100.00%\n",
      "Fold 1 Epoch [28/100], Train Loss: 0.5631, Train Acc: 100.00%, Val Loss: 0.6284, Val Acc: 100.00%\n",
      "Fold 1 Epoch [29/100], Train Loss: 0.5419, Train Acc: 100.00%, Val Loss: 0.6078, Val Acc: 100.00%\n",
      "Fold 1 Epoch [30/100], Train Loss: 0.5206, Train Acc: 100.00%, Val Loss: 0.5872, Val Acc: 100.00%\n",
      "Fold 1 Epoch [31/100], Train Loss: 0.4994, Train Acc: 100.00%, Val Loss: 0.5667, Val Acc: 100.00%\n",
      "Fold 1 Epoch [32/100], Train Loss: 0.4784, Train Acc: 100.00%, Val Loss: 0.5463, Val Acc: 100.00%\n",
      "Fold 1 Epoch [33/100], Train Loss: 0.4576, Train Acc: 100.00%, Val Loss: 0.5261, Val Acc: 100.00%\n",
      "Fold 1 Epoch [34/100], Train Loss: 0.4371, Train Acc: 100.00%, Val Loss: 0.5060, Val Acc: 100.00%\n",
      "Fold 1 Epoch [35/100], Train Loss: 0.4168, Train Acc: 100.00%, Val Loss: 0.4861, Val Acc: 100.00%\n",
      "Fold 1 Epoch [36/100], Train Loss: 0.3969, Train Acc: 100.00%, Val Loss: 0.4664, Val Acc: 100.00%\n",
      "Fold 1 Epoch [37/100], Train Loss: 0.3774, Train Acc: 100.00%, Val Loss: 0.4466, Val Acc: 100.00%\n",
      "Fold 1 Epoch [38/100], Train Loss: 0.3584, Train Acc: 100.00%, Val Loss: 0.4272, Val Acc: 100.00%\n",
      "Fold 1 Epoch [39/100], Train Loss: 0.3399, Train Acc: 100.00%, Val Loss: 0.4081, Val Acc: 100.00%\n",
      "Fold 1 Epoch [40/100], Train Loss: 0.3220, Train Acc: 100.00%, Val Loss: 0.3895, Val Acc: 100.00%\n",
      "Fold 1 Epoch [41/100], Train Loss: 0.3046, Train Acc: 100.00%, Val Loss: 0.3713, Val Acc: 100.00%\n",
      "Fold 1 Epoch [42/100], Train Loss: 0.2877, Train Acc: 100.00%, Val Loss: 0.3533, Val Acc: 100.00%\n",
      "Fold 1 Epoch [43/100], Train Loss: 0.2714, Train Acc: 100.00%, Val Loss: 0.3358, Val Acc: 100.00%\n",
      "Fold 1 Epoch [44/100], Train Loss: 0.2557, Train Acc: 100.00%, Val Loss: 0.3186, Val Acc: 100.00%\n",
      "Fold 1 Epoch [45/100], Train Loss: 0.2406, Train Acc: 100.00%, Val Loss: 0.3020, Val Acc: 100.00%\n",
      "Fold 1 Epoch [46/100], Train Loss: 0.2261, Train Acc: 100.00%, Val Loss: 0.2859, Val Acc: 100.00%\n",
      "Fold 1 Epoch [47/100], Train Loss: 0.2122, Train Acc: 100.00%, Val Loss: 0.2705, Val Acc: 100.00%\n",
      "Fold 1 Epoch [48/100], Train Loss: 0.1988, Train Acc: 100.00%, Val Loss: 0.2556, Val Acc: 100.00%\n",
      "Fold 1 Epoch [49/100], Train Loss: 0.1861, Train Acc: 100.00%, Val Loss: 0.2414, Val Acc: 100.00%\n",
      "Fold 1 Epoch [50/100], Train Loss: 0.1740, Train Acc: 100.00%, Val Loss: 0.2276, Val Acc: 100.00%\n",
      "Fold 1 Epoch [51/100], Train Loss: 0.1626, Train Acc: 100.00%, Val Loss: 0.2146, Val Acc: 100.00%\n",
      "Fold 1 Epoch [52/100], Train Loss: 0.1519, Train Acc: 100.00%, Val Loss: 0.2024, Val Acc: 100.00%\n",
      "Fold 1 Epoch [53/100], Train Loss: 0.1417, Train Acc: 100.00%, Val Loss: 0.1907, Val Acc: 100.00%\n",
      "Fold 1 Epoch [54/100], Train Loss: 0.1320, Train Acc: 100.00%, Val Loss: 0.1796, Val Acc: 100.00%\n",
      "Fold 1 Epoch [55/100], Train Loss: 0.1230, Train Acc: 100.00%, Val Loss: 0.1691, Val Acc: 100.00%\n",
      "Fold 1 Epoch [56/100], Train Loss: 0.1145, Train Acc: 100.00%, Val Loss: 0.1592, Val Acc: 100.00%\n",
      "Fold 1 Epoch [57/100], Train Loss: 0.1066, Train Acc: 100.00%, Val Loss: 0.1499, Val Acc: 100.00%\n",
      "Fold 1 Epoch [58/100], Train Loss: 0.0992, Train Acc: 100.00%, Val Loss: 0.1411, Val Acc: 100.00%\n",
      "Fold 1 Epoch [59/100], Train Loss: 0.0922, Train Acc: 100.00%, Val Loss: 0.1328, Val Acc: 100.00%\n",
      "Fold 1 Epoch [60/100], Train Loss: 0.0856, Train Acc: 100.00%, Val Loss: 0.1249, Val Acc: 100.00%\n",
      "Fold 1 Epoch [61/100], Train Loss: 0.0795, Train Acc: 100.00%, Val Loss: 0.1176, Val Acc: 100.00%\n",
      "Fold 1 Epoch [62/100], Train Loss: 0.0738, Train Acc: 100.00%, Val Loss: 0.1109, Val Acc: 100.00%\n",
      "Fold 1 Epoch [63/100], Train Loss: 0.0686, Train Acc: 100.00%, Val Loss: 0.1045, Val Acc: 100.00%\n",
      "Fold 1 Epoch [64/100], Train Loss: 0.0638, Train Acc: 100.00%, Val Loss: 0.0984, Val Acc: 100.00%\n",
      "Fold 1 Epoch [65/100], Train Loss: 0.0593, Train Acc: 100.00%, Val Loss: 0.0928, Val Acc: 100.00%\n",
      "Fold 1 Epoch [66/100], Train Loss: 0.0552, Train Acc: 100.00%, Val Loss: 0.0876, Val Acc: 100.00%\n",
      "Fold 1 Epoch [67/100], Train Loss: 0.0514, Train Acc: 100.00%, Val Loss: 0.0827, Val Acc: 100.00%\n",
      "Fold 1 Epoch [68/100], Train Loss: 0.0479, Train Acc: 100.00%, Val Loss: 0.0778, Val Acc: 100.00%\n",
      "Fold 1 Epoch [69/100], Train Loss: 0.0446, Train Acc: 100.00%, Val Loss: 0.0733, Val Acc: 100.00%\n",
      "Fold 1 Epoch [70/100], Train Loss: 0.0416, Train Acc: 100.00%, Val Loss: 0.0690, Val Acc: 100.00%\n",
      "Fold 1 Epoch [71/100], Train Loss: 0.0388, Train Acc: 100.00%, Val Loss: 0.0651, Val Acc: 100.00%\n",
      "Fold 1 Epoch [72/100], Train Loss: 0.0363, Train Acc: 100.00%, Val Loss: 0.0615, Val Acc: 100.00%\n",
      "Fold 1 Epoch [73/100], Train Loss: 0.0339, Train Acc: 100.00%, Val Loss: 0.0580, Val Acc: 100.00%\n",
      "Fold 1 Epoch [74/100], Train Loss: 0.0318, Train Acc: 100.00%, Val Loss: 0.0548, Val Acc: 100.00%\n",
      "Fold 1 Epoch [75/100], Train Loss: 0.0298, Train Acc: 100.00%, Val Loss: 0.0519, Val Acc: 100.00%\n",
      "Fold 1 Epoch [76/100], Train Loss: 0.0280, Train Acc: 100.00%, Val Loss: 0.0492, Val Acc: 100.00%\n",
      "Fold 1 Epoch [77/100], Train Loss: 0.0263, Train Acc: 100.00%, Val Loss: 0.0467, Val Acc: 100.00%\n",
      "Fold 1 Epoch [78/100], Train Loss: 0.0248, Train Acc: 100.00%, Val Loss: 0.0443, Val Acc: 100.00%\n",
      "Fold 1 Epoch [79/100], Train Loss: 0.0234, Train Acc: 100.00%, Val Loss: 0.0422, Val Acc: 100.00%\n",
      "Fold 1 Epoch [80/100], Train Loss: 0.0221, Train Acc: 100.00%, Val Loss: 0.0404, Val Acc: 100.00%\n",
      "Fold 1 Epoch [81/100], Train Loss: 0.0208, Train Acc: 100.00%, Val Loss: 0.0386, Val Acc: 100.00%\n",
      "Fold 1 Epoch [82/100], Train Loss: 0.0197, Train Acc: 100.00%, Val Loss: 0.0369, Val Acc: 100.00%\n",
      "Fold 1 Epoch [83/100], Train Loss: 0.0186, Train Acc: 100.00%, Val Loss: 0.0354, Val Acc: 100.00%\n",
      "Fold 1 Epoch [84/100], Train Loss: 0.0177, Train Acc: 100.00%, Val Loss: 0.0340, Val Acc: 100.00%\n",
      "Fold 1 Epoch [85/100], Train Loss: 0.0167, Train Acc: 100.00%, Val Loss: 0.0327, Val Acc: 100.00%\n",
      "Fold 1 Epoch [86/100], Train Loss: 0.0159, Train Acc: 100.00%, Val Loss: 0.0314, Val Acc: 100.00%\n",
      "Fold 1 Epoch [87/100], Train Loss: 0.0151, Train Acc: 100.00%, Val Loss: 0.0301, Val Acc: 100.00%\n",
      "Fold 1 Epoch [88/100], Train Loss: 0.0143, Train Acc: 100.00%, Val Loss: 0.0290, Val Acc: 100.00%\n",
      "Fold 1 Epoch [89/100], Train Loss: 0.0136, Train Acc: 100.00%, Val Loss: 0.0280, Val Acc: 100.00%\n",
      "Fold 1 Epoch [90/100], Train Loss: 0.0130, Train Acc: 100.00%, Val Loss: 0.0270, Val Acc: 100.00%\n",
      "Fold 1 Epoch [91/100], Train Loss: 0.0124, Train Acc: 100.00%, Val Loss: 0.0262, Val Acc: 100.00%\n",
      "Fold 1 Epoch [92/100], Train Loss: 0.0118, Train Acc: 100.00%, Val Loss: 0.0252, Val Acc: 100.00%\n",
      "Fold 1 Epoch [93/100], Train Loss: 0.0113, Train Acc: 100.00%, Val Loss: 0.0243, Val Acc: 100.00%\n",
      "Fold 1 Epoch [94/100], Train Loss: 0.0108, Train Acc: 100.00%, Val Loss: 0.0233, Val Acc: 100.00%\n",
      "Fold 1 Epoch [95/100], Train Loss: 0.0103, Train Acc: 100.00%, Val Loss: 0.0223, Val Acc: 100.00%\n",
      "Fold 1 Epoch [96/100], Train Loss: 0.0099, Train Acc: 100.00%, Val Loss: 0.0214, Val Acc: 100.00%\n",
      "Fold 1 Epoch [97/100], Train Loss: 0.0095, Train Acc: 100.00%, Val Loss: 0.0204, Val Acc: 100.00%\n",
      "Fold 1 Epoch [98/100], Train Loss: 0.0091, Train Acc: 100.00%, Val Loss: 0.0196, Val Acc: 100.00%\n",
      "Fold 1 Epoch [99/100], Train Loss: 0.0087, Train Acc: 100.00%, Val Loss: 0.0188, Val Acc: 100.00%\n",
      "Fold 1 Epoch [100/100], Train Loss: 0.0084, Train Acc: 100.00%, Val Loss: 0.0179, Val Acc: 100.00%\n",
      "Fold 1 Best Accuracy: 100.00%\n",
      "Fold 2/5\n",
      "Fold 2 Epoch [1/100], Train Loss: 1.0468, Train Acc: 68.42%, Val Loss: 1.0622, Val Acc: 60.00%\n",
      "Fold 2 Epoch [2/100], Train Loss: 1.0272, Train Acc: 68.42%, Val Loss: 1.0455, Val Acc: 60.00%\n",
      "Fold 2 Epoch [3/100], Train Loss: 1.0084, Train Acc: 68.42%, Val Loss: 1.0304, Val Acc: 60.00%\n",
      "Fold 2 Epoch [4/100], Train Loss: 0.9898, Train Acc: 68.42%, Val Loss: 1.0160, Val Acc: 60.00%\n",
      "Fold 2 Epoch [5/100], Train Loss: 0.9715, Train Acc: 68.42%, Val Loss: 1.0020, Val Acc: 60.00%\n",
      "Fold 2 Epoch [6/100], Train Loss: 0.9535, Train Acc: 68.42%, Val Loss: 0.9876, Val Acc: 60.00%\n",
      "Fold 2 Epoch [7/100], Train Loss: 0.9365, Train Acc: 68.42%, Val Loss: 0.9739, Val Acc: 60.00%\n",
      "Fold 2 Epoch [8/100], Train Loss: 0.9197, Train Acc: 68.42%, Val Loss: 0.9601, Val Acc: 60.00%\n",
      "Fold 2 Epoch [9/100], Train Loss: 0.9029, Train Acc: 68.42%, Val Loss: 0.9465, Val Acc: 60.00%\n",
      "Fold 2 Epoch [10/100], Train Loss: 0.8857, Train Acc: 73.68%, Val Loss: 0.9325, Val Acc: 60.00%\n",
      "Fold 2 Epoch [11/100], Train Loss: 0.8685, Train Acc: 78.95%, Val Loss: 0.9182, Val Acc: 60.00%\n",
      "Fold 2 Epoch [12/100], Train Loss: 0.8512, Train Acc: 78.95%, Val Loss: 0.9038, Val Acc: 60.00%\n",
      "Fold 2 Epoch [13/100], Train Loss: 0.8338, Train Acc: 84.21%, Val Loss: 0.8896, Val Acc: 60.00%\n",
      "Fold 2 Epoch [14/100], Train Loss: 0.8163, Train Acc: 84.21%, Val Loss: 0.8748, Val Acc: 60.00%\n",
      "Fold 2 Epoch [15/100], Train Loss: 0.7985, Train Acc: 84.21%, Val Loss: 0.8592, Val Acc: 60.00%\n",
      "Fold 2 Epoch [16/100], Train Loss: 0.7804, Train Acc: 84.21%, Val Loss: 0.8436, Val Acc: 60.00%\n",
      "Fold 2 Epoch [17/100], Train Loss: 0.7619, Train Acc: 84.21%, Val Loss: 0.8277, Val Acc: 60.00%\n",
      "Fold 2 Epoch [18/100], Train Loss: 0.7431, Train Acc: 84.21%, Val Loss: 0.8118, Val Acc: 60.00%\n",
      "Fold 2 Epoch [19/100], Train Loss: 0.7242, Train Acc: 84.21%, Val Loss: 0.7956, Val Acc: 60.00%\n",
      "Fold 2 Epoch [20/100], Train Loss: 0.7053, Train Acc: 84.21%, Val Loss: 0.7793, Val Acc: 80.00%\n",
      "Fold 2 Epoch [21/100], Train Loss: 0.6862, Train Acc: 94.74%, Val Loss: 0.7627, Val Acc: 80.00%\n",
      "Fold 2 Epoch [22/100], Train Loss: 0.6668, Train Acc: 94.74%, Val Loss: 0.7459, Val Acc: 80.00%\n",
      "Fold 2 Epoch [23/100], Train Loss: 0.6471, Train Acc: 94.74%, Val Loss: 0.7286, Val Acc: 80.00%\n",
      "Fold 2 Epoch [24/100], Train Loss: 0.6272, Train Acc: 94.74%, Val Loss: 0.7108, Val Acc: 80.00%\n",
      "Fold 2 Epoch [25/100], Train Loss: 0.6069, Train Acc: 94.74%, Val Loss: 0.6929, Val Acc: 80.00%\n",
      "Fold 2 Epoch [26/100], Train Loss: 0.5865, Train Acc: 94.74%, Val Loss: 0.6751, Val Acc: 80.00%\n",
      "Fold 2 Epoch [27/100], Train Loss: 0.5662, Train Acc: 94.74%, Val Loss: 0.6574, Val Acc: 80.00%\n",
      "Fold 2 Epoch [28/100], Train Loss: 0.5460, Train Acc: 94.74%, Val Loss: 0.6397, Val Acc: 80.00%\n",
      "Fold 2 Epoch [29/100], Train Loss: 0.5258, Train Acc: 94.74%, Val Loss: 0.6221, Val Acc: 80.00%\n",
      "Fold 2 Epoch [30/100], Train Loss: 0.5055, Train Acc: 100.00%, Val Loss: 0.6045, Val Acc: 80.00%\n",
      "Fold 2 Epoch [31/100], Train Loss: 0.4856, Train Acc: 100.00%, Val Loss: 0.5871, Val Acc: 80.00%\n",
      "Fold 2 Epoch [32/100], Train Loss: 0.4659, Train Acc: 100.00%, Val Loss: 0.5695, Val Acc: 80.00%\n",
      "Fold 2 Epoch [33/100], Train Loss: 0.4464, Train Acc: 100.00%, Val Loss: 0.5522, Val Acc: 80.00%\n",
      "Fold 2 Epoch [34/100], Train Loss: 0.4273, Train Acc: 100.00%, Val Loss: 0.5351, Val Acc: 80.00%\n",
      "Fold 2 Epoch [35/100], Train Loss: 0.4084, Train Acc: 100.00%, Val Loss: 0.5184, Val Acc: 80.00%\n",
      "Fold 2 Epoch [36/100], Train Loss: 0.3899, Train Acc: 100.00%, Val Loss: 0.5018, Val Acc: 100.00%\n",
      "Fold 2 Epoch [37/100], Train Loss: 0.3718, Train Acc: 100.00%, Val Loss: 0.4854, Val Acc: 100.00%\n",
      "Fold 2 Epoch [38/100], Train Loss: 0.3542, Train Acc: 100.00%, Val Loss: 0.4693, Val Acc: 100.00%\n",
      "Fold 2 Epoch [39/100], Train Loss: 0.3370, Train Acc: 100.00%, Val Loss: 0.4536, Val Acc: 100.00%\n",
      "Fold 2 Epoch [40/100], Train Loss: 0.3203, Train Acc: 100.00%, Val Loss: 0.4383, Val Acc: 100.00%\n",
      "Fold 2 Epoch [41/100], Train Loss: 0.3042, Train Acc: 100.00%, Val Loss: 0.4233, Val Acc: 100.00%\n",
      "Fold 2 Epoch [42/100], Train Loss: 0.2887, Train Acc: 100.00%, Val Loss: 0.4088, Val Acc: 100.00%\n",
      "Fold 2 Epoch [43/100], Train Loss: 0.2737, Train Acc: 100.00%, Val Loss: 0.3947, Val Acc: 100.00%\n",
      "Fold 2 Epoch [44/100], Train Loss: 0.2593, Train Acc: 100.00%, Val Loss: 0.3811, Val Acc: 100.00%\n",
      "Fold 2 Epoch [45/100], Train Loss: 0.2454, Train Acc: 100.00%, Val Loss: 0.3679, Val Acc: 100.00%\n",
      "Fold 2 Epoch [46/100], Train Loss: 0.2320, Train Acc: 100.00%, Val Loss: 0.3550, Val Acc: 100.00%\n",
      "Fold 2 Epoch [47/100], Train Loss: 0.2191, Train Acc: 100.00%, Val Loss: 0.3424, Val Acc: 100.00%\n",
      "Fold 2 Epoch [48/100], Train Loss: 0.2066, Train Acc: 100.00%, Val Loss: 0.3304, Val Acc: 100.00%\n",
      "Fold 2 Epoch [49/100], Train Loss: 0.1946, Train Acc: 100.00%, Val Loss: 0.3186, Val Acc: 100.00%\n",
      "Fold 2 Epoch [50/100], Train Loss: 0.1830, Train Acc: 100.00%, Val Loss: 0.3070, Val Acc: 100.00%\n",
      "Fold 2 Epoch [51/100], Train Loss: 0.1720, Train Acc: 100.00%, Val Loss: 0.2956, Val Acc: 100.00%\n",
      "Fold 2 Epoch [52/100], Train Loss: 0.1615, Train Acc: 100.00%, Val Loss: 0.2844, Val Acc: 100.00%\n",
      "Fold 2 Epoch [53/100], Train Loss: 0.1515, Train Acc: 100.00%, Val Loss: 0.2735, Val Acc: 100.00%\n",
      "Fold 2 Epoch [54/100], Train Loss: 0.1420, Train Acc: 100.00%, Val Loss: 0.2628, Val Acc: 100.00%\n",
      "Fold 2 Epoch [55/100], Train Loss: 0.1330, Train Acc: 100.00%, Val Loss: 0.2521, Val Acc: 100.00%\n",
      "Fold 2 Epoch [56/100], Train Loss: 0.1244, Train Acc: 100.00%, Val Loss: 0.2417, Val Acc: 100.00%\n",
      "Fold 2 Epoch [57/100], Train Loss: 0.1163, Train Acc: 100.00%, Val Loss: 0.2317, Val Acc: 100.00%\n",
      "Fold 2 Epoch [58/100], Train Loss: 0.1087, Train Acc: 100.00%, Val Loss: 0.2221, Val Acc: 100.00%\n",
      "Fold 2 Epoch [59/100], Train Loss: 0.1016, Train Acc: 100.00%, Val Loss: 0.2127, Val Acc: 100.00%\n",
      "Fold 2 Epoch [60/100], Train Loss: 0.0949, Train Acc: 100.00%, Val Loss: 0.2037, Val Acc: 100.00%\n",
      "Fold 2 Epoch [61/100], Train Loss: 0.0886, Train Acc: 100.00%, Val Loss: 0.1951, Val Acc: 100.00%\n",
      "Fold 2 Epoch [62/100], Train Loss: 0.0828, Train Acc: 100.00%, Val Loss: 0.1869, Val Acc: 100.00%\n",
      "Fold 2 Epoch [63/100], Train Loss: 0.0773, Train Acc: 100.00%, Val Loss: 0.1787, Val Acc: 100.00%\n",
      "Fold 2 Epoch [64/100], Train Loss: 0.0722, Train Acc: 100.00%, Val Loss: 0.1710, Val Acc: 100.00%\n",
      "Fold 2 Epoch [65/100], Train Loss: 0.0675, Train Acc: 100.00%, Val Loss: 0.1636, Val Acc: 100.00%\n",
      "Fold 2 Epoch [66/100], Train Loss: 0.0631, Train Acc: 100.00%, Val Loss: 0.1567, Val Acc: 100.00%\n",
      "Fold 2 Epoch [67/100], Train Loss: 0.0590, Train Acc: 100.00%, Val Loss: 0.1501, Val Acc: 100.00%\n",
      "Fold 2 Epoch [68/100], Train Loss: 0.0552, Train Acc: 100.00%, Val Loss: 0.1440, Val Acc: 100.00%\n",
      "Fold 2 Epoch [69/100], Train Loss: 0.0518, Train Acc: 100.00%, Val Loss: 0.1383, Val Acc: 100.00%\n",
      "Fold 2 Epoch [70/100], Train Loss: 0.0485, Train Acc: 100.00%, Val Loss: 0.1328, Val Acc: 100.00%\n",
      "Fold 2 Epoch [71/100], Train Loss: 0.0455, Train Acc: 100.00%, Val Loss: 0.1274, Val Acc: 100.00%\n",
      "Fold 2 Epoch [72/100], Train Loss: 0.0427, Train Acc: 100.00%, Val Loss: 0.1225, Val Acc: 100.00%\n",
      "Fold 2 Epoch [73/100], Train Loss: 0.0401, Train Acc: 100.00%, Val Loss: 0.1175, Val Acc: 100.00%\n",
      "Fold 2 Epoch [74/100], Train Loss: 0.0376, Train Acc: 100.00%, Val Loss: 0.1129, Val Acc: 100.00%\n",
      "Fold 2 Epoch [75/100], Train Loss: 0.0354, Train Acc: 100.00%, Val Loss: 0.1087, Val Acc: 100.00%\n",
      "Fold 2 Epoch [76/100], Train Loss: 0.0333, Train Acc: 100.00%, Val Loss: 0.1046, Val Acc: 100.00%\n",
      "Fold 2 Epoch [77/100], Train Loss: 0.0314, Train Acc: 100.00%, Val Loss: 0.1009, Val Acc: 100.00%\n",
      "Fold 2 Epoch [78/100], Train Loss: 0.0296, Train Acc: 100.00%, Val Loss: 0.0974, Val Acc: 100.00%\n",
      "Fold 2 Epoch [79/100], Train Loss: 0.0279, Train Acc: 100.00%, Val Loss: 0.0939, Val Acc: 100.00%\n",
      "Fold 2 Epoch [80/100], Train Loss: 0.0263, Train Acc: 100.00%, Val Loss: 0.0906, Val Acc: 100.00%\n",
      "Fold 2 Epoch [81/100], Train Loss: 0.0248, Train Acc: 100.00%, Val Loss: 0.0875, Val Acc: 100.00%\n",
      "Fold 2 Epoch [82/100], Train Loss: 0.0234, Train Acc: 100.00%, Val Loss: 0.0846, Val Acc: 100.00%\n",
      "Fold 2 Epoch [83/100], Train Loss: 0.0221, Train Acc: 100.00%, Val Loss: 0.0819, Val Acc: 100.00%\n",
      "Fold 2 Epoch [84/100], Train Loss: 0.0209, Train Acc: 100.00%, Val Loss: 0.0791, Val Acc: 100.00%\n",
      "Fold 2 Epoch [85/100], Train Loss: 0.0197, Train Acc: 100.00%, Val Loss: 0.0765, Val Acc: 100.00%\n",
      "Fold 2 Epoch [86/100], Train Loss: 0.0187, Train Acc: 100.00%, Val Loss: 0.0738, Val Acc: 100.00%\n",
      "Fold 2 Epoch [87/100], Train Loss: 0.0177, Train Acc: 100.00%, Val Loss: 0.0711, Val Acc: 100.00%\n",
      "Fold 2 Epoch [88/100], Train Loss: 0.0168, Train Acc: 100.00%, Val Loss: 0.0687, Val Acc: 100.00%\n",
      "Fold 2 Epoch [89/100], Train Loss: 0.0159, Train Acc: 100.00%, Val Loss: 0.0663, Val Acc: 100.00%\n",
      "Fold 2 Epoch [90/100], Train Loss: 0.0151, Train Acc: 100.00%, Val Loss: 0.0641, Val Acc: 100.00%\n",
      "Fold 2 Epoch [91/100], Train Loss: 0.0144, Train Acc: 100.00%, Val Loss: 0.0616, Val Acc: 100.00%\n",
      "Fold 2 Epoch [92/100], Train Loss: 0.0137, Train Acc: 100.00%, Val Loss: 0.0594, Val Acc: 100.00%\n",
      "Fold 2 Epoch [93/100], Train Loss: 0.0130, Train Acc: 100.00%, Val Loss: 0.0573, Val Acc: 100.00%\n",
      "Fold 2 Epoch [94/100], Train Loss: 0.0124, Train Acc: 100.00%, Val Loss: 0.0555, Val Acc: 100.00%\n",
      "Fold 2 Epoch [95/100], Train Loss: 0.0119, Train Acc: 100.00%, Val Loss: 0.0537, Val Acc: 100.00%\n",
      "Fold 2 Epoch [96/100], Train Loss: 0.0113, Train Acc: 100.00%, Val Loss: 0.0521, Val Acc: 100.00%\n",
      "Fold 2 Epoch [97/100], Train Loss: 0.0108, Train Acc: 100.00%, Val Loss: 0.0506, Val Acc: 100.00%\n",
      "Fold 2 Epoch [98/100], Train Loss: 0.0104, Train Acc: 100.00%, Val Loss: 0.0492, Val Acc: 100.00%\n",
      "Fold 2 Epoch [99/100], Train Loss: 0.0099, Train Acc: 100.00%, Val Loss: 0.0479, Val Acc: 100.00%\n",
      "Fold 2 Epoch [100/100], Train Loss: 0.0095, Train Acc: 100.00%, Val Loss: 0.0468, Val Acc: 100.00%\n",
      "Fold 2 Best Accuracy: 100.00%\n",
      "Fold 3/5\n",
      "Fold 3 Epoch [1/100], Train Loss: 1.0992, Train Acc: 31.58%, Val Loss: 1.0144, Val Acc: 80.00%\n",
      "Fold 3 Epoch [2/100], Train Loss: 1.0808, Train Acc: 36.84%, Val Loss: 0.9961, Val Acc: 80.00%\n",
      "Fold 3 Epoch [3/100], Train Loss: 1.0632, Train Acc: 57.89%, Val Loss: 0.9784, Val Acc: 80.00%\n",
      "Fold 3 Epoch [4/100], Train Loss: 1.0461, Train Acc: 63.16%, Val Loss: 0.9613, Val Acc: 80.00%\n",
      "Fold 3 Epoch [5/100], Train Loss: 1.0293, Train Acc: 68.42%, Val Loss: 0.9433, Val Acc: 100.00%\n",
      "Fold 3 Epoch [6/100], Train Loss: 1.0131, Train Acc: 73.68%, Val Loss: 0.9254, Val Acc: 100.00%\n",
      "Fold 3 Epoch [7/100], Train Loss: 0.9969, Train Acc: 73.68%, Val Loss: 0.9066, Val Acc: 100.00%\n",
      "Fold 3 Epoch [8/100], Train Loss: 0.9810, Train Acc: 78.95%, Val Loss: 0.8885, Val Acc: 100.00%\n",
      "Fold 3 Epoch [9/100], Train Loss: 0.9651, Train Acc: 89.47%, Val Loss: 0.8701, Val Acc: 100.00%\n",
      "Fold 3 Epoch [10/100], Train Loss: 0.9493, Train Acc: 94.74%, Val Loss: 0.8526, Val Acc: 100.00%\n",
      "Fold 3 Epoch [11/100], Train Loss: 0.9334, Train Acc: 94.74%, Val Loss: 0.8353, Val Acc: 100.00%\n",
      "Fold 3 Epoch [12/100], Train Loss: 0.9175, Train Acc: 94.74%, Val Loss: 0.8181, Val Acc: 100.00%\n",
      "Fold 3 Epoch [13/100], Train Loss: 0.9013, Train Acc: 100.00%, Val Loss: 0.8004, Val Acc: 100.00%\n",
      "Fold 3 Epoch [14/100], Train Loss: 0.8849, Train Acc: 100.00%, Val Loss: 0.7824, Val Acc: 100.00%\n",
      "Fold 3 Epoch [15/100], Train Loss: 0.8680, Train Acc: 100.00%, Val Loss: 0.7641, Val Acc: 100.00%\n",
      "Fold 3 Epoch [16/100], Train Loss: 0.8509, Train Acc: 100.00%, Val Loss: 0.7458, Val Acc: 100.00%\n",
      "Fold 3 Epoch [17/100], Train Loss: 0.8335, Train Acc: 100.00%, Val Loss: 0.7272, Val Acc: 100.00%\n",
      "Fold 3 Epoch [18/100], Train Loss: 0.8159, Train Acc: 100.00%, Val Loss: 0.7084, Val Acc: 100.00%\n",
      "Fold 3 Epoch [19/100], Train Loss: 0.7979, Train Acc: 100.00%, Val Loss: 0.6896, Val Acc: 100.00%\n",
      "Fold 3 Epoch [20/100], Train Loss: 0.7797, Train Acc: 100.00%, Val Loss: 0.6711, Val Acc: 100.00%\n",
      "Fold 3 Epoch [21/100], Train Loss: 0.7608, Train Acc: 100.00%, Val Loss: 0.6526, Val Acc: 100.00%\n",
      "Fold 3 Epoch [22/100], Train Loss: 0.7416, Train Acc: 100.00%, Val Loss: 0.6343, Val Acc: 100.00%\n",
      "Fold 3 Epoch [23/100], Train Loss: 0.7220, Train Acc: 100.00%, Val Loss: 0.6161, Val Acc: 100.00%\n",
      "Fold 3 Epoch [24/100], Train Loss: 0.7023, Train Acc: 100.00%, Val Loss: 0.5977, Val Acc: 100.00%\n",
      "Fold 3 Epoch [25/100], Train Loss: 0.6824, Train Acc: 100.00%, Val Loss: 0.5794, Val Acc: 100.00%\n",
      "Fold 3 Epoch [26/100], Train Loss: 0.6622, Train Acc: 100.00%, Val Loss: 0.5611, Val Acc: 100.00%\n",
      "Fold 3 Epoch [27/100], Train Loss: 0.6418, Train Acc: 100.00%, Val Loss: 0.5427, Val Acc: 100.00%\n",
      "Fold 3 Epoch [28/100], Train Loss: 0.6213, Train Acc: 100.00%, Val Loss: 0.5243, Val Acc: 100.00%\n",
      "Fold 3 Epoch [29/100], Train Loss: 0.6007, Train Acc: 100.00%, Val Loss: 0.5060, Val Acc: 100.00%\n",
      "Fold 3 Epoch [30/100], Train Loss: 0.5800, Train Acc: 100.00%, Val Loss: 0.4876, Val Acc: 100.00%\n",
      "Fold 3 Epoch [31/100], Train Loss: 0.5592, Train Acc: 100.00%, Val Loss: 0.4695, Val Acc: 100.00%\n",
      "Fold 3 Epoch [32/100], Train Loss: 0.5384, Train Acc: 100.00%, Val Loss: 0.4516, Val Acc: 100.00%\n",
      "Fold 3 Epoch [33/100], Train Loss: 0.5174, Train Acc: 100.00%, Val Loss: 0.4337, Val Acc: 100.00%\n",
      "Fold 3 Epoch [34/100], Train Loss: 0.4961, Train Acc: 100.00%, Val Loss: 0.4159, Val Acc: 100.00%\n",
      "Fold 3 Epoch [35/100], Train Loss: 0.4751, Train Acc: 100.00%, Val Loss: 0.3982, Val Acc: 100.00%\n",
      "Fold 3 Epoch [36/100], Train Loss: 0.4540, Train Acc: 100.00%, Val Loss: 0.3804, Val Acc: 100.00%\n",
      "Fold 3 Epoch [37/100], Train Loss: 0.4328, Train Acc: 100.00%, Val Loss: 0.3627, Val Acc: 100.00%\n",
      "Fold 3 Epoch [38/100], Train Loss: 0.4120, Train Acc: 100.00%, Val Loss: 0.3451, Val Acc: 100.00%\n",
      "Fold 3 Epoch [39/100], Train Loss: 0.3915, Train Acc: 100.00%, Val Loss: 0.3277, Val Acc: 100.00%\n",
      "Fold 3 Epoch [40/100], Train Loss: 0.3715, Train Acc: 100.00%, Val Loss: 0.3109, Val Acc: 100.00%\n",
      "Fold 3 Epoch [41/100], Train Loss: 0.3517, Train Acc: 100.00%, Val Loss: 0.2944, Val Acc: 100.00%\n",
      "Fold 3 Epoch [42/100], Train Loss: 0.3324, Train Acc: 100.00%, Val Loss: 0.2782, Val Acc: 100.00%\n",
      "Fold 3 Epoch [43/100], Train Loss: 0.3136, Train Acc: 100.00%, Val Loss: 0.2625, Val Acc: 100.00%\n",
      "Fold 3 Epoch [44/100], Train Loss: 0.2951, Train Acc: 100.00%, Val Loss: 0.2474, Val Acc: 100.00%\n",
      "Fold 3 Epoch [45/100], Train Loss: 0.2774, Train Acc: 100.00%, Val Loss: 0.2327, Val Acc: 100.00%\n",
      "Fold 3 Epoch [46/100], Train Loss: 0.2603, Train Acc: 100.00%, Val Loss: 0.2187, Val Acc: 100.00%\n",
      "Fold 3 Epoch [47/100], Train Loss: 0.2440, Train Acc: 100.00%, Val Loss: 0.2052, Val Acc: 100.00%\n",
      "Fold 3 Epoch [48/100], Train Loss: 0.2283, Train Acc: 100.00%, Val Loss: 0.1923, Val Acc: 100.00%\n",
      "Fold 3 Epoch [49/100], Train Loss: 0.2133, Train Acc: 100.00%, Val Loss: 0.1801, Val Acc: 100.00%\n",
      "Fold 3 Epoch [50/100], Train Loss: 0.1991, Train Acc: 100.00%, Val Loss: 0.1685, Val Acc: 100.00%\n",
      "Fold 3 Epoch [51/100], Train Loss: 0.1856, Train Acc: 100.00%, Val Loss: 0.1574, Val Acc: 100.00%\n",
      "Fold 3 Epoch [52/100], Train Loss: 0.1728, Train Acc: 100.00%, Val Loss: 0.1473, Val Acc: 100.00%\n",
      "Fold 3 Epoch [53/100], Train Loss: 0.1607, Train Acc: 100.00%, Val Loss: 0.1375, Val Acc: 100.00%\n",
      "Fold 3 Epoch [54/100], Train Loss: 0.1494, Train Acc: 100.00%, Val Loss: 0.1285, Val Acc: 100.00%\n",
      "Fold 3 Epoch [55/100], Train Loss: 0.1388, Train Acc: 100.00%, Val Loss: 0.1200, Val Acc: 100.00%\n",
      "Fold 3 Epoch [56/100], Train Loss: 0.1288, Train Acc: 100.00%, Val Loss: 0.1122, Val Acc: 100.00%\n",
      "Fold 3 Epoch [57/100], Train Loss: 0.1196, Train Acc: 100.00%, Val Loss: 0.1048, Val Acc: 100.00%\n",
      "Fold 3 Epoch [58/100], Train Loss: 0.1108, Train Acc: 100.00%, Val Loss: 0.0978, Val Acc: 100.00%\n",
      "Fold 3 Epoch [59/100], Train Loss: 0.1028, Train Acc: 100.00%, Val Loss: 0.0912, Val Acc: 100.00%\n",
      "Fold 3 Epoch [60/100], Train Loss: 0.0953, Train Acc: 100.00%, Val Loss: 0.0851, Val Acc: 100.00%\n",
      "Fold 3 Epoch [61/100], Train Loss: 0.0883, Train Acc: 100.00%, Val Loss: 0.0794, Val Acc: 100.00%\n",
      "Fold 3 Epoch [62/100], Train Loss: 0.0819, Train Acc: 100.00%, Val Loss: 0.0741, Val Acc: 100.00%\n",
      "Fold 3 Epoch [63/100], Train Loss: 0.0760, Train Acc: 100.00%, Val Loss: 0.0692, Val Acc: 100.00%\n",
      "Fold 3 Epoch [64/100], Train Loss: 0.0705, Train Acc: 100.00%, Val Loss: 0.0645, Val Acc: 100.00%\n",
      "Fold 3 Epoch [65/100], Train Loss: 0.0656, Train Acc: 100.00%, Val Loss: 0.0604, Val Acc: 100.00%\n",
      "Fold 3 Epoch [66/100], Train Loss: 0.0610, Train Acc: 100.00%, Val Loss: 0.0566, Val Acc: 100.00%\n",
      "Fold 3 Epoch [67/100], Train Loss: 0.0568, Train Acc: 100.00%, Val Loss: 0.0529, Val Acc: 100.00%\n",
      "Fold 3 Epoch [68/100], Train Loss: 0.0529, Train Acc: 100.00%, Val Loss: 0.0493, Val Acc: 100.00%\n",
      "Fold 3 Epoch [69/100], Train Loss: 0.0493, Train Acc: 100.00%, Val Loss: 0.0460, Val Acc: 100.00%\n",
      "Fold 3 Epoch [70/100], Train Loss: 0.0460, Train Acc: 100.00%, Val Loss: 0.0430, Val Acc: 100.00%\n",
      "Fold 3 Epoch [71/100], Train Loss: 0.0430, Train Acc: 100.00%, Val Loss: 0.0404, Val Acc: 100.00%\n",
      "Fold 3 Epoch [72/100], Train Loss: 0.0402, Train Acc: 100.00%, Val Loss: 0.0378, Val Acc: 100.00%\n",
      "Fold 3 Epoch [73/100], Train Loss: 0.0376, Train Acc: 100.00%, Val Loss: 0.0355, Val Acc: 100.00%\n",
      "Fold 3 Epoch [74/100], Train Loss: 0.0352, Train Acc: 100.00%, Val Loss: 0.0331, Val Acc: 100.00%\n",
      "Fold 3 Epoch [75/100], Train Loss: 0.0330, Train Acc: 100.00%, Val Loss: 0.0309, Val Acc: 100.00%\n",
      "Fold 3 Epoch [76/100], Train Loss: 0.0310, Train Acc: 100.00%, Val Loss: 0.0290, Val Acc: 100.00%\n",
      "Fold 3 Epoch [77/100], Train Loss: 0.0292, Train Acc: 100.00%, Val Loss: 0.0271, Val Acc: 100.00%\n",
      "Fold 3 Epoch [78/100], Train Loss: 0.0275, Train Acc: 100.00%, Val Loss: 0.0254, Val Acc: 100.00%\n",
      "Fold 3 Epoch [79/100], Train Loss: 0.0259, Train Acc: 100.00%, Val Loss: 0.0239, Val Acc: 100.00%\n",
      "Fold 3 Epoch [80/100], Train Loss: 0.0245, Train Acc: 100.00%, Val Loss: 0.0225, Val Acc: 100.00%\n",
      "Fold 3 Epoch [81/100], Train Loss: 0.0232, Train Acc: 100.00%, Val Loss: 0.0212, Val Acc: 100.00%\n",
      "Fold 3 Epoch [82/100], Train Loss: 0.0219, Train Acc: 100.00%, Val Loss: 0.0200, Val Acc: 100.00%\n",
      "Fold 3 Epoch [83/100], Train Loss: 0.0208, Train Acc: 100.00%, Val Loss: 0.0189, Val Acc: 100.00%\n",
      "Fold 3 Epoch [84/100], Train Loss: 0.0198, Train Acc: 100.00%, Val Loss: 0.0180, Val Acc: 100.00%\n",
      "Fold 3 Epoch [85/100], Train Loss: 0.0188, Train Acc: 100.00%, Val Loss: 0.0170, Val Acc: 100.00%\n",
      "Fold 3 Epoch [86/100], Train Loss: 0.0178, Train Acc: 100.00%, Val Loss: 0.0160, Val Acc: 100.00%\n",
      "Fold 3 Epoch [87/100], Train Loss: 0.0170, Train Acc: 100.00%, Val Loss: 0.0152, Val Acc: 100.00%\n",
      "Fold 3 Epoch [88/100], Train Loss: 0.0161, Train Acc: 100.00%, Val Loss: 0.0144, Val Acc: 100.00%\n",
      "Fold 3 Epoch [89/100], Train Loss: 0.0153, Train Acc: 100.00%, Val Loss: 0.0136, Val Acc: 100.00%\n",
      "Fold 3 Epoch [90/100], Train Loss: 0.0146, Train Acc: 100.00%, Val Loss: 0.0130, Val Acc: 100.00%\n",
      "Fold 3 Epoch [91/100], Train Loss: 0.0139, Train Acc: 100.00%, Val Loss: 0.0124, Val Acc: 100.00%\n",
      "Fold 3 Epoch [92/100], Train Loss: 0.0133, Train Acc: 100.00%, Val Loss: 0.0118, Val Acc: 100.00%\n",
      "Fold 3 Epoch [93/100], Train Loss: 0.0127, Train Acc: 100.00%, Val Loss: 0.0113, Val Acc: 100.00%\n",
      "Fold 3 Epoch [94/100], Train Loss: 0.0122, Train Acc: 100.00%, Val Loss: 0.0107, Val Acc: 100.00%\n",
      "Fold 3 Epoch [95/100], Train Loss: 0.0116, Train Acc: 100.00%, Val Loss: 0.0102, Val Acc: 100.00%\n",
      "Fold 3 Epoch [96/100], Train Loss: 0.0111, Train Acc: 100.00%, Val Loss: 0.0098, Val Acc: 100.00%\n",
      "Fold 3 Epoch [97/100], Train Loss: 0.0107, Train Acc: 100.00%, Val Loss: 0.0094, Val Acc: 100.00%\n",
      "Fold 3 Epoch [98/100], Train Loss: 0.0103, Train Acc: 100.00%, Val Loss: 0.0090, Val Acc: 100.00%\n",
      "Fold 3 Epoch [99/100], Train Loss: 0.0099, Train Acc: 100.00%, Val Loss: 0.0087, Val Acc: 100.00%\n",
      "Fold 3 Epoch [100/100], Train Loss: 0.0095, Train Acc: 100.00%, Val Loss: 0.0083, Val Acc: 100.00%\n",
      "Fold 3 Best Accuracy: 100.00%\n",
      "Fold 4/5\n",
      "Fold 4 Epoch [1/100], Train Loss: 1.1099, Train Acc: 15.79%, Val Loss: 1.1219, Val Acc: 20.00%\n",
      "Fold 4 Epoch [2/100], Train Loss: 1.0919, Train Acc: 15.79%, Val Loss: 1.1133, Val Acc: 20.00%\n",
      "Fold 4 Epoch [3/100], Train Loss: 1.0748, Train Acc: 31.58%, Val Loss: 1.1030, Val Acc: 40.00%\n",
      "Fold 4 Epoch [4/100], Train Loss: 1.0582, Train Acc: 31.58%, Val Loss: 1.0919, Val Acc: 40.00%\n",
      "Fold 4 Epoch [5/100], Train Loss: 1.0417, Train Acc: 63.16%, Val Loss: 1.0797, Val Acc: 40.00%\n",
      "Fold 4 Epoch [6/100], Train Loss: 1.0256, Train Acc: 73.68%, Val Loss: 1.0672, Val Acc: 40.00%\n",
      "Fold 4 Epoch [7/100], Train Loss: 1.0093, Train Acc: 84.21%, Val Loss: 1.0541, Val Acc: 60.00%\n",
      "Fold 4 Epoch [8/100], Train Loss: 0.9932, Train Acc: 84.21%, Val Loss: 1.0408, Val Acc: 60.00%\n",
      "Fold 4 Epoch [9/100], Train Loss: 0.9775, Train Acc: 84.21%, Val Loss: 1.0272, Val Acc: 80.00%\n",
      "Fold 4 Epoch [10/100], Train Loss: 0.9616, Train Acc: 84.21%, Val Loss: 1.0136, Val Acc: 80.00%\n",
      "Fold 4 Epoch [11/100], Train Loss: 0.9458, Train Acc: 89.47%, Val Loss: 1.0002, Val Acc: 80.00%\n",
      "Fold 4 Epoch [12/100], Train Loss: 0.9298, Train Acc: 89.47%, Val Loss: 0.9875, Val Acc: 80.00%\n",
      "Fold 4 Epoch [13/100], Train Loss: 0.9134, Train Acc: 89.47%, Val Loss: 0.9741, Val Acc: 80.00%\n",
      "Fold 4 Epoch [14/100], Train Loss: 0.8967, Train Acc: 89.47%, Val Loss: 0.9604, Val Acc: 80.00%\n",
      "Fold 4 Epoch [15/100], Train Loss: 0.8798, Train Acc: 89.47%, Val Loss: 0.9464, Val Acc: 80.00%\n",
      "Fold 4 Epoch [16/100], Train Loss: 0.8628, Train Acc: 89.47%, Val Loss: 0.9323, Val Acc: 80.00%\n",
      "Fold 4 Epoch [17/100], Train Loss: 0.8456, Train Acc: 89.47%, Val Loss: 0.9172, Val Acc: 80.00%\n",
      "Fold 4 Epoch [18/100], Train Loss: 0.8281, Train Acc: 89.47%, Val Loss: 0.9011, Val Acc: 80.00%\n",
      "Fold 4 Epoch [19/100], Train Loss: 0.8101, Train Acc: 89.47%, Val Loss: 0.8845, Val Acc: 80.00%\n",
      "Fold 4 Epoch [20/100], Train Loss: 0.7917, Train Acc: 89.47%, Val Loss: 0.8679, Val Acc: 80.00%\n",
      "Fold 4 Epoch [21/100], Train Loss: 0.7730, Train Acc: 89.47%, Val Loss: 0.8512, Val Acc: 80.00%\n",
      "Fold 4 Epoch [22/100], Train Loss: 0.7540, Train Acc: 89.47%, Val Loss: 0.8347, Val Acc: 80.00%\n",
      "Fold 4 Epoch [23/100], Train Loss: 0.7348, Train Acc: 89.47%, Val Loss: 0.8179, Val Acc: 80.00%\n",
      "Fold 4 Epoch [24/100], Train Loss: 0.7153, Train Acc: 89.47%, Val Loss: 0.8009, Val Acc: 80.00%\n",
      "Fold 4 Epoch [25/100], Train Loss: 0.6956, Train Acc: 89.47%, Val Loss: 0.7833, Val Acc: 80.00%\n",
      "Fold 4 Epoch [26/100], Train Loss: 0.6759, Train Acc: 89.47%, Val Loss: 0.7659, Val Acc: 80.00%\n",
      "Fold 4 Epoch [27/100], Train Loss: 0.6563, Train Acc: 89.47%, Val Loss: 0.7481, Val Acc: 80.00%\n",
      "Fold 4 Epoch [28/100], Train Loss: 0.6367, Train Acc: 89.47%, Val Loss: 0.7302, Val Acc: 80.00%\n",
      "Fold 4 Epoch [29/100], Train Loss: 0.6172, Train Acc: 89.47%, Val Loss: 0.7123, Val Acc: 80.00%\n",
      "Fold 4 Epoch [30/100], Train Loss: 0.5974, Train Acc: 89.47%, Val Loss: 0.6941, Val Acc: 80.00%\n",
      "Fold 4 Epoch [31/100], Train Loss: 0.5776, Train Acc: 89.47%, Val Loss: 0.6759, Val Acc: 80.00%\n",
      "Fold 4 Epoch [32/100], Train Loss: 0.5577, Train Acc: 89.47%, Val Loss: 0.6577, Val Acc: 80.00%\n",
      "Fold 4 Epoch [33/100], Train Loss: 0.5380, Train Acc: 89.47%, Val Loss: 0.6397, Val Acc: 80.00%\n",
      "Fold 4 Epoch [34/100], Train Loss: 0.5183, Train Acc: 89.47%, Val Loss: 0.6217, Val Acc: 80.00%\n",
      "Fold 4 Epoch [35/100], Train Loss: 0.4987, Train Acc: 89.47%, Val Loss: 0.6036, Val Acc: 80.00%\n",
      "Fold 4 Epoch [36/100], Train Loss: 0.4792, Train Acc: 89.47%, Val Loss: 0.5854, Val Acc: 80.00%\n",
      "Fold 4 Epoch [37/100], Train Loss: 0.4600, Train Acc: 94.74%, Val Loss: 0.5672, Val Acc: 80.00%\n",
      "Fold 4 Epoch [38/100], Train Loss: 0.4411, Train Acc: 94.74%, Val Loss: 0.5484, Val Acc: 80.00%\n",
      "Fold 4 Epoch [39/100], Train Loss: 0.4226, Train Acc: 94.74%, Val Loss: 0.5299, Val Acc: 80.00%\n",
      "Fold 4 Epoch [40/100], Train Loss: 0.4045, Train Acc: 94.74%, Val Loss: 0.5117, Val Acc: 100.00%\n",
      "Fold 4 Epoch [41/100], Train Loss: 0.3869, Train Acc: 94.74%, Val Loss: 0.4939, Val Acc: 100.00%\n",
      "Fold 4 Epoch [42/100], Train Loss: 0.3699, Train Acc: 94.74%, Val Loss: 0.4766, Val Acc: 100.00%\n",
      "Fold 4 Epoch [43/100], Train Loss: 0.3532, Train Acc: 94.74%, Val Loss: 0.4594, Val Acc: 100.00%\n",
      "Fold 4 Epoch [44/100], Train Loss: 0.3371, Train Acc: 94.74%, Val Loss: 0.4423, Val Acc: 100.00%\n",
      "Fold 4 Epoch [45/100], Train Loss: 0.3215, Train Acc: 94.74%, Val Loss: 0.4255, Val Acc: 100.00%\n",
      "Fold 4 Epoch [46/100], Train Loss: 0.3064, Train Acc: 94.74%, Val Loss: 0.4090, Val Acc: 100.00%\n",
      "Fold 4 Epoch [47/100], Train Loss: 0.2917, Train Acc: 94.74%, Val Loss: 0.3922, Val Acc: 100.00%\n",
      "Fold 4 Epoch [48/100], Train Loss: 0.2777, Train Acc: 100.00%, Val Loss: 0.3755, Val Acc: 100.00%\n",
      "Fold 4 Epoch [49/100], Train Loss: 0.2642, Train Acc: 100.00%, Val Loss: 0.3591, Val Acc: 100.00%\n",
      "Fold 4 Epoch [50/100], Train Loss: 0.2513, Train Acc: 100.00%, Val Loss: 0.3432, Val Acc: 100.00%\n",
      "Fold 4 Epoch [51/100], Train Loss: 0.2391, Train Acc: 100.00%, Val Loss: 0.3274, Val Acc: 100.00%\n",
      "Fold 4 Epoch [52/100], Train Loss: 0.2270, Train Acc: 100.00%, Val Loss: 0.3112, Val Acc: 100.00%\n",
      "Fold 4 Epoch [53/100], Train Loss: 0.2155, Train Acc: 100.00%, Val Loss: 0.2953, Val Acc: 100.00%\n",
      "Fold 4 Epoch [54/100], Train Loss: 0.2044, Train Acc: 100.00%, Val Loss: 0.2799, Val Acc: 100.00%\n",
      "Fold 4 Epoch [55/100], Train Loss: 0.1939, Train Acc: 100.00%, Val Loss: 0.2647, Val Acc: 100.00%\n",
      "Fold 4 Epoch [56/100], Train Loss: 0.1839, Train Acc: 100.00%, Val Loss: 0.2501, Val Acc: 100.00%\n",
      "Fold 4 Epoch [57/100], Train Loss: 0.1743, Train Acc: 100.00%, Val Loss: 0.2359, Val Acc: 100.00%\n",
      "Fold 4 Epoch [58/100], Train Loss: 0.1653, Train Acc: 100.00%, Val Loss: 0.2227, Val Acc: 100.00%\n",
      "Fold 4 Epoch [59/100], Train Loss: 0.1567, Train Acc: 100.00%, Val Loss: 0.2100, Val Acc: 100.00%\n",
      "Fold 4 Epoch [60/100], Train Loss: 0.1486, Train Acc: 100.00%, Val Loss: 0.1980, Val Acc: 100.00%\n",
      "Fold 4 Epoch [61/100], Train Loss: 0.1408, Train Acc: 100.00%, Val Loss: 0.1864, Val Acc: 100.00%\n",
      "Fold 4 Epoch [62/100], Train Loss: 0.1334, Train Acc: 100.00%, Val Loss: 0.1753, Val Acc: 100.00%\n",
      "Fold 4 Epoch [63/100], Train Loss: 0.1264, Train Acc: 100.00%, Val Loss: 0.1649, Val Acc: 100.00%\n",
      "Fold 4 Epoch [64/100], Train Loss: 0.1197, Train Acc: 100.00%, Val Loss: 0.1550, Val Acc: 100.00%\n",
      "Fold 4 Epoch [65/100], Train Loss: 0.1133, Train Acc: 100.00%, Val Loss: 0.1455, Val Acc: 100.00%\n",
      "Fold 4 Epoch [66/100], Train Loss: 0.1072, Train Acc: 100.00%, Val Loss: 0.1366, Val Acc: 100.00%\n",
      "Fold 4 Epoch [67/100], Train Loss: 0.1014, Train Acc: 100.00%, Val Loss: 0.1281, Val Acc: 100.00%\n",
      "Fold 4 Epoch [68/100], Train Loss: 0.0959, Train Acc: 100.00%, Val Loss: 0.1199, Val Acc: 100.00%\n",
      "Fold 4 Epoch [69/100], Train Loss: 0.0907, Train Acc: 100.00%, Val Loss: 0.1122, Val Acc: 100.00%\n",
      "Fold 4 Epoch [70/100], Train Loss: 0.0857, Train Acc: 100.00%, Val Loss: 0.1049, Val Acc: 100.00%\n",
      "Fold 4 Epoch [71/100], Train Loss: 0.0811, Train Acc: 100.00%, Val Loss: 0.0982, Val Acc: 100.00%\n",
      "Fold 4 Epoch [72/100], Train Loss: 0.0766, Train Acc: 100.00%, Val Loss: 0.0916, Val Acc: 100.00%\n",
      "Fold 4 Epoch [73/100], Train Loss: 0.0723, Train Acc: 100.00%, Val Loss: 0.0854, Val Acc: 100.00%\n",
      "Fold 4 Epoch [74/100], Train Loss: 0.0683, Train Acc: 100.00%, Val Loss: 0.0796, Val Acc: 100.00%\n",
      "Fold 4 Epoch [75/100], Train Loss: 0.0646, Train Acc: 100.00%, Val Loss: 0.0741, Val Acc: 100.00%\n",
      "Fold 4 Epoch [76/100], Train Loss: 0.0611, Train Acc: 100.00%, Val Loss: 0.0690, Val Acc: 100.00%\n",
      "Fold 4 Epoch [77/100], Train Loss: 0.0578, Train Acc: 100.00%, Val Loss: 0.0643, Val Acc: 100.00%\n",
      "Fold 4 Epoch [78/100], Train Loss: 0.0547, Train Acc: 100.00%, Val Loss: 0.0599, Val Acc: 100.00%\n",
      "Fold 4 Epoch [79/100], Train Loss: 0.0517, Train Acc: 100.00%, Val Loss: 0.0558, Val Acc: 100.00%\n",
      "Fold 4 Epoch [80/100], Train Loss: 0.0490, Train Acc: 100.00%, Val Loss: 0.0519, Val Acc: 100.00%\n",
      "Fold 4 Epoch [81/100], Train Loss: 0.0464, Train Acc: 100.00%, Val Loss: 0.0486, Val Acc: 100.00%\n",
      "Fold 4 Epoch [82/100], Train Loss: 0.0441, Train Acc: 100.00%, Val Loss: 0.0456, Val Acc: 100.00%\n",
      "Fold 4 Epoch [83/100], Train Loss: 0.0419, Train Acc: 100.00%, Val Loss: 0.0428, Val Acc: 100.00%\n",
      "Fold 4 Epoch [84/100], Train Loss: 0.0398, Train Acc: 100.00%, Val Loss: 0.0401, Val Acc: 100.00%\n",
      "Fold 4 Epoch [85/100], Train Loss: 0.0379, Train Acc: 100.00%, Val Loss: 0.0378, Val Acc: 100.00%\n",
      "Fold 4 Epoch [86/100], Train Loss: 0.0361, Train Acc: 100.00%, Val Loss: 0.0356, Val Acc: 100.00%\n",
      "Fold 4 Epoch [87/100], Train Loss: 0.0343, Train Acc: 100.00%, Val Loss: 0.0335, Val Acc: 100.00%\n",
      "Fold 4 Epoch [88/100], Train Loss: 0.0327, Train Acc: 100.00%, Val Loss: 0.0316, Val Acc: 100.00%\n",
      "Fold 4 Epoch [89/100], Train Loss: 0.0312, Train Acc: 100.00%, Val Loss: 0.0298, Val Acc: 100.00%\n",
      "Fold 4 Epoch [90/100], Train Loss: 0.0298, Train Acc: 100.00%, Val Loss: 0.0282, Val Acc: 100.00%\n",
      "Fold 4 Epoch [91/100], Train Loss: 0.0284, Train Acc: 100.00%, Val Loss: 0.0266, Val Acc: 100.00%\n",
      "Fold 4 Epoch [92/100], Train Loss: 0.0270, Train Acc: 100.00%, Val Loss: 0.0250, Val Acc: 100.00%\n",
      "Fold 4 Epoch [93/100], Train Loss: 0.0258, Train Acc: 100.00%, Val Loss: 0.0236, Val Acc: 100.00%\n",
      "Fold 4 Epoch [94/100], Train Loss: 0.0246, Train Acc: 100.00%, Val Loss: 0.0223, Val Acc: 100.00%\n",
      "Fold 4 Epoch [95/100], Train Loss: 0.0234, Train Acc: 100.00%, Val Loss: 0.0210, Val Acc: 100.00%\n",
      "Fold 4 Epoch [96/100], Train Loss: 0.0223, Train Acc: 100.00%, Val Loss: 0.0198, Val Acc: 100.00%\n",
      "Fold 4 Epoch [97/100], Train Loss: 0.0213, Train Acc: 100.00%, Val Loss: 0.0187, Val Acc: 100.00%\n",
      "Fold 4 Epoch [98/100], Train Loss: 0.0204, Train Acc: 100.00%, Val Loss: 0.0177, Val Acc: 100.00%\n",
      "Fold 4 Epoch [99/100], Train Loss: 0.0195, Train Acc: 100.00%, Val Loss: 0.0168, Val Acc: 100.00%\n",
      "Fold 4 Epoch [100/100], Train Loss: 0.0187, Train Acc: 100.00%, Val Loss: 0.0160, Val Acc: 100.00%\n",
      "Fold 4 Best Accuracy: 100.00%\n",
      "Fold 5/5\n",
      "Fold 5 Epoch [1/100], Train Loss: 1.1057, Train Acc: 0.00%, Val Loss: 1.1144, Val Acc: 0.00%\n",
      "Fold 5 Epoch [2/100], Train Loss: 1.0838, Train Acc: 45.00%, Val Loss: 1.0955, Val Acc: 25.00%\n",
      "Fold 5 Epoch [3/100], Train Loss: 1.0641, Train Acc: 65.00%, Val Loss: 1.0781, Val Acc: 25.00%\n",
      "Fold 5 Epoch [4/100], Train Loss: 1.0456, Train Acc: 75.00%, Val Loss: 1.0607, Val Acc: 25.00%\n",
      "Fold 5 Epoch [5/100], Train Loss: 1.0271, Train Acc: 80.00%, Val Loss: 1.0429, Val Acc: 50.00%\n",
      "Fold 5 Epoch [6/100], Train Loss: 1.0088, Train Acc: 80.00%, Val Loss: 1.0249, Val Acc: 50.00%\n",
      "Fold 5 Epoch [7/100], Train Loss: 0.9908, Train Acc: 85.00%, Val Loss: 1.0067, Val Acc: 100.00%\n",
      "Fold 5 Epoch [8/100], Train Loss: 0.9728, Train Acc: 85.00%, Val Loss: 0.9890, Val Acc: 100.00%\n",
      "Fold 5 Epoch [9/100], Train Loss: 0.9551, Train Acc: 85.00%, Val Loss: 0.9717, Val Acc: 100.00%\n",
      "Fold 5 Epoch [10/100], Train Loss: 0.9372, Train Acc: 95.00%, Val Loss: 0.9550, Val Acc: 100.00%\n",
      "Fold 5 Epoch [11/100], Train Loss: 0.9196, Train Acc: 95.00%, Val Loss: 0.9384, Val Acc: 100.00%\n",
      "Fold 5 Epoch [12/100], Train Loss: 0.9020, Train Acc: 95.00%, Val Loss: 0.9218, Val Acc: 100.00%\n",
      "Fold 5 Epoch [13/100], Train Loss: 0.8840, Train Acc: 95.00%, Val Loss: 0.9051, Val Acc: 100.00%\n",
      "Fold 5 Epoch [14/100], Train Loss: 0.8658, Train Acc: 95.00%, Val Loss: 0.8880, Val Acc: 100.00%\n",
      "Fold 5 Epoch [15/100], Train Loss: 0.8472, Train Acc: 95.00%, Val Loss: 0.8707, Val Acc: 100.00%\n",
      "Fold 5 Epoch [16/100], Train Loss: 0.8282, Train Acc: 100.00%, Val Loss: 0.8541, Val Acc: 100.00%\n",
      "Fold 5 Epoch [17/100], Train Loss: 0.8091, Train Acc: 100.00%, Val Loss: 0.8375, Val Acc: 100.00%\n",
      "Fold 5 Epoch [18/100], Train Loss: 0.7897, Train Acc: 100.00%, Val Loss: 0.8208, Val Acc: 100.00%\n",
      "Fold 5 Epoch [19/100], Train Loss: 0.7701, Train Acc: 100.00%, Val Loss: 0.8038, Val Acc: 100.00%\n",
      "Fold 5 Epoch [20/100], Train Loss: 0.7506, Train Acc: 100.00%, Val Loss: 0.7866, Val Acc: 100.00%\n",
      "Fold 5 Epoch [21/100], Train Loss: 0.7310, Train Acc: 100.00%, Val Loss: 0.7691, Val Acc: 100.00%\n",
      "Fold 5 Epoch [22/100], Train Loss: 0.7114, Train Acc: 100.00%, Val Loss: 0.7513, Val Acc: 100.00%\n",
      "Fold 5 Epoch [23/100], Train Loss: 0.6914, Train Acc: 100.00%, Val Loss: 0.7334, Val Acc: 100.00%\n",
      "Fold 5 Epoch [24/100], Train Loss: 0.6711, Train Acc: 100.00%, Val Loss: 0.7150, Val Acc: 100.00%\n",
      "Fold 5 Epoch [25/100], Train Loss: 0.6506, Train Acc: 100.00%, Val Loss: 0.6963, Val Acc: 100.00%\n",
      "Fold 5 Epoch [26/100], Train Loss: 0.6297, Train Acc: 100.00%, Val Loss: 0.6774, Val Acc: 100.00%\n",
      "Fold 5 Epoch [27/100], Train Loss: 0.6087, Train Acc: 100.00%, Val Loss: 0.6585, Val Acc: 100.00%\n",
      "Fold 5 Epoch [28/100], Train Loss: 0.5877, Train Acc: 100.00%, Val Loss: 0.6396, Val Acc: 100.00%\n",
      "Fold 5 Epoch [29/100], Train Loss: 0.5665, Train Acc: 100.00%, Val Loss: 0.6206, Val Acc: 100.00%\n",
      "Fold 5 Epoch [30/100], Train Loss: 0.5455, Train Acc: 100.00%, Val Loss: 0.6016, Val Acc: 100.00%\n",
      "Fold 5 Epoch [31/100], Train Loss: 0.5244, Train Acc: 100.00%, Val Loss: 0.5826, Val Acc: 100.00%\n",
      "Fold 5 Epoch [32/100], Train Loss: 0.5034, Train Acc: 100.00%, Val Loss: 0.5636, Val Acc: 100.00%\n",
      "Fold 5 Epoch [33/100], Train Loss: 0.4824, Train Acc: 100.00%, Val Loss: 0.5447, Val Acc: 100.00%\n",
      "Fold 5 Epoch [34/100], Train Loss: 0.4614, Train Acc: 100.00%, Val Loss: 0.5256, Val Acc: 100.00%\n",
      "Fold 5 Epoch [35/100], Train Loss: 0.4406, Train Acc: 100.00%, Val Loss: 0.5064, Val Acc: 100.00%\n",
      "Fold 5 Epoch [36/100], Train Loss: 0.4200, Train Acc: 100.00%, Val Loss: 0.4875, Val Acc: 100.00%\n",
      "Fold 5 Epoch [37/100], Train Loss: 0.3999, Train Acc: 100.00%, Val Loss: 0.4689, Val Acc: 100.00%\n",
      "Fold 5 Epoch [38/100], Train Loss: 0.3802, Train Acc: 100.00%, Val Loss: 0.4506, Val Acc: 100.00%\n",
      "Fold 5 Epoch [39/100], Train Loss: 0.3608, Train Acc: 100.00%, Val Loss: 0.4325, Val Acc: 100.00%\n",
      "Fold 5 Epoch [40/100], Train Loss: 0.3419, Train Acc: 100.00%, Val Loss: 0.4148, Val Acc: 100.00%\n",
      "Fold 5 Epoch [41/100], Train Loss: 0.3235, Train Acc: 100.00%, Val Loss: 0.3974, Val Acc: 100.00%\n",
      "Fold 5 Epoch [42/100], Train Loss: 0.3055, Train Acc: 100.00%, Val Loss: 0.3803, Val Acc: 100.00%\n",
      "Fold 5 Epoch [43/100], Train Loss: 0.2882, Train Acc: 100.00%, Val Loss: 0.3636, Val Acc: 100.00%\n",
      "Fold 5 Epoch [44/100], Train Loss: 0.2715, Train Acc: 100.00%, Val Loss: 0.3474, Val Acc: 100.00%\n",
      "Fold 5 Epoch [45/100], Train Loss: 0.2555, Train Acc: 100.00%, Val Loss: 0.3315, Val Acc: 100.00%\n",
      "Fold 5 Epoch [46/100], Train Loss: 0.2402, Train Acc: 100.00%, Val Loss: 0.3162, Val Acc: 100.00%\n",
      "Fold 5 Epoch [47/100], Train Loss: 0.2255, Train Acc: 100.00%, Val Loss: 0.3012, Val Acc: 100.00%\n",
      "Fold 5 Epoch [48/100], Train Loss: 0.2115, Train Acc: 100.00%, Val Loss: 0.2868, Val Acc: 100.00%\n",
      "Fold 5 Epoch [49/100], Train Loss: 0.1981, Train Acc: 100.00%, Val Loss: 0.2729, Val Acc: 100.00%\n",
      "Fold 5 Epoch [50/100], Train Loss: 0.1854, Train Acc: 100.00%, Val Loss: 0.2594, Val Acc: 100.00%\n",
      "Fold 5 Epoch [51/100], Train Loss: 0.1733, Train Acc: 100.00%, Val Loss: 0.2464, Val Acc: 100.00%\n",
      "Fold 5 Epoch [52/100], Train Loss: 0.1618, Train Acc: 100.00%, Val Loss: 0.2337, Val Acc: 100.00%\n",
      "Fold 5 Epoch [53/100], Train Loss: 0.1509, Train Acc: 100.00%, Val Loss: 0.2215, Val Acc: 100.00%\n",
      "Fold 5 Epoch [54/100], Train Loss: 0.1407, Train Acc: 100.00%, Val Loss: 0.2100, Val Acc: 100.00%\n",
      "Fold 5 Epoch [55/100], Train Loss: 0.1312, Train Acc: 100.00%, Val Loss: 0.1988, Val Acc: 100.00%\n",
      "Fold 5 Epoch [56/100], Train Loss: 0.1223, Train Acc: 100.00%, Val Loss: 0.1883, Val Acc: 100.00%\n",
      "Fold 5 Epoch [57/100], Train Loss: 0.1138, Train Acc: 100.00%, Val Loss: 0.1780, Val Acc: 100.00%\n",
      "Fold 5 Epoch [58/100], Train Loss: 0.1059, Train Acc: 100.00%, Val Loss: 0.1683, Val Acc: 100.00%\n",
      "Fold 5 Epoch [59/100], Train Loss: 0.0985, Train Acc: 100.00%, Val Loss: 0.1590, Val Acc: 100.00%\n",
      "Fold 5 Epoch [60/100], Train Loss: 0.0918, Train Acc: 100.00%, Val Loss: 0.1503, Val Acc: 100.00%\n",
      "Fold 5 Epoch [61/100], Train Loss: 0.0855, Train Acc: 100.00%, Val Loss: 0.1422, Val Acc: 100.00%\n",
      "Fold 5 Epoch [62/100], Train Loss: 0.0795, Train Acc: 100.00%, Val Loss: 0.1343, Val Acc: 100.00%\n",
      "Fold 5 Epoch [63/100], Train Loss: 0.0740, Train Acc: 100.00%, Val Loss: 0.1269, Val Acc: 100.00%\n",
      "Fold 5 Epoch [64/100], Train Loss: 0.0688, Train Acc: 100.00%, Val Loss: 0.1198, Val Acc: 100.00%\n",
      "Fold 5 Epoch [65/100], Train Loss: 0.0640, Train Acc: 100.00%, Val Loss: 0.1131, Val Acc: 100.00%\n",
      "Fold 5 Epoch [66/100], Train Loss: 0.0596, Train Acc: 100.00%, Val Loss: 0.1067, Val Acc: 100.00%\n",
      "Fold 5 Epoch [67/100], Train Loss: 0.0555, Train Acc: 100.00%, Val Loss: 0.1007, Val Acc: 100.00%\n",
      "Fold 5 Epoch [68/100], Train Loss: 0.0517, Train Acc: 100.00%, Val Loss: 0.0950, Val Acc: 100.00%\n",
      "Fold 5 Epoch [69/100], Train Loss: 0.0482, Train Acc: 100.00%, Val Loss: 0.0900, Val Acc: 100.00%\n",
      "Fold 5 Epoch [70/100], Train Loss: 0.0450, Train Acc: 100.00%, Val Loss: 0.0853, Val Acc: 100.00%\n",
      "Fold 5 Epoch [71/100], Train Loss: 0.0421, Train Acc: 100.00%, Val Loss: 0.0809, Val Acc: 100.00%\n",
      "Fold 5 Epoch [72/100], Train Loss: 0.0394, Train Acc: 100.00%, Val Loss: 0.0767, Val Acc: 100.00%\n",
      "Fold 5 Epoch [73/100], Train Loss: 0.0369, Train Acc: 100.00%, Val Loss: 0.0727, Val Acc: 100.00%\n",
      "Fold 5 Epoch [74/100], Train Loss: 0.0346, Train Acc: 100.00%, Val Loss: 0.0691, Val Acc: 100.00%\n",
      "Fold 5 Epoch [75/100], Train Loss: 0.0324, Train Acc: 100.00%, Val Loss: 0.0658, Val Acc: 100.00%\n",
      "Fold 5 Epoch [76/100], Train Loss: 0.0305, Train Acc: 100.00%, Val Loss: 0.0627, Val Acc: 100.00%\n",
      "Fold 5 Epoch [77/100], Train Loss: 0.0287, Train Acc: 100.00%, Val Loss: 0.0598, Val Acc: 100.00%\n",
      "Fold 5 Epoch [78/100], Train Loss: 0.0270, Train Acc: 100.00%, Val Loss: 0.0570, Val Acc: 100.00%\n",
      "Fold 5 Epoch [79/100], Train Loss: 0.0255, Train Acc: 100.00%, Val Loss: 0.0544, Val Acc: 100.00%\n",
      "Fold 5 Epoch [80/100], Train Loss: 0.0241, Train Acc: 100.00%, Val Loss: 0.0520, Val Acc: 100.00%\n",
      "Fold 5 Epoch [81/100], Train Loss: 0.0228, Train Acc: 100.00%, Val Loss: 0.0497, Val Acc: 100.00%\n",
      "Fold 5 Epoch [82/100], Train Loss: 0.0216, Train Acc: 100.00%, Val Loss: 0.0475, Val Acc: 100.00%\n",
      "Fold 5 Epoch [83/100], Train Loss: 0.0204, Train Acc: 100.00%, Val Loss: 0.0455, Val Acc: 100.00%\n",
      "Fold 5 Epoch [84/100], Train Loss: 0.0194, Train Acc: 100.00%, Val Loss: 0.0437, Val Acc: 100.00%\n",
      "Fold 5 Epoch [85/100], Train Loss: 0.0184, Train Acc: 100.00%, Val Loss: 0.0419, Val Acc: 100.00%\n",
      "Fold 5 Epoch [86/100], Train Loss: 0.0175, Train Acc: 100.00%, Val Loss: 0.0402, Val Acc: 100.00%\n",
      "Fold 5 Epoch [87/100], Train Loss: 0.0166, Train Acc: 100.00%, Val Loss: 0.0386, Val Acc: 100.00%\n",
      "Fold 5 Epoch [88/100], Train Loss: 0.0158, Train Acc: 100.00%, Val Loss: 0.0371, Val Acc: 100.00%\n",
      "Fold 5 Epoch [89/100], Train Loss: 0.0150, Train Acc: 100.00%, Val Loss: 0.0356, Val Acc: 100.00%\n",
      "Fold 5 Epoch [90/100], Train Loss: 0.0143, Train Acc: 100.00%, Val Loss: 0.0343, Val Acc: 100.00%\n",
      "Fold 5 Epoch [91/100], Train Loss: 0.0136, Train Acc: 100.00%, Val Loss: 0.0330, Val Acc: 100.00%\n",
      "Fold 5 Epoch [92/100], Train Loss: 0.0130, Train Acc: 100.00%, Val Loss: 0.0318, Val Acc: 100.00%\n",
      "Fold 5 Epoch [93/100], Train Loss: 0.0124, Train Acc: 100.00%, Val Loss: 0.0306, Val Acc: 100.00%\n",
      "Fold 5 Epoch [94/100], Train Loss: 0.0118, Train Acc: 100.00%, Val Loss: 0.0295, Val Acc: 100.00%\n",
      "Fold 5 Epoch [95/100], Train Loss: 0.0112, Train Acc: 100.00%, Val Loss: 0.0284, Val Acc: 100.00%\n",
      "Fold 5 Epoch [96/100], Train Loss: 0.0107, Train Acc: 100.00%, Val Loss: 0.0275, Val Acc: 100.00%\n",
      "Fold 5 Epoch [97/100], Train Loss: 0.0103, Train Acc: 100.00%, Val Loss: 0.0265, Val Acc: 100.00%\n",
      "Fold 5 Epoch [98/100], Train Loss: 0.0098, Train Acc: 100.00%, Val Loss: 0.0257, Val Acc: 100.00%\n",
      "Fold 5 Epoch [99/100], Train Loss: 0.0094, Train Acc: 100.00%, Val Loss: 0.0248, Val Acc: 100.00%\n",
      "Fold 5 Epoch [100/100], Train Loss: 0.0090, Train Acc: 100.00%, Val Loss: 0.0240, Val Acc: 100.00%\n",
      "Fold 5 Best Accuracy: 100.00%\n",
      "\n",
      "5-Fold CV Results (Stephen Only):\n",
      "Average Accuracy: 100.00% (0.00)\n",
      "Fold Accuracies: ['100.00%', '100.00%', '100.00%', '100.00%', '100.00%']\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation setup\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "fold_losses = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_all)):\n",
    "    print(f'Fold {fold + 1}/{n_folds}')\n",
    "    \n",
    "    # Split data\n",
    "    X_train_fold = X_all[train_idx]\n",
    "    y_train_fold = y_all[train_idx]\n",
    "    X_val_fold = X_all[val_idx]\n",
    "    y_val_fold = y_all[val_idx]\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_train_fold, y_train_fold)\n",
    "    val_dataset = TensorDataset(X_val_fold, y_val_fold)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = FeatureModel(num_classes=NUM_CLASSES, input_features=18)\n",
    "    optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-4)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_accuracy = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        train_loss, train_accuracy = evaluate_model(train_loader, model, criterion)\n",
    "        val_loss, val_accuracy = evaluate_model(val_loader, model, criterion)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Log and print\n",
    "        log_message = (f\"Fold {fold+1} Epoch [{epoch+1}/100], \"\n",
    "                      f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
    "                      f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "        logging.info(log_message)\n",
    "        print(log_message)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_val_loss = val_loss\n",
    "            save_best_model(epoch, model, optimizer, val_loss, val_accuracy, train_losses, val_losses)\n",
    "    \n",
    "    # Store fold results\n",
    "    fold_accuracies.append(best_val_accuracy)\n",
    "    fold_losses.append(best_val_loss)\n",
    "    print(f'Fold {fold + 1} Best Accuracy: {best_val_accuracy:.2f}%')\n",
    "    \n",
    "# Print summary\n",
    "mean_accuracy = np.mean(fold_accuracies)\n",
    "std_accuracy = np.std(fold_accuracies)\n",
    "print(f'\\n5-Fold CV Results (Stephen Only):')\n",
    "print(f'Average Accuracy: {mean_accuracy:.2f}% ({std_accuracy:.2f})')\n",
    "print(f'Fold Accuracies: {[f\"{acc:.2f}%\" for acc in fold_accuracies]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "37f5a0a4-374a-40dd-9d27-81279b433394",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (14,) and (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[164], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Plotting\u001b[39;00m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold_accuracies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbo-\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChunk Size (seconds)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy (\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\matplotlib\\pyplot.py:3829\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3821\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   3822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplot\u001b[39m(\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3827\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3828\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[1;32m-> 3829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3832\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\matplotlib\\axes\\_axes.py:1777\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1534\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1535\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1536\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1776\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1777\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1778\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1779\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\matplotlib\\axes\\_base.py:297\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    295\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    296\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 297\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_kwargs\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\matplotlib\\axes\\_base.py:494\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    491\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    495\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    498\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (14,) and (5,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0UAAAH/CAYAAACYSXaPAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHl9JREFUeJzt3XtsV/X9+PF3AQHNBHUMEFZl6rxNBQVBQGJc0CYaHH8sY2iEEC9zOqMQJ+AFvOO8hWRWiajTZHGgRJwRUqdMYhwsRNBEM8AoKsRYLnNQRAWFzy/v8/21o9g6ir1QXo9Hcgaf03Pa81ne1D57znmfslKpVEoAAABBdWjrAwAAAGhLoggAAAhNFAEAAKGJIgAAIDRRBAAAhCaKAACA0EQRAAAQmigCAABCE0UAAEBooggAAAityVH0+uuvp1GjRqU+ffqksrKy9MILL/zPfRYvXpzOOOOM1KVLl3Tcccelp556al+PFwAAoG2jaNu2bal///6psrJyr7b/8MMP04UXXpjOPffc9Pbbb6frr78+XX755enll1/el+MFAABoVmWlUqm0zzuXlaX58+en0aNHN7rN5MmT04IFC9K7775bt+7Xv/512rx5c6qqqtrXLw0AANAsOqUWtnTp0jRy5Mh66yoqKoozRo3Zvn17sdTatWtX+uyzz9IPf/jDIsQAAICYSqVS2rp1a3E7T4cOHdpHFFVXV6devXrVW5df19TUpC+//DIdfPDB39pnxowZ6fbbb2/pQwMAANqpdevWpR//+MftI4r2xdSpU9OkSZPqXm/ZsiUdddRRxRvv1q1bmx4bAADQdvLJlfLy8nTooYc22+ds8Sjq3bt3Wr9+fb11+XWOm4bOEmV5lrq87CnvI4oAAICyZrytpsWfUzR06NC0aNGieuteeeWVYj0AAEBba3IUff7558XU2nmpnXI7/33t2rV1l76NGzeubvurrroqrVmzJt14441p1apV6ZFHHknPPvtsmjhxYnO+DwAAgNaJojfffDOdfvrpxZLle3/y36dNm1a8/vTTT+sCKfvJT35STMmdzw7l5xs9+OCD6fHHHy9moAMAAGjXzylqzZupunfvXky44J4iAACIq6YF2qDF7ykCAADYn4kiAAAgNFEEAACEJooAAIDQRBEAABCaKAIAAEITRQAAQGiiCAAACE0UAQAAoYkiAAAgNFEEAACEJooAAIDQRBEAABCaKAIAAEITRQAAQGiiCAAACE0UAQAAoYkiAAAgNFEEAACEJooAAIDQRBEAABCaKAIAAEITRQAAQGiiCAAACE0UAQAAoYkiAAAgNFEEAACEJooAAIDQRBEAABCaKAIAAEITRQAAQGiiCAAACE0UAQAAoYkiAAAgNFEEAACEJooAAIDQRBEAABCaKAIAAEITRQAAQGiiCAAACE0UAQAAoYkiAAAgNFEEAACEJooAAIDQRBEAABCaKAIAAEITRQAAQGiiCAAACE0UAQAAoYkiAAAgNFEEAACEJooAAIDQRBEAABCaKAIAAEITRQAAQGiiCAAACE0UAQAAoYkiAAAgNFEEAACEJooAAIDQRBEAABCaKAIAAEITRQAAQGiiCAAACE0UAQAAoYkiAAAgNFEEAACEJooAAIDQRBEAABCaKAIAAEITRQAAQGiiCAAACE0UAQAAoYkiAAAgNFEEAACEJooAAIDQRBEAABCaKAIAAEITRQAAQGiiCAAACE0UAQAAoe1TFFVWVqZ+/fqlrl27piFDhqRly5Z95/YzZ85MJ5xwQjr44INTeXl5mjhxYvrqq6/29ZgBAADaLormzp2bJk2alKZPn55WrFiR+vfvnyoqKtKGDRsa3P6ZZ55JU6ZMKbZfuXJleuKJJ4rPcdNNNzXH8QMAALRuFD300EPpiiuuSBMmTEgnn3xymjVrVjrkkEPSk08+2eD2S5YsScOHD08XX3xxcXbp/PPPT2PHjv2fZ5cAAAD2uyjasWNHWr58eRo5cuR/P0GHDsXrpUuXNrjPsGHDin1qI2jNmjVp4cKF6YILLvi+xw4AAPC9dWrKxps2bUo7d+5MvXr1qrc+v161alWD++QzRHm/s88+O5VKpfTNN9+kq6666jsvn9u+fXux1KqpqWnKYQIAAOw/s88tXrw43XPPPemRRx4p7kF6/vnn04IFC9Kdd97Z6D4zZsxI3bt3r1vy5AwAAAAtoayUT9804fK5fP/QvHnz0ujRo+vWjx8/Pm3evDn99a9//dY+I0aMSGeddVa6//7769b9+c9/TldeeWX6/PPPi8vv9uZMUQ6jLVu2pG7dujX1PQIAAAeImpqa4sRJc7ZBk84Ude7cOQ0cODAtWrSobt2uXbuK10OHDm1wny+++OJb4dOxY8fiz8Z6rEuXLsUb3H0BAABo83uKsjwddz4zNGjQoDR48ODiGUTbtm0rZqPLxo0bl/r27VtcApeNGjWqmLHu9NNPL55p9P7776dbb721WF8bRwAAAO0misaMGZM2btyYpk2blqqrq9OAAQNSVVVV3eQLa9eurXdm6JZbbkllZWXFn5988kn60Y9+VATR3Xff3bzvBAAAoKXvKTqQrhsEAADanza/pwgAAOBAI4oAAIDQRBEAABCaKAIAAEITRQAAQGiiCAAACE0UAQAAoYkiAAAgNFEEAACEJooAAIDQRBEAABCaKAIAAEITRQAAQGiiCAAACE0UAQAAoYkiAAAgNFEEAACEJooAAIDQRBEAABCaKAIAAEITRQAAQGiiCAAACE0UAQAAoYkiAAAgNFEEAACEJooAAIDQRBEAABCaKAIAAEITRQAAQGiiCAAACE0UAQAAoYkiAAAgNFEEAACEJooAAIDQRBEAABCaKAIAAEITRQAAQGiiCAAACE0UAQAAoYkiAAAgNFEEAACEJooAAIDQRBEAABCaKAIAAEITRQAAQGiiCAAACE0UAQAAoYkiAAAgNFEEAACEJooAAIDQRBEAABCaKAIAAEITRQAAQGiiCAAACE0UAQAAoYkiAAAgNFEEAACEJooAAIDQRBEAABCaKAIAAEITRQAAQGiiCAAACE0UAQAAoYkiAAAgNFEEAACEJooAAIDQRBEAABCaKAIAAEITRQAAQGiiCAAACE0UAQAAoYkiAAAgNFEEAACEJooAAIDQRBEAABCaKAIAAEITRQAAQGiiCAAACE0UAQAAoYkiAAAgNFEEAACEJooAAIDQ9imKKisrU79+/VLXrl3TkCFD0rJly75z+82bN6drrrkmHXnkkalLly7p+OOPTwsXLtzXYwYAAGg2nZq6w9y5c9OkSZPSrFmziiCaOXNmqqioSKtXr049e/b81vY7duxI5513XvGxefPmpb59+6aPP/44HXbYYc31HgAAAPZZWalUKjVlhxxCZ555Znr44YeL17t27Url5eXp2muvTVOmTPnW9jme7r///rRq1ap00EEH7dNB1tTUpO7du6ctW7akbt267dPnAAAA2r+aFmiDJl0+l8/6LF++PI0cOfK/n6BDh+L10qVLG9znxRdfTEOHDi0un+vVq1c65ZRT0j333JN27tzZ6NfZvn178WZ3XwAAAFpCk6Jo06ZNRczkuNldfl1dXd3gPmvWrCkum8v75fuIbr311vTggw+mu+66q9GvM2PGjKL+apd8JgoAAKBdzj6XL6/L9xM99thjaeDAgWnMmDHp5ptvLi6ra8zUqVOL02G1y7p161r6MAEAgKCaNNFCjx49UseOHdP69evrrc+ve/fu3eA+eca5fC9R3q/WSSedVJxZypfjde7c+Vv75Bnq8gIAALBfnSnKAZPP9ixatKjemaD8Ot831JDhw4en999/v9iu1nvvvVfEUkNBBAAAsF9fPpen4549e3Z6+umn08qVK9Nvf/vbtG3btjRhwoTi4+PGjSsuf6uVP/7ZZ5+l6667roihBQsWFBMt5IkXAAAA2t1zivI9QRs3bkzTpk0rLoEbMGBAqqqqqpt8Ye3atcWMdLXyJAkvv/xymjhxYjrttNOK5xTlQJo8eXLzvhMAAIDWeE5RW/CcIgAAYL94ThEAAMCBRhQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAIbZ+iqLKyMvXr1y917do1DRkyJC1btmyv9pszZ04qKytLo0eP3pcvCwAA0PZRNHfu3DRp0qQ0ffr0tGLFitS/f/9UUVGRNmzY8J37ffTRR+mGG25II0aM+D7HCwAA0LZR9NBDD6UrrrgiTZgwIZ188slp1qxZ6ZBDDklPPvlko/vs3LkzXXLJJen2229PxxxzzPc9ZgAAgLaJoh07dqTly5enkSNH/vcTdOhQvF66dGmj+91xxx2pZ8+e6bLLLturr7N9+/ZUU1NTbwEAAGjzKNq0aVNx1qdXr1711ufX1dXVDe7zxhtvpCeeeCLNnj17r7/OjBkzUvfu3euW8vLyphwmAADA/jH73NatW9Oll15aBFGPHj32er+pU6emLVu21C3r1q1rycMEAAAC69SUjXPYdOzYMa1fv77e+vy6d+/e39r+gw8+KCZYGDVqVN26Xbt2/d8X7tQprV69Oh177LHf2q9Lly7FAgAAsF+dKercuXMaOHBgWrRoUb3Iya+HDh36re1PPPHE9M4776S33367brnooovSueeeW/zdZXEAAEC7OlOU5em4x48fnwYNGpQGDx6cZs6cmbZt21bMRpeNGzcu9e3bt7gvKD/H6JRTTqm3/2GHHVb8ued6AACAdhFFY8aMSRs3bkzTpk0rJlcYMGBAqqqqqpt8Ye3atcWMdAAAAO1BWalUKqX9XJ6SO89Clydd6NatW1sfDgAAcAC1gVM6AABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAIbZ+iqLKyMvXr1y917do1DRkyJC1btqzRbWfPnp1GjBiRDj/88GIZOXLkd24PAACwX0fR3Llz06RJk9L06dPTihUrUv/+/VNFRUXasGFDg9svXrw4jR07Nr322mtp6dKlqby8PJ1//vnpk08+aY7jBwAA+F7KSqVSqSk75DNDZ555Znr44YeL17t27SpC59prr01Tpkz5n/vv3LmzOGOU9x83btxefc2amprUvXv3tGXLltStW7emHC4AAHAAqWmBNmjSmaIdO3ak5cuXF5fA1X2CDh2K1/ks0N744osv0tdff52OOOKIRrfZvn178WZ3XwAAAFpCk6Jo06ZNxZmeXr161VufX1dXV+/V55g8eXLq06dPvbDa04wZM4r6q13ymSgAAIB2P/vcvffem+bMmZPmz59fTNLQmKlTpxanw2qXdevWteZhAgAAgXRqysY9evRIHTt2TOvXr6+3Pr/u3bv3d+77wAMPFFH06quvptNOO+07t+3SpUuxAAAA7Fdnijp37pwGDhyYFi1aVLcuT7SQXw8dOrTR/e6777505513pqqqqjRo0KDvd8QAAABtdaYoy9Nxjx8/voibwYMHp5kzZ6Zt27alCRMmFB/PM8r17du3uC8o+8Mf/pCmTZuWnnnmmeLZRrX3Hv3gBz8oFgAAgHYVRWPGjEkbN24sQicHzoABA4ozQLWTL6xdu7aYka7Wo48+Wsxa98tf/rLe58nPObrtttua4z0AAAC03nOK2oLnFAEAAPvFc4oAAAAONKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaKIIAAAITRQBAAChiSIAACA0UQQAAIQmigAAgNBEEQAAEJooAgAAQhNFAABAaPsURZWVlalfv36pa9euaciQIWnZsmXfuf1zzz2XTjzxxGL7U089NS1cuHBfjxcAAKBto2ju3Llp0qRJafr06WnFihWpf//+qaKiIm3YsKHB7ZcsWZLGjh2bLrvssvTWW2+l0aNHF8u7777bHMcPAADwvZSVSqVSU3bIZ4bOPPPM9PDDDxevd+3alcrLy9O1116bpkyZ8q3tx4wZk7Zt25ZeeumlunVnnXVWGjBgQJo1a9Zefc2amprUvXv3tGXLltStW7emHC4AAHAAqWmBNujUlI137NiRli9fnqZOnVq3rkOHDmnkyJFp6dKlDe6T1+czS7vLZ5ZeeOGFRr/O9u3bi6VWfsO1/wcAAABx1fz/JmjiuZ3mi6JNmzalnTt3pl69etVbn1+vWrWqwX2qq6sb3D6vb8yMGTPS7bff/q31+YwUAADAv//97+KMUatHUWvJZ6J2P7u0efPmdPTRR6e1a9c22xuHxn7zkON73bp1LtWkRRlrtBZjjdZirNFa8lVkRx11VDriiCOa7XM2KYp69OiROnbsmNavX19vfX7du3fvBvfJ65uyfdalS5di2VMOIv/IaA15nBlrtAZjjdZirNFajDVaS76Np9k+V1M27ty5cxo4cGBatGhR3bo80UJ+PXTo0Ab3yet33z575ZVXGt0eAACgNTX58rl8Wdv48ePToEGD0uDBg9PMmTOL2eUmTJhQfHzcuHGpb9++xX1B2XXXXZfOOeec9OCDD6YLL7wwzZkzJ7355pvpsccea/53AwAA0NJRlKfY3rhxY5o2bVoxWUKeWruqqqpuMoV838/up7KGDRuWnnnmmXTLLbekm266Kf30pz8tZp475ZRT9vpr5kvp8nORGrqkDpqTsUZrMdZoLcYarcVYoz2PtSY/pwgAAOBA0nx3JwEAALRDoggAAAhNFAEAAKGJIgAAILT9JooqKytTv379UteuXdOQIUPSsmXLvnP75557Lp144onF9qeeempauHBhqx0r7VtTxtrs2bPTiBEj0uGHH14sI0eO/J9jE/b1+1qt/OiCsrKyNHr06BY/RmKOtc2bN6drrrkmHXnkkcXsTccff7z/jtIiYy0/uuWEE05IBx98cCovL08TJ05MX331VasdL+3P66+/nkaNGpX69OlT/Lcwz1r9vyxevDidccYZxfez4447Lj311FPtM4rmzp1bPP8oT623YsWK1L9//1RRUZE2bNjQ4PZLlixJY8eOTZdddll66623ih8c8vLuu++2+rHTvjR1rOV/ZHmsvfbaa2np0qXFN/Tzzz8/ffLJJ61+7BzYY63WRx99lG644YYixqElxtqOHTvSeeedV4y1efPmpdWrVxe/AMrPGITmHGv5kSxTpkwptl+5cmV64oknis+RH9ECjcnPP81jKwf43vjwww+LZ6Gee+656e23307XX399uvzyy9PLL7+cmqS0Hxg8eHDpmmuuqXu9c+fOUp8+fUozZsxocPtf/epXpQsvvLDeuiFDhpR+85vftPix0r41dazt6ZtvvikdeuihpaeffroFj5KoYy2Pr2HDhpUef/zx0vjx40u/+MUvWuloiTTWHn300dIxxxxT2rFjRyseJRHHWt725z//eb11kyZNKg0fPrzFj5UDQ0qpNH/+/O/c5sYbbyz97Gc/q7duzJgxpYqKiiZ9rTY/U5R/Y7V8+fLisqRa+eGv+XX+zXxD8vrdt8/ybyoa2x72dazt6Ysvvkhff/11OuKII1rwSIk61u64447Us2fP4iw4tNRYe/HFF9PQoUOLy+fyg9fzw9TvueeetHPnzlY8ciKMtWHDhhX71F5it2bNmuIyzQsuuKDVjpsD39Jm6oJOqY1t2rSp+EacvzHvLr9etWpVg/tUV1c3uH1eD8051vY0efLk4hrXPf/xwfcda2+88UZxaUk+9Q8tOdbyD6Z///vf0yWXXFL8gPr++++nq6++uviFT77MCZprrF188cXFfmeffXa+Mil988036aqrrnL5HM2qsS6oqalJX375ZXE/295o8zNF0F7ce++9xQ3w8+fPL24wheaydevWdOmllxb3dfTo0aOtD4cD3K5du4ozko899lgaOHBgGjNmTLr55pvTrFmz2vrQOMDk+3LzWchHHnmkuAfp+eefTwsWLEh33nlnWx8a7H9nivIPAB07dkzr16+vtz6/7t27d4P75PVN2R72dazVeuCBB4ooevXVV9Npp53WwkdKtLH2wQcfFDe959l2dv/BNevUqVNxI/yxxx7bCkdOhO9reca5gw46qNiv1kknnVT8tjVfItW5c+cWP25ijLVbb721+IVPvuk9y7MF55vor7zyyiLE8+V38H011gXdunXb67NEWZuPxvzNN/+matGiRfV+GMiv8zXPDcnrd98+e+WVVxrdHvZ1rGX33Xdf8VutqqqqNGjQoFY6WiKNtfx4gXfeeae4dK52ueiii+pm0smzHkJzfV8bPnx4cclcbXhn7733XhFLgojmHGv5Ptw9w6c2xv/vHnr4/pqtC0r7gTlz5pS6dOlSeuqpp0r/+te/SldeeWXpsMMOK1VXVxcfv/TSS0tTpkyp2/4f//hHqVOnTqUHHnigtHLlytL06dNLBx10UOmdd95pw3dBe9DUsXbvvfeWOnfuXJo3b17p008/rVu2bt3ahu+CA3Gs7cnsc7TUWFu7dm0xi+bvfve70urVq0svvfRSqWfPnqW77rqrDd8FB+JYyz+f5bH2l7/8pbRmzZrS3/72t9Kxxx5bzCIMjck/Y7311lvFklPloYceKv7+8ccfFx/PYyyPtVp5bB1yyCGl3//+90UXVFZWljp27FiqqqoqNcV+EUXZH//4x9JRRx1V/ACap3z85z//Wfexc845p/gBYXfPPvts6fjjjy+2z9PwLViwoA2OmvaoKWPt6KOPLv5B7rnkb/TQ3N/XdieKaMmxtmTJkuJRFvkH3Dw99913311MCQ/NOda+/vrr0m233VaEUNeuXUvl5eWlq6++uvSf//ynjY6e9uC1115r8Gev2rGV/8xjbc99BgwYUIzL/D3tT3/6U5O/bln+n2Y4cwUAANAutfk9RQAAAG1JFAEAAKGJIgAAIDRRBAAAhCaKAACA0EQRAAAQmigCAABCE0UAAEBooggAAAhNFAEAAKGJIgAAIDRRBAAApMj+Hw1SB7JF2kG7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(chunk_sizes, fold_accuracies, 'bo-')\n",
    "plt.xlabel('Chunk Size (seconds)')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.title('LSTM Model Accuracy vs Chunk Size')\n",
    "plt.grid(True)\n",
    "\n",
    "# Add accuracy values as text\n",
    "for i, (x, y) in enumerate(zip(chunk_sizes, fold_accuracies)):\n",
    "    plt.text(x, y, f'{y:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.savefig('lstm_chunk_size_vs_accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "# Print results table\n",
    "results = pd.DataFrame({\n",
    "    'Chunk Size (s)': chunk_sizes,\n",
    "    'Accuracy (%)': fold_accuracies\n",
    "})\n",
    "print(\"\\nLSTM Results Summary:\")\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6c79cb82-164f-4ed0-ba0b-63dcc6f6bbc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[162], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Loop through the test loader to collect predictions and true labels\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtest_loader\u001b[49m:\n\u001b[0;32m     16\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(batch_X)\n\u001b[0;32m     17\u001b[0m         _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Activities for the Confusion matrix\n",
    "ACTIVITIES = ['sit', 'walk','upstair']\n",
    "\n",
    "# Model evaluation (confusion matrix)\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Loop through the test loader to collect predictions and true labels\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Compute the confusion matrix, explicitly specifying the labels\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions, labels=[0, 1, 2]) \n",
    "\n",
    "# Assuming conf_matrix and ACTIVITIES are already defined\n",
    "class_accuracies = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)  # Compute per-class accuracy\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(conf_matrix, display_labels=ACTIVITIES)\n",
    "fig, ax = plt.subplots()\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "\n",
    "# Add per-class accuracy text\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "    acc_text = f\"{class_accuracies[i] * 100:.2f}%\"\n",
    "    ax.text(\n",
    "        len(ACTIVITIES) + 0.3, i, acc_text, \n",
    "        fontsize=12, verticalalignment='center', color='black'\n",
    "    )\n",
    "\n",
    "plt.title(\"Confusion Matrix for Activities with Per-Class Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f841cd-23c7-425c-a8c0-6e363b2c94f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label: 1, Predicted: 2, Confidence: 0.5379\n",
      "Full Probabilities: [0.04630952328443527, 0.40478968620300293, 0.5378606915473938, 0.0032725243363529444, 0.0037186944391578436, 0.0018561289180070162, 0.0015050942311063409, 0.0006875882390886545]\n",
      "True Label: 1, Predicted: 2, Confidence: 0.6603\n",
      "Full Probabilities: [0.012942157685756683, 0.3238711655139923, 0.6603409051895142, 0.0006900696316733956, 0.0012600854970514774, 0.00047482800437137485, 0.0003185397945344448, 0.00010225101141259074]\n",
      "True Label: 2, Predicted: 1, Confidence: 0.5808\n",
      "Full Probabilities: [0.002849593525752425, 0.5807549357414246, 0.4157315492630005, 0.00019912587595172226, 0.000295086792903021, 0.00010057010513264686, 5.421382957138121e-05, 1.4962002751417458e-05]\n",
      "True Label: 2, Predicted: 1, Confidence: 0.5663\n",
      "Full Probabilities: [0.004601947031915188, 0.5663013458251953, 0.42831793427467346, 0.0002489405160304159, 0.0003278449294157326, 0.00011758197069866583, 6.75682895234786e-05, 1.6763236999395303e-05]\n",
      "True Label: 2, Predicted: 1, Confidence: 0.5180\n",
      "Full Probabilities: [0.020252525806427002, 0.5179758667945862, 0.4608774185180664, 0.0002581731241662055, 0.00039337240741588175, 0.0001394125574734062, 8.091720519587398e-05, 2.237147418782115e-05]\n",
      "True Label: 0, Predicted: 1, Confidence: 0.7273\n",
      "Full Probabilities: [0.24202822148799896, 0.7272588014602661, 0.01800217106938362, 0.002684463746845722, 0.003459376050159335, 0.0026517054066061974, 0.0024689871352165937, 0.0014463680563494563]\n",
      "True Label: 1, Predicted: 2, Confidence: 0.5364\n",
      "Full Probabilities: [0.0010767356725409627, 0.461995929479599, 0.5364068150520325, 0.0001329303195234388, 0.00027212704299017787, 7.19023373676464e-05, 3.5536650102585554e-05, 8.04939008958172e-06]\n",
      "True Label: 1, Predicted: 2, Confidence: 0.6007\n",
      "Full Probabilities: [0.0015494574327021837, 0.39651644229888916, 0.6007441282272339, 0.000301126652630046, 0.0005974904634058475, 0.00017378314805682749, 9.326271538157016e-05, 2.4287490305141546e-05]\n",
      "True Label: 1, Predicted: 2, Confidence: 0.8370\n",
      "Full Probabilities: [0.0041362508200109005, 0.15798449516296387, 0.8370068073272705, 0.0001938036730280146, 0.00045545579632744193, 0.00012768110900651664, 7.822032057447359e-05, 1.728711140458472e-05]\n",
      "True Label: 1, Predicted: 2, Confidence: 0.7846\n",
      "Full Probabilities: [0.0027064322493970394, 0.2118576020002365, 0.7845824360847473, 0.00020613192464224994, 0.0004299695719964802, 0.0001244906452484429, 7.59389586164616e-05, 1.701954533928074e-05]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "incorrect_samples = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)  # Get raw logits\n",
    "        probabilities = F.softmax(outputs, dim=1)  # Convert to probabilities\n",
    "        predicted_labels = torch.argmax(probabilities, dim=1)  # Get predicted class\n",
    "        confidence_scores = torch.max(probabilities, dim=1).values  # Get max confidence\n",
    "\n",
    "        # Find incorrect predictions\n",
    "        incorrect_indices = (predicted_labels != batch_y)\n",
    "        for i in range(len(batch_y)):\n",
    "            if incorrect_indices[i]:\n",
    "                incorrect_samples.append({\n",
    "                    \"True Label\": batch_y[i].item(),\n",
    "                    \"Predicted Label\": predicted_labels[i].item(),\n",
    "                    \"Confidence\": confidence_scores[i].item(),\n",
    "                    \"Probabilities\": probabilities[i].tolist()\n",
    "                })\n",
    "\n",
    "# Print results\n",
    "for sample in incorrect_samples:\n",
    "    print(f\"True Label: {sample['True Label']}, Predicted: {sample['Predicted Label']}, Confidence: {sample['Confidence']:.4f}\")\n",
    "    print(f\"Full Probabilities: {sample['Probabilities']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
