{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8719255d-019b-484a-9aa3-6af82889074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75d9dc-98cd-496e-9c45-a7eebd3c82d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5904c3-0b5a-4ee1-9b59-9a76b68aab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a26188-64f2-43cb-899f-e70c373f0d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2736861-5c1a-4956-8766-b39c65beec82",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\yuanh\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\__init__.py:764\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[0;32m    763\u001b[0m __name, __obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m __name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[43m_C\u001b[49m):\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m __name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m __name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    766\u001b[0m         __all__\u001b[38;5;241m.\u001b[39mappend(__name)\n",
      "\u001b[1;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "import firebase_admin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from firebase_admin import credentials, firestore\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Firebase Initialization\n",
    "cred = credentials.Certificate(\"adminkey.json\")\n",
    "#firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()\n",
    "\n",
    "# Constants\n",
    "INCLUDE_ONLY = ['44444']\n",
    "ACTIVITIES = ['sit', 'walk', 'upstairs']\n",
    "SAMPLING_RATE = 100  # Hz\n",
    "OVERLAP = 0.5\n",
    "CHUNK_SIZE = 1  # seconds\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adde0e2-b2a3-4487-a94b-509417bbc7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_by_treatment(treatment_filter):\n",
    "    data_raw, docs = [], []\n",
    "    data_ref = db.collection(\"data\")\n",
    "    treatment_doc = data_ref.document(treatment_filter)\n",
    "\n",
    "    for patient_id in INCLUDE_ONLY:\n",
    "        subject_ref = treatment_doc.collection(patient_id)\n",
    "        for activity in ACTIVITIES:\n",
    "            activity_ref = subject_ref.document(activity)\n",
    "            phone_location_col = activity_ref.collections()\n",
    "            for phone_location in phone_location_col:\n",
    "                for recording in phone_location.stream():\n",
    "                    record = recording.to_dict()\n",
    "                    if 'acceleration' not in record:\n",
    "                        continue\n",
    "                    df = pd.DataFrame(record['acceleration'])\n",
    "                    df = df.iloc[25:-25]  # remove first and last 2.5s if needed\n",
    "                    combined = pd.DataFrame({\n",
    "                        'ax': df['accelerometer'].apply(lambda x: x['x']),\n",
    "                        'ay': df['accelerometer'].apply(lambda x: x['y']),\n",
    "                        'az': df['accelerometer'].apply(lambda x: x['z']),\n",
    "                        'gx': df['gyroscope'].apply(lambda x: x['x']),\n",
    "                        'gy': df['gyroscope'].apply(lambda x: x['y']),\n",
    "                        'gz': df['gyroscope'].apply(lambda x: x['z']),\n",
    "                    })\n",
    "                    combined_scaled = pd.DataFrame(scaler.fit_transform(combined), columns=combined.columns)\n",
    "                    data_raw.append(combined_scaled)\n",
    "                    docs.append({'activity': activity})\n",
    "    return data_raw, docs\n",
    "\n",
    "def chunk_data(data_raw, docs, chunk_size_sec, sampling_rate, overlap=OVERLAP):\n",
    "    data, labels = [], []\n",
    "    chunk_samples = int(chunk_size_sec * sampling_rate)\n",
    "    step = int(chunk_samples * (1 - overlap))\n",
    "    for i, df in enumerate(data_raw):\n",
    "        for start in range(0, len(df) - chunk_samples + 1, step):\n",
    "            chunk = df.iloc[start:start + chunk_samples]\n",
    "            if len(chunk) == chunk_samples:\n",
    "                data.append(chunk.values)\n",
    "                labels.append(ACTIVITIES.index(docs[i]['activity']))\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "def extract_features(chunk):\n",
    "    features = []\n",
    "    for i in range(chunk.shape[1]):\n",
    "        col = chunk[:, i]\n",
    "        features.extend([col.mean(), np.median(col), col.std(), col.var(), col.min(), col.max()])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a95d2e8-f0e8-45d3-a7c2-5756c9359253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_knn_data(data_raw, docs, chunk_size_sec, sampling_rate, overlap=OVERLAP):\n",
    "    X, y = [], []\n",
    "    chunk_samples = int(chunk_size_sec * sampling_rate)\n",
    "    step = int(chunk_samples * (1 - overlap))\n",
    "    for i, df in enumerate(data_raw):\n",
    "        for start in range(0, len(df) - chunk_samples + 1, step):\n",
    "            chunk = df.iloc[start:start + chunk_samples].values\n",
    "            if len(chunk) == chunk_samples:\n",
    "                X.append(extract_features(chunk))\n",
    "                y.append(ACTIVITIES.index(docs[i]['activity']))\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3808dc-4bb4-4b98-a712-7a7dcf5f64a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(6, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * (CHUNK_SIZE * SAMPLING_RATE // 4), 128)\n",
    "        self.fc2 = nn.Linear(128, len(ACTIVITIES))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # batch x channels x seq_len\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44c876e-3b1f-495e-9c0b-252875fc2715",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(6, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.lstm = nn.LSTM(32, 64, batch_first=True)\n",
    "        self.fc = nn.Linear(64, len(ACTIVITIES))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9dc671e-48bc-4e10-90da-18ba029b5f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dl_model(model_class, X_train, y_train, X_test, y_test):\n",
    "    model = model_class().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss.item():.4f}\")\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(torch.tensor(X_test, dtype=torch.float32).to(device)).argmax(dim=1).cpu().numpy()\n",
    "    print(classification_report(y_test, preds, target_names=ACTIVITIES))\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=ACTIVITIES, yticklabels=ACTIVITIES)\n",
    "    plt.title(f\"{model_class.__name__} Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50261b48-5df3-4776-9118-0ab625e1e147",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fetch_data_by_treatment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Fetch and chunk data\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     before_raw, before_docs \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_data_by_treatment\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBeforeTreatment\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m     after_raw, after_docs \u001b[38;5;241m=\u001b[39m fetch_data_by_treatment(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAfterTreatment\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m     X_train_cnn, y_train \u001b[38;5;241m=\u001b[39m chunk_data(before_raw, before_docs, CHUNK_SIZE, SAMPLING_RATE)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fetch_data_by_treatment' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Fetch and chunk data\n",
    "    before_raw, before_docs = fetch_data_by_treatment('BeforeTreatment')\n",
    "    after_raw, after_docs = fetch_data_by_treatment('AfterTreatment')\n",
    "    X_train_cnn, y_train = chunk_data(before_raw, before_docs, CHUNK_SIZE, SAMPLING_RATE)\n",
    "    X_test_cnn, y_test = chunk_data(after_raw, after_docs, CHUNK_SIZE, SAMPLING_RATE)\n",
    "\n",
    "    print(\"\\n=== Running CNN Model ===\")\n",
    "    train_dl_model(SimpleCNN, X_train_cnn, y_train, X_test_cnn, y_test)\n",
    "\n",
    "    print(\"\\n=== Running CNN-LSTM Model ===\")\n",
    "    train_dl_model(CNNLSTM, X_train_cnn, y_train, X_test_cnn, y_test)\n",
    "\n",
    "    # KNN\n",
    "    print(\"\\n=== Running KNN Model ===\")\n",
    "    X_train_knn, y_train_knn = prepare_knn_data(before_raw, before_docs, CHUNK_SIZE, SAMPLING_RATE)\n",
    "    X_test_knn, y_test_knn = prepare_knn_data(after_raw, after_docs, CHUNK_SIZE, SAMPLING_RATE)\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train_knn, y_train_knn)\n",
    "    preds_knn = knn.predict(X_test_knn)\n",
    "    print(classification_report(y_test_knn, preds_knn, target_names=ACTIVITIES))\n",
    "    cm_knn = confusion_matrix(y_test_knn, preds_knn)\n",
    "    sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Blues', xticklabels=ACTIVITIES, yticklabels=ACTIVITIES)\n",
    "    plt.title(\"KNN Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0773113f-e528-4e06-9715-eec33c10a85a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
