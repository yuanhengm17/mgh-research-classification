{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30e998e0-f6cd-4d3d-82f0-08637032af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from firebase_admin import credentials, firestore\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Firebase Initialization\n",
    "cred = credentials.Certificate(\"adminkey.json\")\n",
    "#firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename='training_log.txt', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "INCLUDE_ONLY = ['Stephen', 'Lillian', 'Ren', 'Yuanheng', 'Ethan Shao', 'z']\n",
    "ACTIVITIES = ['sit','walk','upstair']\n",
    "CHUNK_SIZE = 1.375  # in seconds (can be a decimal)\n",
    "SAMPLING_RATE = 100  # Hz\n",
    "NUM_CLASSES = 3\n",
    "OVERLAP = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24622a39-7713-4bc5-be27-67c17c8c5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(chunk):\n",
    "    \"\"\"Extract features from a chunked acceleration segment with selected statistics.\"\"\"\n",
    "    feature_vector = []\n",
    "    \n",
    "    for axis in ['x', 'y', 'z']:\n",
    "        data_series = pd.Series(chunk[axis])\n",
    "        # Apply smoothing\n",
    "        smoothed_data = data_series.rolling(window=5, min_periods=1).mean()\n",
    "        feature_vector.extend([\n",
    "            smoothed_data.mean(),                  # Mean\n",
    "            smoothed_data.median(),                # Median\n",
    "            smoothed_data.std(),                   # Standard Deviation\n",
    "            smoothed_data.var(),                   # Variance\n",
    "            smoothed_data.min(),                   # Minimum\n",
    "            smoothed_data.max(),                   # Maximum\n",
    "        ])\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51a0fe31-556e-43bf-84e4-fd36d0a77bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data_with_overlap(data_raw, docs, chunk_size, activities, sampling_rate, overlap=0.5):\n",
    "    \"\"\"Chunk raw acceleration data into smaller labeled segments using overlapping windows.\"\"\"\n",
    "    data, labels = [], []\n",
    "    chunk_samples = int(chunk_size * sampling_rate)\n",
    "    step = int(chunk_samples * (1 - overlap))  # compute step size based on overlap\n",
    "\n",
    "    for i, df in enumerate(data_raw):\n",
    "        # Slide over the data with the defined step\n",
    "        for start in range(0, len(df) - chunk_samples + 1, step):\n",
    "            end = start + chunk_samples\n",
    "            chunk = df.iloc[start:end]\n",
    "            activity = docs[i]['activity']\n",
    "            label = activities.index(activity)\n",
    "            data.append(extract_features(chunk))\n",
    "            labels.append(label)\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Use overlapping window chunking\n",
    "X_train, y_train = chunk_data_with_overlap(training_data_raw, training_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE, OVERLAP)\n",
    "X_test, y_test = chunk_data_with_overlap(testing_data_raw, testing_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE, OVERLAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "287ad27b-8007-4ccd-b113-f8016c2aaec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Structures\n",
    "def fetch_data(collection_name, activities, include_only, time_start=500, time_end=6000):\n",
    "    \"\"\"Fetch and preprocess data from Firestore.\"\"\"\n",
    "    data, docs = [], []\n",
    "    for person in db.collection(collection_name).stream():\n",
    "        person_name = str(person.to_dict().get('name', ''))\n",
    "        if person_name not in include_only:\n",
    "            continue\n",
    "\n",
    "        for activity in activities:\n",
    "            for recording in db.collection(collection_name).document(person_name).collection(activity).stream():\n",
    "                record = recording.to_dict()\n",
    "                if 'acceleration' not in record:\n",
    "                    continue\n",
    "\n",
    "                docs.append(record)\n",
    "                df = pd.DataFrame(record['acceleration'])\n",
    "                \n",
    "                if 'time' in df.columns:\n",
    "                    filtered_df = df[(df['time'] >= time_start) & (df['time'] <= time_end)]\n",
    "                    data.append(filtered_df)\n",
    "                else:\n",
    "                    raise ValueError(\"The 'acceleration' field must include a 'time' column.\")\n",
    "    return data, docs\n",
    "\n",
    "training_data_raw, training_docs = fetch_data(\"training\", ACTIVITIES, INCLUDE_ONLY)\n",
    "testing_data_raw, testing_docs = fetch_data(\"testing\", ACTIVITIES, INCLUDE_ONLY)\n",
    "\n",
    "# Split into 80% training, 20% testing\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42, stratify=y_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1284a61-2522-4348-abbf-1d4e31e36bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to chunk data into smaller segments for training/testing\n",
    "def chunk_data(data_raw, docs, chunk_size, activities, sampling_rate):\n",
    "    \"\"\"Split data into chunks and assign labels.\"\"\"\n",
    "    data, labels = [], []\n",
    "    activity_distribution = np.zeros(len(activities))\n",
    "    chunk_samples = int(chunk_size * sampling_rate)  # Convert time to sample count\n",
    "\n",
    "    for i in range(len(data_raw)):\n",
    "        num_chunks = len(data_raw[i]) // chunk_samples  # Number of full chunks\n",
    "        for j in range(num_chunks):\n",
    "            start = j * chunk_samples\n",
    "            end = start + chunk_samples\n",
    "            x = list(data_raw[i][\"x\"])[start:end]\n",
    "            y = list(data_raw[i][\"y\"])[start:end]\n",
    "            z = list(data_raw[i][\"z\"])[start:end]\n",
    "            activity = docs[i]['activity']\n",
    "            label = activities.index(activity)\n",
    "\n",
    "            activity_distribution[label] += 1\n",
    "            data.append([x, y, z])\n",
    "            labels.append(label)\n",
    "\n",
    "    return data, labels, activity_distribution\n",
    "\n",
    "\n",
    "#training_data, training_labels, training_distribution = chunk_data(training_data_raw, training_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE)\n",
    "#testing_data, testing_labels, testing_distribution = chunk_data(testing_data_raw, testing_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE)\n",
    "\n",
    "all_data_raw = training_data_raw + testing_data_raw\n",
    "all_docs = training_docs + testing_docs\n",
    "all_data, all_labels, all_distribution = chunk_data(all_data_raw, all_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE)\n",
    "\n",
    "indices = np.arange(len(all_data))\n",
    "train, test = train_test_split(indices, test_size=0.2, random_state=42, stratify=all_labels)\n",
    "\n",
    "training_data = [all_data[i] for i in train]\n",
    "training_labels = [all_labels[i] for i in train]\n",
    "testing_data = [all_data[i] for i in test]\n",
    "testing_labels = [all_labels[i] for i in test]\n",
    "\n",
    "training_distribution = np.bincount(training_labels, minlength=len(ACTIVITIES))\n",
    "testing_distribution = np.bincount(testing_labels, minlength=len(ACTIVITIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43de7965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset summary:\n",
      "+----------+------------------+\n",
      "| Dataset  | number of chunks |\n",
      "+----------+------------------+\n",
      "| training |       230        |\n",
      "| testing  |        58        |\n",
      "+----------+------------------+\n",
      "Training Activities Count\n",
      "sit: 77 chunks\n",
      "walk: 77 chunks\n",
      "upstair: 76 chunks\n",
      "\n",
      "Testing Activity Count\n",
      "sit:19 chunks\n",
      "walk:19 chunks\n",
      "upstair:20 chunks\n",
      "230\n",
      "58\n",
      "(230, 3, 137)\n",
      "36\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate #for table formatting\n",
    "\n",
    "#Calculate the number of training and testing samples\n",
    "num_training_samples = len(training_data)\n",
    "num_testing_samples = len(testing_data)\n",
    "\n",
    "#table\n",
    "summary_table = [[\"training\", num_training_samples], [\"testing\", num_testing_samples]]\n",
    "\n",
    "#print\n",
    "print(\"dataset summary:\")\n",
    "print(tabulate(summary_table, headers = [\"Dataset\", \"number of chunks\"], tablefmt=\"pretty\"))\n",
    "\n",
    "print(\"Training Activities Count\")\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "    print(f\"{activity}: {int(training_distribution[i])} chunks\")\n",
    "\n",
    "print(\"\\nTesting Activity Count\")\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "    print(f\"{activity}:{int(testing_distribution[i])} chunks\")\n",
    "print(len(training_data))\n",
    "print(len(testing_data))\n",
    "print(np.array(training_data).shape)\n",
    "print(len(training_data_raw))\n",
    "print(len(testing_data_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fae1da3e-3e2c-40ad-82cb-6d090f2cc7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to NumPy arrays\n",
    "training_data = np.array(training_data)\n",
    "testing_data = np.array(testing_data)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(training_data.reshape(-1, training_data.shape[-1])).reshape(training_data.shape)\n",
    "X_test = scaler.transform(testing_data.reshape(-1, testing_data.shape[-1])).reshape(testing_data.shape)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(training_labels, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(testing_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset and DataLoader for batching\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ec68f8d-85ff-48e1-b219-a547e9393c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedCNNModel(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=3, seq_length=int(CHUNK_SIZE * 100)):\n",
    "        super(OptimizedCNNModel, self).__init__()\n",
    "\n",
    "        def depthwise_separable_conv(in_channels, out_channels, kernel_size=3, padding=1):\n",
    "            \"\"\"Depthwise Separable Convolution for efficiency.\"\"\"\n",
    "            return nn.Sequential(\n",
    "                nn.Conv1d(in_channels, in_channels, kernel_size, groups=in_channels, padding=padding, bias=False),\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "                nn.GroupNorm(8, out_channels),  # More stable than BatchNorm for small batches\n",
    "                nn.SiLU()  # Swish activation (better than ReLU)\n",
    "            )\n",
    "\n",
    "        self.conv1 = depthwise_separable_conv(input_channels, 8)\n",
    "        self.conv2 = depthwise_separable_conv(8, 16)\n",
    "        self.conv3 = depthwise_separable_conv(16, 32)\n",
    "\n",
    "        # Adaptive pooling to ensure flexible sequence length compatibility\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, num_classes)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "        # Weight initialization\n",
    "        self.apply(self.initialize_weights)\n",
    "\n",
    "    def initialize_weights(self, m):\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "\n",
    "        # Global Average Pooling to reduce to fixed size\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.silu(self.fc1(x))  # Swish activation\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b114befb-33f1-4747-9e0f-b9422cda6a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" need to fix lstm cnn combo\\nclass OptimizedCNNModel(nn.Module):\\n    def __init__(self, num_classes, input_channels=3, seq_length=int(CHUNK_SIZE * 100), lstm_hidden_size=64, lstm_layers=1):\\n        super(OptimizedCNNModel, self).__init__()\\n\\n        def depthwise_separable_conv(in_channels, out_channels, kernel_size=3, padding=1):\\n            return nn.Sequential(\\n                nn.Conv1d(in_channels, in_channels, kernel_size, groups=in_channels, padding=padding, bias=False),\\n                nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False),\\n                nn.GroupNorm(8, out_channels),\\n                nn.SiLU()\\n            )\\n\\n        self.conv1 = depthwise_separable_conv(input_channels, 8)\\n        self.conv2 = depthwise_separable_conv(8, 16)\\n        self.conv3 = depthwise_separable_conv(16, 32)\\n\\n        self.global_pool = nn.AdaptiveAvgPool1d(seq_length // 8)\\n\\n        self.lstm = nn.LSTM(input_size=32, hidden_size=lstm_hidden_size, num_layers=lstm_layers, batch_first=True)\\n\\n        self.fc1 = nn.Linear(lstm_hidden_size, 16)\\n        self.fc2 = nn.Linear(16, 8)\\n        self.fc3 = nn.Linear(8, num_classes)\\n\\n        self.dropout = nn.Dropout(0.4)\\n        self.apply(self.initialize_weights)\\n\\n    def initialize_weights(self, m):\\n        if isinstance(m, nn.Conv1d):\\n            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\\n            if m.bias is not None:\\n                nn.init.zeros_(m.bias)\\n        elif isinstance(m, nn.Linear):\\n            nn.init.xavier_normal_(m.weight)\\n            if m.bias is not None:\\n                nn.init.zeros_(m.bias)\\n\\n    def forward(self, x):\\n        x = self.conv1(x)\\n        x = F.max_pool1d(x, 2)\\n        x = self.conv2(x)\\n        x = F.max_pool1d(x, 2)\\n        x = self.conv3(x)\\n        x = F.max_pool1d(x, 2)\\n        x = self.global_pool(x)\\n        x = x.permute(0, 2, 1)\\n        x, _ = self.lstm(x)\\n        x = x[:, -1, :]\\n        x = F.silu(self.fc1(x))\\n        x = self.dropout(x)\\n        x = self.fc2(x)\\n        x = self.fc3(x)\\n        return x\\n\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" need to fix lstm cnn combo\n",
    "class OptimizedCNNModel(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=3, seq_length=int(CHUNK_SIZE * 100), lstm_hidden_size=64, lstm_layers=1):\n",
    "        super(OptimizedCNNModel, self).__init__()\n",
    "\n",
    "        def depthwise_separable_conv(in_channels, out_channels, kernel_size=3, padding=1):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv1d(in_channels, in_channels, kernel_size, groups=in_channels, padding=padding, bias=False),\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "                nn.GroupNorm(8, out_channels),\n",
    "                nn.SiLU()\n",
    "            )\n",
    "\n",
    "        self.conv1 = depthwise_separable_conv(input_channels, 8)\n",
    "        self.conv2 = depthwise_separable_conv(8, 16)\n",
    "        self.conv3 = depthwise_separable_conv(16, 32)\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(seq_length // 8)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=32, hidden_size=lstm_hidden_size, num_layers=lstm_layers, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(lstm_hidden_size, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.apply(self.initialize_weights)\n",
    "\n",
    "    def initialize_weights(self, m):\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = self.conv3(x)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = F.silu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2fea2d82-2721-4226-87c8-d0386cb60e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save best model and metadata\n",
    "BEST_MODEL_PATH = \"best_model.pth\"\n",
    "BEST_METADATA_PATH = \"best_model.json\"\n",
    "\n",
    "# Function to save best model\n",
    "def save_best_model(epoch, model, optimizer, test_loss, train_losses, test_losses):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'test_loss': test_loss,\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses\n",
    "    }, BEST_MODEL_PATH)\n",
    "\n",
    "    # Save metadata\n",
    "    with open(BEST_METADATA_PATH, \"w\") as f:\n",
    "        json.dump({\"epoch\": epoch, \"test_loss\": test_loss}, f)\n",
    "\n",
    "# Function to load best model if exists\n",
    "def load_best_model(model, optimizer):\n",
    "    if os.path.exists(BEST_MODEL_PATH) and os.path.exists(BEST_METADATA_PATH):\n",
    "        checkpoint = torch.load(BEST_MODEL_PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_loss = checkpoint['test_loss']\n",
    "        train_losses = checkpoint.get('train_losses', [])\n",
    "        test_losses = checkpoint.get('test_losses', [])\n",
    "\n",
    "        print(f\"Loaded best model from epoch {start_epoch} with Avg Loss: {best_loss:.4f}\")\n",
    "        return start_epoch, best_loss, train_losses, test_losses\n",
    "    return 0, float('inf'), 0.0, [], []  # Start fresh if no saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "afb6cc02-f6d5-441e-8ea1-736efc276305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(loader, model, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # disable gradient calc\n",
    "        for data, targets in loader:\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # accuracy\n",
    "            _, predicted = torch.max(outputs, 1)  # get predicted class (max value)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "01254ebf-d89d-4173-9f04-ba08155090cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Data Augmentation Functions\n",
    "def add_jitter(batch_X, noise_level=0.01):\n",
    "    noise = torch.randn_like(batch_X) * noise_level\n",
    "    return batch_X + noise\n",
    "\n",
    "def scale_signal(batch_X, scale_range=(0.9, 1.1)):\n",
    "    scale_factor = torch.FloatTensor(batch_X.shape[0], 1, 1).uniform_(*scale_range)\n",
    "    return batch_X * scale_factor\n",
    "\n",
    "def time_warp(batch_X, sigma=0.2):\n",
    "    batch_size, num_channels, seq_length = batch_X.shape\n",
    "    time_steps = np.arange(seq_length)\n",
    "    warping_curve = np.cumsum(np.random.normal(0, sigma, size=(batch_size, seq_length)), axis=1)\n",
    "    warping_curve = (warping_curve - warping_curve.min(axis=1, keepdims=True)) / \\\n",
    "                    (warping_curve.max(axis=1, keepdims=True) - warping_curve.min(axis=1, keepdims=True)) * seq_length\n",
    "    warped_X = torch.zeros_like(batch_X)\n",
    "    for i in range(batch_size):\n",
    "        for j in range(num_channels):\n",
    "            f = interp1d(time_steps, batch_X[i, j, :].cpu().numpy(), kind='linear', fill_value='extrapolate')\n",
    "            warped_X[i, j, :] = torch.tensor(f(warping_curve[i]), dtype=batch_X.dtype)\n",
    "    return warped_X\n",
    "\n",
    "def random_crop(batch_X, crop_size=0.9):\n",
    "    batch_size, num_channels, seq_length = batch_X.shape\n",
    "    new_length = int(seq_length * crop_size)\n",
    "    start_idx = torch.randint(0, seq_length - new_length, (batch_size,))\n",
    "    cropped_X = torch.zeros(batch_size, num_channels, new_length)\n",
    "    for i in range(batch_size):\n",
    "        cropped_X[i] = batch_X[i, :, start_idx[i]:start_idx[i] + new_length]\n",
    "    return cropped_X\n",
    "\n",
    "def permute_segments(batch_X, num_segments=5):\n",
    "    batch_size, num_channels, seq_length = batch_X.shape\n",
    "    segment_length = seq_length // num_segments\n",
    "    permuted_X = batch_X.clone()\n",
    "    for i in range(batch_size):\n",
    "        perm = torch.randperm(num_segments)\n",
    "        for j in range(num_segments):\n",
    "            permuted_X[i, :, j * segment_length:(j + 1) * segment_length] = \\\n",
    "                batch_X[i, :, perm[j] * segment_length:(perm[j] + 1) * segment_length]\n",
    "    return permuted_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc68f1f4-225f-455c-b6d0-80a76a3ce598",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OptimizedCNNModel(num_classes=NUM_CLASSES, input_channels=3, seq_length=int(CHUNK_SIZE * 100))\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Weight decay helps reduce overfitting\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-4)\n",
    "\n",
    "# Load best model if exists\n",
    "#start_epoch, best_loss, best_avg_accuracy, train_losses, test_losses = load_best_model(model, optimizer)\n",
    "start_epoch, best_loss, best_avg_accuracy, train_losses, test_losses =  0, float('inf'), 0.0, [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd628c08-376b-468a-ab5f-6c1a88c7587c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 2.1213, Train Acc: 17.83%, Test Loss: 2.1268, Test Acc: 10.34%, LR: 0.001000\n",
      "New best model saved at epoch 1 with Test Loss: 2.1268, Test Accuracy: 10.34%\n",
      "Epoch [1/100] - Train Loss: 2.1213, Train Acc: 17.83%, Test Loss: 2.1268, Test Acc: 10.34%\n",
      "Epoch [2/100], Train Loss: 2.0363, Train Acc: 31.74%, Test Loss: 2.0464, Test Acc: 32.76%, LR: 0.001000\n",
      "New best model saved at epoch 2 with Test Loss: 2.0464, Test Accuracy: 32.76%\n",
      "Epoch [2/100] - Train Loss: 2.0363, Train Acc: 31.74%, Test Loss: 2.0464, Test Acc: 32.76%\n",
      "Epoch [3/100], Train Loss: 1.9630, Train Acc: 33.91%, Test Loss: 1.9718, Test Acc: 34.48%, LR: 0.001000\n",
      "New best model saved at epoch 3 with Test Loss: 1.9718, Test Accuracy: 34.48%\n",
      "Epoch [3/100] - Train Loss: 1.9630, Train Acc: 33.91%, Test Loss: 1.9718, Test Acc: 34.48%\n",
      "Epoch [4/100], Train Loss: 1.8892, Train Acc: 47.83%, Test Loss: 1.8996, Test Acc: 43.10%, LR: 0.001000\n",
      "New best model saved at epoch 4 with Test Loss: 1.8996, Test Accuracy: 43.10%\n",
      "Epoch [4/100] - Train Loss: 1.8892, Train Acc: 47.83%, Test Loss: 1.8996, Test Acc: 43.10%\n",
      "Epoch [5/100], Train Loss: 1.8181, Train Acc: 58.26%, Test Loss: 1.8290, Test Acc: 58.62%, LR: 0.001000\n",
      "New best model saved at epoch 5 with Test Loss: 1.8290, Test Accuracy: 58.62%\n",
      "Epoch [5/100] - Train Loss: 1.8181, Train Acc: 58.26%, Test Loss: 1.8290, Test Acc: 58.62%\n",
      "Epoch [6/100], Train Loss: 1.7487, Train Acc: 59.57%, Test Loss: 1.7595, Test Acc: 56.90%, LR: 0.001000\n",
      "New best model saved at epoch 6 with Test Loss: 1.7595, Test Accuracy: 56.90%\n",
      "Epoch [6/100] - Train Loss: 1.7487, Train Acc: 59.57%, Test Loss: 1.7595, Test Acc: 56.90%\n",
      "Epoch [7/100], Train Loss: 1.6824, Train Acc: 55.22%, Test Loss: 1.6924, Test Acc: 55.17%, LR: 0.001000\n",
      "New best model saved at epoch 7 with Test Loss: 1.6924, Test Accuracy: 55.17%\n",
      "Epoch [7/100] - Train Loss: 1.6824, Train Acc: 55.22%, Test Loss: 1.6924, Test Acc: 55.17%\n",
      "Epoch [8/100], Train Loss: 1.6145, Train Acc: 55.65%, Test Loss: 1.6258, Test Acc: 56.90%, LR: 0.001000\n",
      "New best model saved at epoch 8 with Test Loss: 1.6258, Test Accuracy: 56.90%\n",
      "Epoch [8/100] - Train Loss: 1.6145, Train Acc: 55.65%, Test Loss: 1.6258, Test Acc: 56.90%\n",
      "Epoch [9/100], Train Loss: 1.5525, Train Acc: 56.52%, Test Loss: 1.5613, Test Acc: 55.17%, LR: 0.001000\n",
      "New best model saved at epoch 9 with Test Loss: 1.5613, Test Accuracy: 55.17%\n",
      "Epoch [9/100] - Train Loss: 1.5525, Train Acc: 56.52%, Test Loss: 1.5613, Test Acc: 55.17%\n",
      "Epoch [10/100], Train Loss: 1.4868, Train Acc: 56.52%, Test Loss: 1.4991, Test Acc: 53.45%, LR: 0.001000\n",
      "New best model saved at epoch 10 with Test Loss: 1.4991, Test Accuracy: 53.45%\n",
      "Epoch [10/100] - Train Loss: 1.4868, Train Acc: 56.52%, Test Loss: 1.4991, Test Acc: 53.45%\n",
      "Epoch [11/100], Train Loss: 1.4296, Train Acc: 55.65%, Test Loss: 1.4394, Test Acc: 55.17%, LR: 0.001000\n",
      "New best model saved at epoch 11 with Test Loss: 1.4394, Test Accuracy: 55.17%\n",
      "Epoch [11/100] - Train Loss: 1.4296, Train Acc: 55.65%, Test Loss: 1.4394, Test Acc: 55.17%\n",
      "Epoch [12/100], Train Loss: 1.3690, Train Acc: 54.78%, Test Loss: 1.3810, Test Acc: 55.17%, LR: 0.001000\n",
      "New best model saved at epoch 12 with Test Loss: 1.3810, Test Accuracy: 55.17%\n",
      "Epoch [12/100] - Train Loss: 1.3690, Train Acc: 54.78%, Test Loss: 1.3810, Test Acc: 55.17%\n",
      "Epoch [13/100], Train Loss: 1.3103, Train Acc: 56.52%, Test Loss: 1.3262, Test Acc: 58.62%, LR: 0.001000\n",
      "New best model saved at epoch 13 with Test Loss: 1.3262, Test Accuracy: 58.62%\n",
      "Epoch [13/100] - Train Loss: 1.3103, Train Acc: 56.52%, Test Loss: 1.3262, Test Acc: 58.62%\n",
      "Epoch [14/100], Train Loss: 1.2625, Train Acc: 56.52%, Test Loss: 1.2779, Test Acc: 60.34%, LR: 0.001000\n",
      "New best model saved at epoch 14 with Test Loss: 1.2779, Test Accuracy: 60.34%\n",
      "Epoch [14/100] - Train Loss: 1.2625, Train Acc: 56.52%, Test Loss: 1.2779, Test Acc: 60.34%\n",
      "Epoch [15/100], Train Loss: 1.2164, Train Acc: 63.04%, Test Loss: 1.2338, Test Acc: 63.79%, LR: 0.001000\n",
      "New best model saved at epoch 15 with Test Loss: 1.2338, Test Accuracy: 63.79%\n",
      "Epoch [15/100] - Train Loss: 1.2164, Train Acc: 63.04%, Test Loss: 1.2338, Test Acc: 63.79%\n",
      "Epoch [16/100], Train Loss: 1.1763, Train Acc: 69.57%, Test Loss: 1.1935, Test Acc: 65.52%, LR: 0.001000\n",
      "New best model saved at epoch 16 with Test Loss: 1.1935, Test Accuracy: 65.52%\n",
      "Epoch [16/100] - Train Loss: 1.1763, Train Acc: 69.57%, Test Loss: 1.1935, Test Acc: 65.52%\n",
      "Epoch [17/100], Train Loss: 1.1371, Train Acc: 65.65%, Test Loss: 1.1594, Test Acc: 65.52%, LR: 0.001000\n",
      "New best model saved at epoch 17 with Test Loss: 1.1594, Test Accuracy: 65.52%\n",
      "Epoch [17/100] - Train Loss: 1.1371, Train Acc: 65.65%, Test Loss: 1.1594, Test Acc: 65.52%\n",
      "Epoch [18/100], Train Loss: 1.0991, Train Acc: 66.96%, Test Loss: 1.1257, Test Acc: 65.52%, LR: 0.001000\n",
      "New best model saved at epoch 18 with Test Loss: 1.1257, Test Accuracy: 65.52%\n",
      "Epoch [18/100] - Train Loss: 1.0991, Train Acc: 66.96%, Test Loss: 1.1257, Test Acc: 65.52%\n",
      "Epoch [19/100], Train Loss: 1.0683, Train Acc: 65.65%, Test Loss: 1.0942, Test Acc: 65.52%, LR: 0.001000\n",
      "New best model saved at epoch 19 with Test Loss: 1.0942, Test Accuracy: 65.52%\n",
      "Epoch [19/100] - Train Loss: 1.0683, Train Acc: 65.65%, Test Loss: 1.0942, Test Acc: 65.52%\n",
      "Epoch [20/100], Train Loss: 1.0398, Train Acc: 64.35%, Test Loss: 1.0634, Test Acc: 63.79%, LR: 0.001000\n",
      "New best model saved at epoch 20 with Test Loss: 1.0634, Test Accuracy: 63.79%\n",
      "Epoch [20/100] - Train Loss: 1.0398, Train Acc: 64.35%, Test Loss: 1.0634, Test Acc: 63.79%\n",
      "Epoch [21/100], Train Loss: 1.0041, Train Acc: 63.91%, Test Loss: 1.0328, Test Acc: 63.79%, LR: 0.001000\n",
      "New best model saved at epoch 21 with Test Loss: 1.0328, Test Accuracy: 63.79%\n",
      "Epoch [21/100] - Train Loss: 1.0041, Train Acc: 63.91%, Test Loss: 1.0328, Test Acc: 63.79%\n",
      "Epoch [22/100], Train Loss: 0.9678, Train Acc: 63.91%, Test Loss: 1.0020, Test Acc: 63.79%, LR: 0.001000\n",
      "New best model saved at epoch 22 with Test Loss: 1.0020, Test Accuracy: 63.79%\n",
      "Epoch [22/100] - Train Loss: 0.9678, Train Acc: 63.91%, Test Loss: 1.0020, Test Acc: 63.79%\n",
      "Epoch [23/100], Train Loss: 0.9419, Train Acc: 64.78%, Test Loss: 0.9744, Test Acc: 63.79%, LR: 0.001000\n",
      "New best model saved at epoch 23 with Test Loss: 0.9744, Test Accuracy: 63.79%\n",
      "Epoch [23/100] - Train Loss: 0.9419, Train Acc: 64.78%, Test Loss: 0.9744, Test Acc: 63.79%\n",
      "Epoch [24/100], Train Loss: 0.9166, Train Acc: 63.91%, Test Loss: 0.9448, Test Acc: 63.79%, LR: 0.001000\n",
      "New best model saved at epoch 24 with Test Loss: 0.9448, Test Accuracy: 63.79%\n",
      "Epoch [24/100] - Train Loss: 0.9166, Train Acc: 63.91%, Test Loss: 0.9448, Test Acc: 63.79%\n",
      "Epoch [25/100], Train Loss: 0.8826, Train Acc: 63.48%, Test Loss: 0.9157, Test Acc: 62.07%, LR: 0.001000\n",
      "New best model saved at epoch 25 with Test Loss: 0.9157, Test Accuracy: 62.07%\n",
      "Epoch [25/100] - Train Loss: 0.8826, Train Acc: 63.48%, Test Loss: 0.9157, Test Acc: 62.07%\n",
      "Epoch [26/100], Train Loss: 0.8451, Train Acc: 64.78%, Test Loss: 0.8887, Test Acc: 63.79%, LR: 0.001000\n",
      "New best model saved at epoch 26 with Test Loss: 0.8887, Test Accuracy: 63.79%\n",
      "Epoch [26/100] - Train Loss: 0.8451, Train Acc: 64.78%, Test Loss: 0.8887, Test Acc: 63.79%\n",
      "Epoch [27/100], Train Loss: 0.8239, Train Acc: 66.52%, Test Loss: 0.8631, Test Acc: 63.79%, LR: 0.001000\n",
      "New best model saved at epoch 27 with Test Loss: 0.8631, Test Accuracy: 63.79%\n",
      "Epoch [27/100] - Train Loss: 0.8239, Train Acc: 66.52%, Test Loss: 0.8631, Test Acc: 63.79%\n",
      "Epoch [28/100], Train Loss: 0.7954, Train Acc: 67.83%, Test Loss: 0.8380, Test Acc: 65.52%, LR: 0.001000\n",
      "New best model saved at epoch 28 with Test Loss: 0.8380, Test Accuracy: 65.52%\n",
      "Epoch [28/100] - Train Loss: 0.7954, Train Acc: 67.83%, Test Loss: 0.8380, Test Acc: 65.52%\n",
      "Epoch [29/100], Train Loss: 0.7726, Train Acc: 66.52%, Test Loss: 0.8122, Test Acc: 65.52%, LR: 0.001000\n",
      "New best model saved at epoch 29 with Test Loss: 0.8122, Test Accuracy: 65.52%\n",
      "Epoch [29/100] - Train Loss: 0.7726, Train Acc: 66.52%, Test Loss: 0.8122, Test Acc: 65.52%\n",
      "Epoch [30/100], Train Loss: 0.7466, Train Acc: 66.09%, Test Loss: 0.7872, Test Acc: 63.79%, LR: 0.001000\n",
      "New best model saved at epoch 30 with Test Loss: 0.7872, Test Accuracy: 63.79%\n",
      "Epoch [30/100] - Train Loss: 0.7466, Train Acc: 66.09%, Test Loss: 0.7872, Test Acc: 63.79%\n",
      "Epoch [31/100], Train Loss: 0.7105, Train Acc: 65.65%, Test Loss: 0.7687, Test Acc: 63.79%, LR: 0.001000\n",
      "New best model saved at epoch 31 with Test Loss: 0.7687, Test Accuracy: 63.79%\n",
      "Epoch [31/100] - Train Loss: 0.7105, Train Acc: 65.65%, Test Loss: 0.7687, Test Acc: 63.79%\n",
      "Epoch [32/100], Train Loss: 0.6932, Train Acc: 65.65%, Test Loss: 0.7480, Test Acc: 63.79%, LR: 0.001000\n",
      "New best model saved at epoch 32 with Test Loss: 0.7480, Test Accuracy: 63.79%\n",
      "Epoch [32/100] - Train Loss: 0.6932, Train Acc: 65.65%, Test Loss: 0.7480, Test Acc: 63.79%\n",
      "Epoch [33/100], Train Loss: 0.6691, Train Acc: 67.83%, Test Loss: 0.7279, Test Acc: 65.52%, LR: 0.001000\n",
      "New best model saved at epoch 33 with Test Loss: 0.7279, Test Accuracy: 65.52%\n",
      "Epoch [33/100] - Train Loss: 0.6691, Train Acc: 67.83%, Test Loss: 0.7279, Test Acc: 65.52%\n",
      "Epoch [34/100], Train Loss: 0.6563, Train Acc: 71.30%, Test Loss: 0.7083, Test Acc: 63.79%, LR: 0.001000\n",
      "New best model saved at epoch 34 with Test Loss: 0.7083, Test Accuracy: 63.79%\n",
      "Epoch [34/100] - Train Loss: 0.6563, Train Acc: 71.30%, Test Loss: 0.7083, Test Acc: 63.79%\n",
      "Epoch [35/100], Train Loss: 0.6364, Train Acc: 77.83%, Test Loss: 0.6909, Test Acc: 65.52%, LR: 0.001000\n",
      "New best model saved at epoch 35 with Test Loss: 0.6909, Test Accuracy: 65.52%\n",
      "Epoch [35/100] - Train Loss: 0.6364, Train Acc: 77.83%, Test Loss: 0.6909, Test Acc: 65.52%\n",
      "Epoch [36/100], Train Loss: 0.6125, Train Acc: 77.83%, Test Loss: 0.6744, Test Acc: 67.24%, LR: 0.001000\n",
      "New best model saved at epoch 36 with Test Loss: 0.6744, Test Accuracy: 67.24%\n",
      "Epoch [36/100] - Train Loss: 0.6125, Train Acc: 77.83%, Test Loss: 0.6744, Test Acc: 67.24%\n",
      "Epoch [37/100], Train Loss: 0.6048, Train Acc: 73.91%, Test Loss: 0.6639, Test Acc: 68.97%, LR: 0.001000\n",
      "New best model saved at epoch 37 with Test Loss: 0.6639, Test Accuracy: 68.97%\n",
      "Epoch [37/100] - Train Loss: 0.6048, Train Acc: 73.91%, Test Loss: 0.6639, Test Acc: 68.97%\n",
      "Epoch [38/100], Train Loss: 0.5905, Train Acc: 74.35%, Test Loss: 0.6523, Test Acc: 70.69%, LR: 0.001000\n",
      "New best model saved at epoch 38 with Test Loss: 0.6523, Test Accuracy: 70.69%\n",
      "Epoch [38/100] - Train Loss: 0.5905, Train Acc: 74.35%, Test Loss: 0.6523, Test Acc: 70.69%\n",
      "Epoch [39/100], Train Loss: 0.5753, Train Acc: 76.09%, Test Loss: 0.6344, Test Acc: 70.69%, LR: 0.001000\n",
      "New best model saved at epoch 39 with Test Loss: 0.6344, Test Accuracy: 70.69%\n",
      "Epoch [39/100] - Train Loss: 0.5753, Train Acc: 76.09%, Test Loss: 0.6344, Test Acc: 70.69%\n",
      "Epoch [40/100], Train Loss: 0.5685, Train Acc: 76.52%, Test Loss: 0.6194, Test Acc: 70.69%, LR: 0.001000\n",
      "New best model saved at epoch 40 with Test Loss: 0.6194, Test Accuracy: 70.69%\n",
      "Epoch [40/100] - Train Loss: 0.5685, Train Acc: 76.52%, Test Loss: 0.6194, Test Acc: 70.69%\n",
      "Epoch [41/100], Train Loss: 0.5594, Train Acc: 75.65%, Test Loss: 0.6124, Test Acc: 70.69%, LR: 0.001000\n",
      "New best model saved at epoch 41 with Test Loss: 0.6124, Test Accuracy: 70.69%\n",
      "Epoch [41/100] - Train Loss: 0.5594, Train Acc: 75.65%, Test Loss: 0.6124, Test Acc: 70.69%\n",
      "Epoch [42/100], Train Loss: 0.5500, Train Acc: 76.96%, Test Loss: 0.6041, Test Acc: 68.97%, LR: 0.001000\n",
      "New best model saved at epoch 42 with Test Loss: 0.6041, Test Accuracy: 68.97%\n",
      "Epoch [42/100] - Train Loss: 0.5500, Train Acc: 76.96%, Test Loss: 0.6041, Test Acc: 68.97%\n",
      "Epoch [43/100], Train Loss: 0.5499, Train Acc: 77.83%, Test Loss: 0.5977, Test Acc: 68.97%, LR: 0.001000\n",
      "New best model saved at epoch 43 with Test Loss: 0.5977, Test Accuracy: 68.97%\n",
      "Epoch [43/100] - Train Loss: 0.5499, Train Acc: 77.83%, Test Loss: 0.5977, Test Acc: 68.97%\n",
      "Epoch [44/100], Train Loss: 0.5313, Train Acc: 80.87%, Test Loss: 0.5941, Test Acc: 70.69%, LR: 0.001000\n",
      "New best model saved at epoch 44 with Test Loss: 0.5941, Test Accuracy: 70.69%\n",
      "Epoch [44/100] - Train Loss: 0.5313, Train Acc: 80.87%, Test Loss: 0.5941, Test Acc: 70.69%\n",
      "Epoch [45/100], Train Loss: 0.5258, Train Acc: 79.57%, Test Loss: 0.5885, Test Acc: 68.97%, LR: 0.001000\n",
      "New best model saved at epoch 45 with Test Loss: 0.5885, Test Accuracy: 68.97%\n",
      "Epoch [45/100] - Train Loss: 0.5258, Train Acc: 79.57%, Test Loss: 0.5885, Test Acc: 68.97%\n",
      "Epoch [46/100], Train Loss: 0.5273, Train Acc: 79.57%, Test Loss: 0.5791, Test Acc: 67.24%, LR: 0.001000\n",
      "New best model saved at epoch 46 with Test Loss: 0.5791, Test Accuracy: 67.24%\n",
      "Epoch [46/100] - Train Loss: 0.5273, Train Acc: 79.57%, Test Loss: 0.5791, Test Acc: 67.24%\n",
      "Epoch [47/100], Train Loss: 0.5153, Train Acc: 79.13%, Test Loss: 0.5697, Test Acc: 68.97%, LR: 0.001000\n",
      "New best model saved at epoch 47 with Test Loss: 0.5697, Test Accuracy: 68.97%\n",
      "Epoch [47/100] - Train Loss: 0.5153, Train Acc: 79.13%, Test Loss: 0.5697, Test Acc: 68.97%\n",
      "Epoch [48/100], Train Loss: 0.5048, Train Acc: 76.96%, Test Loss: 0.5637, Test Acc: 70.69%, LR: 0.001000\n",
      "New best model saved at epoch 48 with Test Loss: 0.5637, Test Accuracy: 70.69%\n",
      "Epoch [48/100] - Train Loss: 0.5048, Train Acc: 76.96%, Test Loss: 0.5637, Test Acc: 70.69%\n",
      "Epoch [49/100], Train Loss: 0.5051, Train Acc: 77.39%, Test Loss: 0.5567, Test Acc: 70.69%, LR: 0.001000\n",
      "New best model saved at epoch 49 with Test Loss: 0.5567, Test Accuracy: 70.69%\n",
      "Epoch [49/100] - Train Loss: 0.5051, Train Acc: 77.39%, Test Loss: 0.5567, Test Acc: 70.69%\n",
      "Epoch [50/100], Train Loss: 0.4972, Train Acc: 75.22%, Test Loss: 0.5538, Test Acc: 70.69%, LR: 0.001000\n",
      "New best model saved at epoch 50 with Test Loss: 0.5538, Test Accuracy: 70.69%\n",
      "Epoch [50/100] - Train Loss: 0.4972, Train Acc: 75.22%, Test Loss: 0.5538, Test Acc: 70.69%\n",
      "Epoch [51/100], Train Loss: 0.4863, Train Acc: 78.70%, Test Loss: 0.5452, Test Acc: 70.69%, LR: 0.001000\n",
      "New best model saved at epoch 51 with Test Loss: 0.5452, Test Accuracy: 70.69%\n",
      "Epoch [51/100] - Train Loss: 0.4863, Train Acc: 78.70%, Test Loss: 0.5452, Test Acc: 70.69%\n",
      "Epoch [52/100], Train Loss: 0.4749, Train Acc: 79.13%, Test Loss: 0.5386, Test Acc: 70.69%, LR: 0.001000\n",
      "New best model saved at epoch 52 with Test Loss: 0.5386, Test Accuracy: 70.69%\n",
      "Epoch [52/100] - Train Loss: 0.4749, Train Acc: 79.13%, Test Loss: 0.5386, Test Acc: 70.69%\n",
      "Epoch [53/100], Train Loss: 0.4754, Train Acc: 82.61%, Test Loss: 0.5339, Test Acc: 68.97%, LR: 0.001000\n",
      "New best model saved at epoch 53 with Test Loss: 0.5339, Test Accuracy: 68.97%\n",
      "Epoch [53/100] - Train Loss: 0.4754, Train Acc: 82.61%, Test Loss: 0.5339, Test Acc: 68.97%\n",
      "Epoch [54/100], Train Loss: 0.4756, Train Acc: 81.74%, Test Loss: 0.5264, Test Acc: 70.69%, LR: 0.001000\n",
      "New best model saved at epoch 54 with Test Loss: 0.5264, Test Accuracy: 70.69%\n",
      "Epoch [54/100] - Train Loss: 0.4756, Train Acc: 81.74%, Test Loss: 0.5264, Test Acc: 70.69%\n",
      "Epoch [55/100], Train Loss: 0.4672, Train Acc: 83.48%, Test Loss: 0.5200, Test Acc: 65.52%, LR: 0.001000\n",
      "New best model saved at epoch 55 with Test Loss: 0.5200, Test Accuracy: 65.52%\n",
      "Epoch [55/100] - Train Loss: 0.4672, Train Acc: 83.48%, Test Loss: 0.5200, Test Acc: 65.52%\n",
      "Epoch [56/100], Train Loss: 0.4620, Train Acc: 82.17%, Test Loss: 0.5130, Test Acc: 68.97%, LR: 0.001000\n",
      "New best model saved at epoch 56 with Test Loss: 0.5130, Test Accuracy: 68.97%\n",
      "Epoch [56/100] - Train Loss: 0.4620, Train Acc: 82.17%, Test Loss: 0.5130, Test Acc: 68.97%\n",
      "Epoch [57/100], Train Loss: 0.4624, Train Acc: 80.00%, Test Loss: 0.5124, Test Acc: 72.41%, LR: 0.001000\n",
      "New best model saved at epoch 57 with Test Loss: 0.5124, Test Accuracy: 72.41%\n",
      "Epoch [57/100] - Train Loss: 0.4624, Train Acc: 80.00%, Test Loss: 0.5124, Test Acc: 72.41%\n",
      "Epoch [58/100], Train Loss: 0.4503, Train Acc: 80.87%, Test Loss: 0.5092, Test Acc: 68.97%, LR: 0.001000\n",
      "New best model saved at epoch 58 with Test Loss: 0.5092, Test Accuracy: 68.97%\n",
      "Epoch [58/100] - Train Loss: 0.4503, Train Acc: 80.87%, Test Loss: 0.5092, Test Acc: 68.97%\n",
      "Epoch [59/100], Train Loss: 0.4510, Train Acc: 79.57%, Test Loss: 0.5075, Test Acc: 70.69%, LR: 0.001000\n",
      "New best model saved at epoch 59 with Test Loss: 0.5075, Test Accuracy: 70.69%\n",
      "Epoch [59/100] - Train Loss: 0.4510, Train Acc: 79.57%, Test Loss: 0.5075, Test Acc: 70.69%\n",
      "Epoch [60/100], Train Loss: 0.4463, Train Acc: 82.17%, Test Loss: 0.4984, Test Acc: 68.97%, LR: 0.001000\n",
      "New best model saved at epoch 60 with Test Loss: 0.4984, Test Accuracy: 68.97%\n",
      "Epoch [60/100] - Train Loss: 0.4463, Train Acc: 82.17%, Test Loss: 0.4984, Test Acc: 68.97%\n",
      "Epoch [61/100], Train Loss: 0.4424, Train Acc: 82.61%, Test Loss: 0.4923, Test Acc: 72.41%, LR: 0.001000\n",
      "New best model saved at epoch 61 with Test Loss: 0.4923, Test Accuracy: 72.41%\n",
      "Epoch [61/100] - Train Loss: 0.4424, Train Acc: 82.61%, Test Loss: 0.4923, Test Acc: 72.41%\n",
      "Epoch [62/100], Train Loss: 0.4345, Train Acc: 83.04%, Test Loss: 0.4908, Test Acc: 68.97%, LR: 0.001000\n",
      "New best model saved at epoch 62 with Test Loss: 0.4908, Test Accuracy: 68.97%\n",
      "Epoch [62/100] - Train Loss: 0.4345, Train Acc: 83.04%, Test Loss: 0.4908, Test Acc: 68.97%\n",
      "Epoch [63/100], Train Loss: 0.4296, Train Acc: 82.61%, Test Loss: 0.4933, Test Acc: 68.97%, LR: 0.001000\n",
      "Epoch [63/100] - Train Loss: 0.4296, Train Acc: 82.61%, Test Loss: 0.4933, Test Acc: 68.97%\n",
      "Epoch [64/100], Train Loss: 0.4296, Train Acc: 80.87%, Test Loss: 0.4995, Test Acc: 65.52%, LR: 0.001000\n",
      "Epoch [64/100] - Train Loss: 0.4296, Train Acc: 80.87%, Test Loss: 0.4995, Test Acc: 65.52%\n",
      "Epoch [65/100], Train Loss: 0.4176, Train Acc: 80.87%, Test Loss: 0.4918, Test Acc: 68.97%, LR: 0.001000\n",
      "Epoch [65/100] - Train Loss: 0.4176, Train Acc: 80.87%, Test Loss: 0.4918, Test Acc: 68.97%\n",
      "Epoch [66/100], Train Loss: 0.4243, Train Acc: 82.17%, Test Loss: 0.4816, Test Acc: 68.97%, LR: 0.001000\n",
      "New best model saved at epoch 66 with Test Loss: 0.4816, Test Accuracy: 68.97%\n",
      "Epoch [66/100] - Train Loss: 0.4243, Train Acc: 82.17%, Test Loss: 0.4816, Test Acc: 68.97%\n",
      "Epoch [67/100], Train Loss: 0.4105, Train Acc: 83.48%, Test Loss: 0.4772, Test Acc: 68.97%, LR: 0.001000\n",
      "New best model saved at epoch 67 with Test Loss: 0.4772, Test Accuracy: 68.97%\n",
      "Epoch [67/100] - Train Loss: 0.4105, Train Acc: 83.48%, Test Loss: 0.4772, Test Acc: 68.97%\n",
      "Epoch [68/100], Train Loss: 0.4149, Train Acc: 82.17%, Test Loss: 0.4752, Test Acc: 67.24%, LR: 0.001000\n",
      "New best model saved at epoch 68 with Test Loss: 0.4752, Test Accuracy: 67.24%\n",
      "Epoch [68/100] - Train Loss: 0.4149, Train Acc: 82.17%, Test Loss: 0.4752, Test Acc: 67.24%\n",
      "Epoch [69/100], Train Loss: 0.4182, Train Acc: 80.43%, Test Loss: 0.4794, Test Acc: 70.69%, LR: 0.001000\n",
      "Epoch [69/100] - Train Loss: 0.4182, Train Acc: 80.43%, Test Loss: 0.4794, Test Acc: 70.69%\n",
      "Epoch [70/100], Train Loss: 0.4138, Train Acc: 81.30%, Test Loss: 0.4809, Test Acc: 68.97%, LR: 0.001000\n",
      "Epoch [70/100] - Train Loss: 0.4138, Train Acc: 81.30%, Test Loss: 0.4809, Test Acc: 68.97%\n",
      "Epoch [71/100], Train Loss: 0.4056, Train Acc: 83.48%, Test Loss: 0.4766, Test Acc: 70.69%, LR: 0.001000\n",
      "Epoch [71/100] - Train Loss: 0.4056, Train Acc: 83.48%, Test Loss: 0.4766, Test Acc: 70.69%\n",
      "Epoch [72/100], Train Loss: 0.4007, Train Acc: 83.91%, Test Loss: 0.4774, Test Acc: 70.69%, LR: 0.001000\n",
      "Epoch [72/100] - Train Loss: 0.4007, Train Acc: 83.91%, Test Loss: 0.4774, Test Acc: 70.69%\n",
      "Epoch [73/100], Train Loss: 0.3961, Train Acc: 83.91%, Test Loss: 0.4729, Test Acc: 70.69%, LR: 0.001000\n",
      "New best model saved at epoch 73 with Test Loss: 0.4729, Test Accuracy: 70.69%\n",
      "Epoch [73/100] - Train Loss: 0.3961, Train Acc: 83.91%, Test Loss: 0.4729, Test Acc: 70.69%\n",
      "Epoch [74/100], Train Loss: 0.3966, Train Acc: 83.48%, Test Loss: 0.4690, Test Acc: 68.97%, LR: 0.001000\n",
      "New best model saved at epoch 74 with Test Loss: 0.4690, Test Accuracy: 68.97%\n",
      "Epoch [74/100] - Train Loss: 0.3966, Train Acc: 83.48%, Test Loss: 0.4690, Test Acc: 68.97%\n",
      "Epoch [75/100], Train Loss: 0.3979, Train Acc: 81.30%, Test Loss: 0.4661, Test Acc: 70.69%, LR: 0.001000\n",
      "New best model saved at epoch 75 with Test Loss: 0.4661, Test Accuracy: 70.69%\n",
      "Epoch [75/100] - Train Loss: 0.3979, Train Acc: 81.30%, Test Loss: 0.4661, Test Acc: 70.69%\n",
      "Epoch [76/100], Train Loss: 0.3910, Train Acc: 81.30%, Test Loss: 0.4627, Test Acc: 70.69%, LR: 0.001000\n",
      "New best model saved at epoch 76 with Test Loss: 0.4627, Test Accuracy: 70.69%\n",
      "Epoch [76/100] - Train Loss: 0.3910, Train Acc: 81.30%, Test Loss: 0.4627, Test Acc: 70.69%\n",
      "Epoch [77/100], Train Loss: 0.3988, Train Acc: 83.91%, Test Loss: 0.4614, Test Acc: 68.97%, LR: 0.001000\n",
      "New best model saved at epoch 77 with Test Loss: 0.4614, Test Accuracy: 68.97%\n",
      "Epoch [77/100] - Train Loss: 0.3988, Train Acc: 83.91%, Test Loss: 0.4614, Test Acc: 68.97%\n",
      "Epoch [78/100], Train Loss: 0.3745, Train Acc: 83.91%, Test Loss: 0.4606, Test Acc: 74.14%, LR: 0.001000\n",
      "New best model saved at epoch 78 with Test Loss: 0.4606, Test Accuracy: 74.14%\n",
      "Epoch [78/100] - Train Loss: 0.3745, Train Acc: 83.91%, Test Loss: 0.4606, Test Acc: 74.14%\n",
      "Epoch [79/100], Train Loss: 0.3889, Train Acc: 84.35%, Test Loss: 0.4622, Test Acc: 72.41%, LR: 0.001000\n",
      "Epoch [79/100] - Train Loss: 0.3889, Train Acc: 84.35%, Test Loss: 0.4622, Test Acc: 72.41%\n",
      "Epoch [80/100], Train Loss: 0.3894, Train Acc: 84.35%, Test Loss: 0.4579, Test Acc: 72.41%, LR: 0.001000\n",
      "New best model saved at epoch 80 with Test Loss: 0.4579, Test Accuracy: 72.41%\n",
      "Epoch [80/100] - Train Loss: 0.3894, Train Acc: 84.35%, Test Loss: 0.4579, Test Acc: 72.41%\n",
      "Epoch [81/100], Train Loss: 0.3831, Train Acc: 83.91%, Test Loss: 0.4508, Test Acc: 72.41%, LR: 0.001000\n",
      "New best model saved at epoch 81 with Test Loss: 0.4508, Test Accuracy: 72.41%\n",
      "Epoch [81/100] - Train Loss: 0.3831, Train Acc: 83.91%, Test Loss: 0.4508, Test Acc: 72.41%\n",
      "Epoch [82/100], Train Loss: 0.3731, Train Acc: 83.48%, Test Loss: 0.4513, Test Acc: 72.41%, LR: 0.001000\n",
      "Epoch [82/100] - Train Loss: 0.3731, Train Acc: 83.48%, Test Loss: 0.4513, Test Acc: 72.41%\n",
      "Epoch [83/100], Train Loss: 0.3793, Train Acc: 83.91%, Test Loss: 0.4561, Test Acc: 70.69%, LR: 0.001000\n",
      "Epoch [83/100] - Train Loss: 0.3793, Train Acc: 83.91%, Test Loss: 0.4561, Test Acc: 70.69%\n",
      "Epoch [84/100], Train Loss: 0.3702, Train Acc: 83.91%, Test Loss: 0.4530, Test Acc: 72.41%, LR: 0.001000\n",
      "Epoch [84/100] - Train Loss: 0.3702, Train Acc: 83.91%, Test Loss: 0.4530, Test Acc: 72.41%\n",
      "Epoch [85/100], Train Loss: 0.3650, Train Acc: 84.78%, Test Loss: 0.4496, Test Acc: 72.41%, LR: 0.001000\n",
      "New best model saved at epoch 85 with Test Loss: 0.4496, Test Accuracy: 72.41%\n",
      "Epoch [85/100] - Train Loss: 0.3650, Train Acc: 84.78%, Test Loss: 0.4496, Test Acc: 72.41%\n",
      "Epoch [86/100], Train Loss: 0.3652, Train Acc: 83.91%, Test Loss: 0.4489, Test Acc: 72.41%, LR: 0.001000\n",
      "New best model saved at epoch 86 with Test Loss: 0.4489, Test Accuracy: 72.41%\n",
      "Epoch [86/100] - Train Loss: 0.3652, Train Acc: 83.91%, Test Loss: 0.4489, Test Acc: 72.41%\n",
      "Epoch [87/100], Train Loss: 0.3596, Train Acc: 84.35%, Test Loss: 0.4485, Test Acc: 74.14%, LR: 0.001000\n",
      "New best model saved at epoch 87 with Test Loss: 0.4485, Test Accuracy: 74.14%\n",
      "Epoch [87/100] - Train Loss: 0.3596, Train Acc: 84.35%, Test Loss: 0.4485, Test Acc: 74.14%\n",
      "Epoch [88/100], Train Loss: 0.3742, Train Acc: 85.22%, Test Loss: 0.4447, Test Acc: 74.14%, LR: 0.001000\n",
      "New best model saved at epoch 88 with Test Loss: 0.4447, Test Accuracy: 74.14%\n",
      "Epoch [88/100] - Train Loss: 0.3742, Train Acc: 85.22%, Test Loss: 0.4447, Test Acc: 74.14%\n",
      "Epoch [89/100], Train Loss: 0.3563, Train Acc: 84.78%, Test Loss: 0.4432, Test Acc: 74.14%, LR: 0.001000\n",
      "New best model saved at epoch 89 with Test Loss: 0.4432, Test Accuracy: 74.14%\n",
      "Epoch [89/100] - Train Loss: 0.3563, Train Acc: 84.78%, Test Loss: 0.4432, Test Acc: 74.14%\n",
      "Epoch [90/100], Train Loss: 0.3566, Train Acc: 84.78%, Test Loss: 0.4453, Test Acc: 75.86%, LR: 0.001000\n",
      "Epoch [90/100] - Train Loss: 0.3566, Train Acc: 84.78%, Test Loss: 0.4453, Test Acc: 75.86%\n",
      "Epoch [91/100], Train Loss: 0.3452, Train Acc: 84.35%, Test Loss: 0.4472, Test Acc: 75.86%, LR: 0.001000\n",
      "Epoch [91/100] - Train Loss: 0.3452, Train Acc: 84.35%, Test Loss: 0.4472, Test Acc: 75.86%\n",
      "Epoch [92/100], Train Loss: 0.3493, Train Acc: 84.35%, Test Loss: 0.4483, Test Acc: 72.41%, LR: 0.001000\n",
      "Epoch [92/100] - Train Loss: 0.3493, Train Acc: 84.35%, Test Loss: 0.4483, Test Acc: 72.41%\n",
      "Epoch [93/100], Train Loss: 0.3400, Train Acc: 84.35%, Test Loss: 0.4334, Test Acc: 74.14%, LR: 0.001000\n",
      "New best model saved at epoch 93 with Test Loss: 0.4334, Test Accuracy: 74.14%\n",
      "Epoch [93/100] - Train Loss: 0.3400, Train Acc: 84.35%, Test Loss: 0.4334, Test Acc: 74.14%\n",
      "Epoch [94/100], Train Loss: 0.3452, Train Acc: 84.35%, Test Loss: 0.4276, Test Acc: 74.14%, LR: 0.001000\n",
      "New best model saved at epoch 94 with Test Loss: 0.4276, Test Accuracy: 74.14%\n",
      "Epoch [94/100] - Train Loss: 0.3452, Train Acc: 84.35%, Test Loss: 0.4276, Test Acc: 74.14%\n",
      "Epoch [95/100], Train Loss: 0.3368, Train Acc: 84.78%, Test Loss: 0.4298, Test Acc: 75.86%, LR: 0.001000\n",
      "Epoch [95/100] - Train Loss: 0.3368, Train Acc: 84.78%, Test Loss: 0.4298, Test Acc: 75.86%\n",
      "Epoch [96/100], Train Loss: 0.3369, Train Acc: 86.96%, Test Loss: 0.4309, Test Acc: 75.86%, LR: 0.001000\n",
      "Epoch [96/100] - Train Loss: 0.3369, Train Acc: 86.96%, Test Loss: 0.4309, Test Acc: 75.86%\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(0, epochs):\n",
    "    model.train()\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        \"\"\"if torch.rand(1).item() < 0.5:\n",
    "            batch_X = add_jitter(batch_X, noise_level=0.02)\n",
    "        if torch.rand(1).item() < 0.3:\n",
    "            batch_X = scale_signal(batch_X, scale_range=(0.8, 1.2))\n",
    "        if torch.rand(1).item() < 0.3:\n",
    "            batch_X = time_warp(batch_X, sigma=0.2)\n",
    "        if torch.rand(1).item() < 0.3:\n",
    "            batch_X = random_crop(batch_X, crop_size=0.8)\n",
    "        if torch.rand(1).item() < 0.2:\n",
    "            batch_X = permute_segments(batch_X, num_segments=4)\"\"\"\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss, train_accuracy = evaluate_model(train_loader, model, criterion)\n",
    "    test_loss, test_accuracy = evaluate_model(test_loader, model, criterion)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    log_message = (f\"Epoch [{epoch+1}/{epochs}], \"\n",
    "                   f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
    "                   f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2f}%, \"\n",
    "                   f\"LR: {current_lr:.6f}\")\n",
    "\n",
    "    logging.info(log_message)\n",
    "    print(log_message)\n",
    "\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        save_best_model(epoch, model, optimizer, test_loss, train_losses, test_losses)\n",
    "        print(f\"New best model saved at epoch {epoch+1} with Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
    "          f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\")\n",
    "\n",
    "    scheduler.step(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f5a0a4-374a-40dd-9d27-81279b433394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(test_losses) + 1), test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Testing Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79cb82-164f-4ed0-ba0b-63dcc6f6bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Activities for the Confusion matrix\n",
    "ACTIVITIES = ['sit', 'walk','upstair']\n",
    "\n",
    "# Model evaluation (confusion matrix)\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Loop through the test loader to collect predictions and true labels\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Compute the confusion matrix, explicitly specifying the labels\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions, labels=[0, 1, 2]) \n",
    "\n",
    "# Assuming conf_matrix and ACTIVITIES are already defined\n",
    "class_accuracies = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)  # Compute per-class accuracy\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(conf_matrix, display_labels=ACTIVITIES)\n",
    "fig, ax = plt.subplots()\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "\n",
    "# Add per-class accuracy text\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "    acc_text = f\"{class_accuracies[i] * 100:.2f}%\"\n",
    "    ax.text(\n",
    "        len(ACTIVITIES) + 0.3, i, acc_text, \n",
    "        fontsize=12, verticalalignment='center', color='black'\n",
    "    )\n",
    "\n",
    "plt.title(\"Confusion Matrix for Activities with Per-Class Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f841cd-23c7-425c-a8c0-6e363b2c94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "incorrect_samples = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)  # Get raw logits\n",
    "        probabilities = F.softmax(outputs, dim=1)  # Convert to probabilities\n",
    "        predicted_labels = torch.argmax(probabilities, dim=1)  # Get predicted class\n",
    "        confidence_scores = torch.max(probabilities, dim=1).values  # Get max confidence\n",
    "\n",
    "        # Find incorrect predictions\n",
    "        incorrect_indices = (predicted_labels != batch_y)\n",
    "        for i in range(len(batch_y)):\n",
    "            if incorrect_indices[i]:\n",
    "                incorrect_samples.append({\n",
    "                    \"True Label\": batch_y[i].item(),\n",
    "                    \"Predicted Label\": predicted_labels[i].item(),\n",
    "                    \"Confidence\": confidence_scores[i].item(),\n",
    "                    \"Probabilities\": probabilities[i].tolist()\n",
    "                })\n",
    "\n",
    "# Print results\n",
    "for sample in incorrect_samples:\n",
    "    print(f\"True Label: {sample['True Label']}, Predicted: {sample['Predicted Label']}, Confidence: {sample['Confidence']:.4f}\")\n",
    "    print(f\"Full Probabilities: {sample['Probabilities']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ec9986-bae2-4e5f-9a83-0fa0902dc6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#move up to use if need to find best chunk size\n",
    "chunk_sizes = [\n",
    "    0.25, 0.5, 1.0, 1.375, 2.0, 2.75, 3.0, 3.5, 4.0, 5, 5.5\n",
    "]\n",
    "accuracies = []\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    print(f\"\\nTesting chunk size: {chunk_size} seconds\")\n",
    "    \n",
    "    training_data, training_labels, _ = chunk_data(training_data_raw, training_docs, chunk_size, ACTIVITIES, SAMPLING_RATE)\n",
    "    testing_data, testing_labels, _ = chunk_data(testing_data_raw, testing_docs, chunk_size, ACTIVITIES, SAMPLING_RATE)\n",
    "    \n",
    "    training_data = np.array(training_data)\n",
    "    testing_data = np.array(testing_data)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(training_data.reshape(-1, training_data.shape[-1])).reshape(training_data.shape)\n",
    "    X_test = scaler.transform(testing_data.reshape(-1, testing_data.shape[-1])).reshape(testing_data.shape)\n",
    "    \n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(training_labels, dtype=torch.long)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(testing_labels, dtype=torch.long)\n",
    "    \n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    test_data = TensorDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True, pin_memory=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=64, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    model = OptimizedCNNModel(num_classes=NUM_CLASSES, input_channels=3, seq_length=int(CHUNK_SIZE * 100))\n",
    "    optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    test_loss, test_accuracy = evaluate_model(test_loader, model, criterion)\n",
    "    accuracies.append(test_accuracy)\n",
    "    print(f\"Chunk size {chunk_size}s - Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "best_chunk_size = chunk_sizes[np.argmax(accuracies)]\n",
    "print(f\"\\nBest Chunk Size: {best_chunk_size} with Test Accuracy: {max(accuracies):.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
