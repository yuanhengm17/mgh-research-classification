{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30e998e0-f6cd-4d3d-82f0-08637032af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from firebase_admin import credentials, firestore\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Firebase Initialization\n",
    "cred = credentials.Certificate(\"adminkey.json\")\n",
    "#firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename='training_log.txt', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "INCLUDE_ONLY = ['Stephen', 'Lillian', 'Ren', 'Yuanheng', 'Ethan Shao', 'z']\n",
    "ACTIVITIES = ['sit','walk','upstair']\n",
    "CHUNK_SIZE = 1.375  # in seconds (can be a decimal)\n",
    "SAMPLING_RATE = 100  # Hz\n",
    "NUM_CLASSES = 3\n",
    "OVERLAP = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24622a39-7713-4bc5-be27-67c17c8c5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(chunk):\n",
    "    \"\"\"Extract features from a chunked acceleration segment with selected statistics.\"\"\"\n",
    "    feature_vector = []\n",
    "    \n",
    "    for axis in ['x', 'y', 'z']:\n",
    "        data_series = pd.Series(chunk[axis])\n",
    "        # Apply smoothing\n",
    "        smoothed_data = data_series.rolling(window=5, min_periods=1).mean()\n",
    "        feature_vector.extend([\n",
    "            smoothed_data.mean(),                  # Mean\n",
    "            smoothed_data.median(),                # Median\n",
    "            smoothed_data.std(),                   # Standard Deviation\n",
    "            smoothed_data.var(),                   # Variance\n",
    "            smoothed_data.min(),                   # Minimum\n",
    "            smoothed_data.max(),                   # Maximum\n",
    "        ])\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51a0fe31-556e-43bf-84e4-fd36d0a77bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data_with_overlap(data_raw, docs, chunk_size, activities, sampling_rate, overlap=0.5):\n",
    "    \"\"\"Chunk raw acceleration data into smaller labeled segments using overlapping windows.\"\"\"\n",
    "    data, labels = [], []\n",
    "    chunk_samples = int(chunk_size * sampling_rate)\n",
    "    step = int(chunk_samples * (1 - overlap))  # compute step size based on overlap\n",
    "\n",
    "    for i, df in enumerate(data_raw):\n",
    "        # Slide over the data with the defined step\n",
    "        for start in range(0, len(df) - chunk_samples + 1, step):\n",
    "            end = start + chunk_samples     \n",
    "            chunk = df.iloc[start:end]\n",
    "            activity = docs[i]['activity']\n",
    "            label = activities.index(activity)\n",
    "            data.append(extract_features(chunk))\n",
    "            labels.append(label)\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Use overlapping window chunking\n",
    "X_train, y_train = chunk_data_with_overlap(training_data_raw, training_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE, OVERLAP)\n",
    "X_test, y_test = chunk_data_with_overlap(testing_data_raw, testing_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE, OVERLAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "287ad27b-8007-4ccd-b113-f8016c2aaec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Structures\n",
    "def fetch_data(collection_name, activities, include_only, time_start=500, time_end=6000):\n",
    "    \"\"\"Fetch and preprocess data from Firestore.\"\"\"\n",
    "    data, docs = [], []\n",
    "    for person in db.collection(collection_name).stream():\n",
    "        person_name = str(person.to_dict().get('name', ''))\n",
    "        if person_name not in include_only:\n",
    "            continue\n",
    "\n",
    "        for activity in activities:\n",
    "            for recording in db.collection(collection_name).document(person_name).collection(activity).stream():\n",
    "                record = recording.to_dict()\n",
    "                if 'acceleration' not in record:\n",
    "                    continue\n",
    "\n",
    "                docs.append(record)\n",
    "                df = pd.DataFrame(record['acceleration'])\n",
    "                \n",
    "                if 'time' in df.columns:\n",
    "                    filtered_df = df[(df['time'] >= time_start) & (df['time'] <= time_end)]\n",
    "                    data.append(filtered_df)\n",
    "                else:\n",
    "                    raise ValueError(\"The 'acceleration' field must include a 'time' column.\")\n",
    "    return data, docs\n",
    "\n",
    "training_data_raw, training_docs = fetch_data(\"training\", ACTIVITIES, INCLUDE_ONLY)\n",
    "testing_data_raw, testing_docs = fetch_data(\"testing\", ACTIVITIES, INCLUDE_ONLY)\n",
    "\n",
    "# Split into 80% training, 20% testing\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42, stratify=y_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1284a61-2522-4348-abbf-1d4e31e36bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data_raw, docs, chunk_size, activities, sampling_rate):\n",
    "    \"\"\"Split data into chunks and assign labels.\"\"\"\n",
    "    data, labels = [], []\n",
    "    activity_distribution = np.zeros(len(activities))\n",
    "    chunk_samples = int(chunk_size * sampling_rate)  # Convert time to sample count\n",
    "\n",
    "    for i in range(len(data_raw)):\n",
    "        num_chunks = len(data_raw[i]) // chunk_samples\n",
    "        for j in range(num_chunks):\n",
    "            start = j * chunk_samples\n",
    "            end = start + chunk_samples\n",
    "            x = list(data_raw[i][\"x\"])[start:end]\n",
    "            y = list(data_raw[i][\"y\"])[start:end]\n",
    "            z = list(data_raw[i][\"z\"])[start:end]\n",
    "            activity = docs[i]['activity']\n",
    "            label = activities.index(activity)\n",
    "\n",
    "            activity_distribution[label] += 1\n",
    "            data.append([x, y, z])\n",
    "            labels.append(label)\n",
    "\n",
    "    return data, labels, activity_distribution\n",
    "\n",
    "\n",
    "#training_data, training_labels, training_distribution = chunk_data(training_data_raw, training_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE)\n",
    "#testing_data, testing_labels, testing_distribution = chunk_data(testing_data_raw, testing_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE)\n",
    "\n",
    "all_data_raw = training_data_raw + testing_data_raw\n",
    "all_docs = training_docs + testing_docs\n",
    "all_data, all_labels, all_distribution = chunk_data(all_data_raw, all_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE)\n",
    "\n",
    "indices = np.arange(len(all_data))\n",
    "train, test = train_test_split(indices, test_size=0.2, random_state=42, stratify=all_labels)\n",
    "\n",
    "training_data = [all_data[i] for i in train]\n",
    "training_labels = [all_labels[i] for i in train]\n",
    "testing_data = [all_data[i] for i in test]\n",
    "testing_labels = [all_labels[i] for i in test]\n",
    "\n",
    "training_distribution = np.bincount(training_labels, minlength=len(ACTIVITIES))\n",
    "testing_distribution = np.bincount(testing_labels, minlength=len(ACTIVITIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43de7965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset summary:\n",
      "+----------+------------------+\n",
      "| Dataset  | number of chunks |\n",
      "+----------+------------------+\n",
      "| training |       230        |\n",
      "| testing  |        58        |\n",
      "+----------+------------------+\n",
      "Training Activities Count\n",
      "sit: 77 chunks\n",
      "walk: 77 chunks\n",
      "upstair: 76 chunks\n",
      "\n",
      "Testing Activity Count\n",
      "sit:19 chunks\n",
      "walk:19 chunks\n",
      "upstair:20 chunks\n",
      "230\n",
      "58\n",
      "(230, 3, 137)\n",
      "36\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate #for table formatting\n",
    "\n",
    "num_training_samples = len(training_data)\n",
    "num_testing_samples = len(testing_data)\n",
    "\n",
    "#table\n",
    "summary_table = [[\"training\", num_training_samples], [\"testing\", num_testing_samples]]\n",
    "\n",
    "print(\"dataset summary:\")\n",
    "print(tabulate(summary_table, headers = [\"Dataset\", \"number of chunks\"], tablefmt=\"pretty\"))\n",
    "\n",
    "print(\"Training Activities Count\")\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "    print(f\"{activity}: {int(training_distribution[i])} chunks\")\n",
    "\n",
    "print(\"\\nTesting Activity Count\")\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "    print(f\"{activity}:{int(testing_distribution[i])} chunks\")\n",
    "print(len(training_data))\n",
    "print(len(testing_data))\n",
    "print(np.array(training_data).shape)\n",
    "print(len(training_data_raw))\n",
    "print(len(testing_data_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae1da3e-3e2c-40ad-82cb-6d090f2cc7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all data for cross-validation (already done earlier as all_data)\n",
    "all_data = np.array(all_data)  # Convert to numpy array\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Normalize the entire dataset\n",
    "scaler = StandardScaler()\n",
    "X_all = scaler.fit_transform(all_data.reshape(-1, all_data.shape[-1])).reshape(all_data.shape)\n",
    "y_all = all_labels\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_all = torch.tensor(X_all, dtype=torch.float32)\n",
    "y_all = torch.tensor(y_all, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec68f8d-85ff-48e1-b219-a547e9393c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedCNNModel(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=3, seq_length=int(CHUNK_SIZE * 100)):\n",
    "        super(OptimizedCNNModel, self).__init__()\n",
    "\n",
    "        def depthwise_separable_conv(in_channels, out_channels, kernel_size=3, padding=1):\n",
    "            \"\"\"Depthwise Separable Convolution for efficiency.\"\"\"\n",
    "            return nn.Sequential(\n",
    "                nn.Conv1d(in_channels, in_channels, kernel_size, groups=in_channels, padding=padding, bias=False),\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "                nn.GroupNorm(8, out_channels),  # More stable than BatchNorm for small batches\n",
    "                nn.SiLU()  # Swish activation (better than ReLU)\n",
    "            )\n",
    "\n",
    "        self.conv1 = depthwise_separable_conv(input_channels, 8)\n",
    "        self.conv2 = depthwise_separable_conv(8, 16)\n",
    "        self.conv3 = depthwise_separable_conv(16, 32)\n",
    "\n",
    "        # Adaptive pooling to ensure flexible sequence length compatibility\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        self.fc1 = nn.Linear(32, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, num_classes)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "        self.apply(self.initialize_weights)\n",
    "\n",
    "    def initialize_weights(self, m):\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "\n",
    "        # Global Average Pooling to reduce to fixed size\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = F.silu(self.fc1(x))  # Swish activation\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fea2d82-2721-4226-87c8-d0386cb60e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_PATH = \"best_model.pth\"\n",
    "BEST_METADATA_PATH = \"best_model.json\"\n",
    "\n",
    "def save_best_model(epoch, model, optimizer, test_loss, train_losses, test_losses):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'test_loss': test_loss,\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses\n",
    "    }, BEST_MODEL_PATH)\n",
    "\n",
    "    # Save metadata\n",
    "    with open(BEST_METADATA_PATH, \"w\") as f:\n",
    "        json.dump({\"epoch\": epoch, \"test_loss\": test_loss}, f)\n",
    "\n",
    "def load_best_model(model, optimizer):\n",
    "    if os.path.exists(BEST_MODEL_PATH) and os.path.exists(BEST_METADATA_PATH):\n",
    "        checkpoint = torch.load(BEST_MODEL_PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_loss = checkpoint['test_loss']\n",
    "        train_losses = checkpoint.get('train_losses', [])\n",
    "        test_losses = checkpoint.get('test_losses', [])\n",
    "\n",
    "        print(f\"Loaded best model from epoch {start_epoch} with Avg Loss: {best_loss:.4f}\")\n",
    "        return start_epoch, best_loss, train_losses, test_losses\n",
    "    return 0, float('inf'), 0.0, [], []  # Start fresh if no saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb6cc02-f6d5-441e-8ea1-736efc276305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(loader, model, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # disable gradient calc\n",
    "        for data, targets in loader:\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # accuracy\n",
    "            _, predicted = torch.max(outputs, 1)  # get predicted class (max value)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc68f1f4-225f-455c-b6d0-80a76a3ce598",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OptimizedCNNModel(num_classes=NUM_CLASSES, input_channels=3, seq_length=int(CHUNK_SIZE * 100))\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Weight decay helps reduce overfitting\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-4)\n",
    "\n",
    "# Load best model if exists\n",
    "#start_epoch, best_loss, best_avg_accuracy, train_losses, test_losses = load_best_model(model, optimizer)\n",
    "start_epoch, best_loss, best_avg_accuracy, train_losses, test_losses =  0, float('inf'), 0.0, [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd628c08-376b-468a-ab5f-6c1a88c7587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "fold_losses = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_all)):\n",
    "    print(f'Fold {fold + 1}/{n_folds}')\n",
    "    \n",
    "    # Split data\n",
    "    X_train_fold = X_all[train_idx]\n",
    "    y_train_fold = y_all[train_idx]\n",
    "    X_val_fold = X_all[val_idx]\n",
    "    y_val_fold = y_all[val_idx]\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_train_fold, y_train_fold)\n",
    "    val_dataset = TensorDataset(X_val_fold, y_val_fold)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    # Initialize model for each fold\n",
    "    model = OptimizedCNNModel(\n",
    "        num_classes=NUM_CLASSES,\n",
    "        input_channels=3,\n",
    "        seq_length=int(CHUNK_SIZE * SAMPLING_RATE)\n",
    "    )\n",
    "    optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-4)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_accuracy = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(300): \n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        train_loss, train_accuracy = evaluate_model(train_loader, model, criterion)\n",
    "        val_loss, val_accuracy = evaluate_model(val_loader, model, criterion)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Log and print\n",
    "        log_message = (f\"Fold {fold+1} Epoch [{epoch+1}/300], \"\n",
    "                      f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
    "                      f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "        logging.info(log_message)\n",
    "        print(log_message)\n",
    "        \n",
    "        # Save best model based on validation accuracy\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()  # Save the model state\n",
    "            save_best_model(epoch, model, optimizer, val_loss, train_losses, val_losses)\n",
    "    \n",
    "    # After training, store the best results for this fold\n",
    "    fold_accuracies.append(best_val_accuracy)\n",
    "    fold_losses.append(best_val_loss)\n",
    "    print(f'Fold {fold + 1} Best Accuracy: {best_val_accuracy:.2f}%')\n",
    "    \n",
    "\n",
    "# Print summary\n",
    "mean_accuracy = np.mean(fold_accuracies)\n",
    "std_accuracy = np.std(fold_accuracies)\n",
    "print(f'\\n5-Fold CV Results:')\n",
    "print(f'Average Accuracy: {mean_accuracy:.2f}% (±{std_accuracy:.2f})')\n",
    "print(f'Fold Accuracies: {[f\"{acc:.2f}%\" for acc in fold_accuracies]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f5a0a4-374a-40dd-9d27-81279b433394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(test_losses) + 1), test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Testing Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79cb82-164f-4ed0-ba0b-63dcc6f6bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Activities for Confusion matrix\n",
    "ACTIVITIES = ['sit', 'walk','upstair']\n",
    "\n",
    "# Model evaluation (confusion matrix)\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Loop through the test loader to collect predictions and true labels\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Compute the confusion matrix, explicitly specifying the labels\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions, labels=[0, 1, 2]) \n",
    "\n",
    "# Assuming conf_matrix and ACTIVITIES are already defined\n",
    "class_accuracies = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)  # Compute per-class accuracy\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(conf_matrix, display_labels=ACTIVITIES)\n",
    "fig, ax = plt.subplots()\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "\n",
    "# Add per-class accuracy text\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "    acc_text = f\"{class_accuracies[i] * 100:.2f}%\"\n",
    "    ax.text(\n",
    "        len(ACTIVITIES) + 0.3, i, acc_text, \n",
    "        fontsize=12, verticalalignment='center', color='black'\n",
    "    )\n",
    "\n",
    "plt.title(\"Confusion Matrix for Activities with Per-Class Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f841cd-23c7-425c-a8c0-6e363b2c94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "incorrect_samples = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)  # Get raw logits\n",
    "        probabilities = F.softmax(outputs, dim=1)  # Convert to probabilities\n",
    "        predicted_labels = torch.argmax(probabilities, dim=1)  # Get predicted class\n",
    "        confidence_scores = torch.max(probabilities, dim=1).values  # Get max confidence\n",
    "\n",
    "        # Find incorrect predictions\n",
    "        incorrect_indices = (predicted_labels != batch_y)\n",
    "        for i in range(len(batch_y)):\n",
    "            if incorrect_indices[i]:\n",
    "                incorrect_samples.append({\n",
    "                    \"True Label\": batch_y[i].item(),\n",
    "                    \"Predicted Label\": predicted_labels[i].item(),\n",
    "                    \"Confidence\": confidence_scores[i].item(),\n",
    "                    \"Probabilities\": probabilities[i].tolist()\n",
    "                })\n",
    "\n",
    "for sample in incorrect_samples:\n",
    "    print(f\"True Label: {sample['True Label']}, Predicted: {sample['Predicted Label']}, Confidence: {sample['Confidence']:.4f}\")\n",
    "    print(f\"Full Probabilities: {sample['Probabilities']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ec9986-bae2-4e5f-9a83-0fa0902dc6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "chunk_sizes = [\n",
    "    0.25, 0.5, 1.0, 1.375, 2.75, 5.5\n",
    "]\n",
    "accuracies = []\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    print(f\"\\nTesting chunk size: {chunk_size} seconds\")\n",
    "    \n",
    "    training_data, training_labels, _ = chunk_data(training_data_raw, training_docs, chunk_size, ACTIVITIES, SAMPLING_RATE)\n",
    "    testing_data, testing_labels, _ = chunk_data(testing_data_raw, testing_docs, chunk_size, ACTIVITIES, SAMPLING_RATE)\n",
    "    \n",
    "    training_data = np.array(training_data)\n",
    "    testing_data = np.array(testing_data)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(training_data.reshape(-1, training_data.shape[-1])).reshape(training_data.shape)\n",
    "    X_test = scaler.transform(testing_data.reshape(-1, testing_data.shape[-1])).reshape(testing_data.shape)\n",
    "    \n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(training_labels, dtype=torch.long)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(testing_labels, dtype=torch.long)\n",
    "    \n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    test_data = TensorDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True, pin_memory=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=64, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    model = OptimizedCNNModel(num_classes=NUM_CLASSES, input_channels=3, seq_length=int(chunk_size * 100))\n",
    "    optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    epochs = 300\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    test_loss, test_accuracy = evaluate_model(test_loader, model, criterion)\n",
    "    accuracies.append(test_accuracy)\n",
    "    print(f\"Chunk size {chunk_size}s - Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "best_chunk_size = chunk_sizes[np.argmax(accuracies)]\n",
    "print(f\"\\nBest Chunk Size: {best_chunk_size} with Test Accuracy: {max(accuracies):.2f}%\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(chunk_sizes, accuracies, marker='o', linestyle='-')\n",
    "plt.xlabel('Chunk Size (seconds)')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.title('Chunk Size vs Test Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b55c2f2-c90e-4e63-bc46-bce2ed3468ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
