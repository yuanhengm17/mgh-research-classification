{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e998e0-f6cd-4d3d-82f0-08637032af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from firebase_admin import credentials, firestore\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Firebase Initialization\n",
    "cred = credentials.Certificate(\"adminkey.json\")\n",
    "firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename='training_log.txt', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "INCLUDE_ONLY = ['Stephen', 'Lillian', 'Ren', 'Yuanheng', 'Ethan Shao', 'z']\n",
    "ACTIVITIES = ['sit','walk','upstair']\n",
    "CHUNK_SIZE = 1.375  # in seconds (can be a decimal)\n",
    "SAMPLING_RATE = 100  # Hz\n",
    "NUM_CLASSES = 3\n",
    "OVERLAP = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24622a39-7713-4bc5-be27-67c17c8c5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(chunk):\n",
    "    \"\"\"Extract features from a chunked acceleration segment with selected statistics.\"\"\"\n",
    "    feature_vector = []\n",
    "    \n",
    "    for axis in ['x', 'y', 'z']:\n",
    "        data_series = pd.Series(chunk[axis])\n",
    "        # Apply smoothing\n",
    "        smoothed_data = data_series.rolling(window=5, min_periods=1).mean()\n",
    "        feature_vector.extend([\n",
    "            smoothed_data.mean(),                  # Mean\n",
    "            smoothed_data.median(),                # Median\n",
    "            smoothed_data.std(),                   # Standard Deviation\n",
    "            smoothed_data.var(),                   # Variance\n",
    "            smoothed_data.min(),                   # Minimum\n",
    "            smoothed_data.max(),                   # Maximum\n",
    "        ])\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "287ad27b-8007-4ccd-b113-f8016c2aaec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Structures\n",
    "def fetch_data(collection_name, activities, include_only, time_start=500, time_end=6000):\n",
    "    \"\"\"Fetch and preprocess data from Firestore.\"\"\"\n",
    "    data, docs = [], []\n",
    "    for person in db.collection(collection_name).stream():\n",
    "        person_name = str(person.to_dict().get('name', ''))\n",
    "        if person_name not in include_only:\n",
    "            continue\n",
    "\n",
    "        for activity in activities:\n",
    "            for recording in db.collection(collection_name).document(person_name).collection(activity).stream():\n",
    "                record = recording.to_dict()\n",
    "                if 'acceleration' not in record:\n",
    "                    continue\n",
    "\n",
    "                docs.append(record)\n",
    "                df = pd.DataFrame(record['acceleration'])\n",
    "                \n",
    "                if 'time' in df.columns:\n",
    "                    filtered_df = df[(df['time'] >= time_start) & (df['time'] <= time_end)]\n",
    "                    data.append(filtered_df)\n",
    "                else:\n",
    "                    raise ValueError(\"The 'acceleration' field must include a 'time' column.\")\n",
    "    return data, docs\n",
    "\n",
    "training_data_raw, training_docs = fetch_data(\"training\", ACTIVITIES, INCLUDE_ONLY)\n",
    "testing_data_raw, testing_docs = fetch_data(\"testing\", ACTIVITIES, INCLUDE_ONLY)\n",
    "\n",
    "# Split into 80% training, 20% testing\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42, stratify=y_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51a0fe31-556e-43bf-84e4-fd36d0a77bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data_with_overlap(data_raw, docs, chunk_size, activities, sampling_rate, overlap=0.5):\n",
    "    \"\"\"Chunk raw acceleration data into smaller labeled segments using overlapping windows.\"\"\"\n",
    "    data, labels = [], []\n",
    "    chunk_samples = int(chunk_size * sampling_rate)\n",
    "    step = int(chunk_samples * (1 - overlap))  # compute step size based on overlap\n",
    "\n",
    "    for i, df in enumerate(data_raw):\n",
    "        # Slide over the data with the defined step\n",
    "        for start in range(0, len(df) - chunk_samples + 1, step):\n",
    "            end = start + chunk_samples     \n",
    "            chunk = df.iloc[start:end]\n",
    "            activity = docs[i]['activity']\n",
    "            label = activities.index(activity)\n",
    "            data.append(extract_features(chunk))\n",
    "            labels.append(label)\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Use overlapping window chunking\n",
    "X_train, y_train = chunk_data_with_overlap(training_data_raw, training_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE, OVERLAP)\n",
    "X_test, y_test = chunk_data_with_overlap(testing_data_raw, testing_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE, OVERLAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1284a61-2522-4348-abbf-1d4e31e36bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data_raw, docs, chunk_size, activities, sampling_rate):\n",
    "    \"\"\"Split data into chunks and assign labels.\"\"\"\n",
    "    data, labels = [], []\n",
    "    activity_distribution = np.zeros(len(activities))\n",
    "    chunk_samples = int(chunk_size * sampling_rate)  # Convert time to sample count\n",
    "\n",
    "    for i in range(len(data_raw)):\n",
    "        num_chunks = len(data_raw[i]) // chunk_samples\n",
    "        for j in range(num_chunks):\n",
    "            start = j * chunk_samples\n",
    "            end = start + chunk_samples\n",
    "            x = list(data_raw[i][\"x\"])[start:end]\n",
    "            y = list(data_raw[i][\"y\"])[start:end]\n",
    "            z = list(data_raw[i][\"z\"])[start:end]\n",
    "            activity = docs[i]['activity']\n",
    "            label = activities.index(activity)\n",
    "\n",
    "            activity_distribution[label] += 1\n",
    "            data.append([x, y, z])\n",
    "            labels.append(label)\n",
    "\n",
    "    return data, labels, activity_distribution\n",
    "\n",
    "\n",
    "#training_data, training_labels, training_distribution = chunk_data(training_data_raw, training_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE)\n",
    "#testing_data, testing_labels, testing_distribution = chunk_data(testing_data_raw, testing_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE)\n",
    "\n",
    "all_data_raw = training_data_raw + testing_data_raw\n",
    "all_docs = training_docs + testing_docs\n",
    "all_data, all_labels, all_distribution = chunk_data(all_data_raw, all_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE)\n",
    "\n",
    "indices = np.arange(len(all_data))\n",
    "train, test = train_test_split(indices, test_size=0.2, random_state=42, stratify=all_labels)\n",
    "\n",
    "training_data = [all_data[i] for i in train]\n",
    "training_labels = [all_labels[i] for i in train]\n",
    "testing_data = [all_data[i] for i in test]\n",
    "testing_labels = [all_labels[i] for i in test]\n",
    "\n",
    "training_distribution = np.bincount(training_labels, minlength=len(ACTIVITIES))\n",
    "testing_distribution = np.bincount(testing_labels, minlength=len(ACTIVITIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43de7965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset summary:\n",
      "+----------+------------------+\n",
      "| Dataset  | number of chunks |\n",
      "+----------+------------------+\n",
      "| training |       230        |\n",
      "| testing  |        58        |\n",
      "+----------+------------------+\n",
      "Training Activities Count\n",
      "sit: 77 chunks\n",
      "walk: 77 chunks\n",
      "upstair: 76 chunks\n",
      "\n",
      "Testing Activity Count\n",
      "sit:19 chunks\n",
      "walk:19 chunks\n",
      "upstair:20 chunks\n",
      "230\n",
      "58\n",
      "(230, 3, 137)\n",
      "36\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate #for table formatting\n",
    "\n",
    "num_training_samples = len(training_data)\n",
    "num_testing_samples = len(testing_data)\n",
    "\n",
    "#table\n",
    "summary_table = [[\"training\", num_training_samples], [\"testing\", num_testing_samples]]\n",
    "\n",
    "print(\"dataset summary:\")\n",
    "print(tabulate(summary_table, headers = [\"Dataset\", \"number of chunks\"], tablefmt=\"pretty\"))\n",
    "\n",
    "print(\"Training Activities Count\")\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "    print(f\"{activity}: {int(training_distribution[i])} chunks\")\n",
    "\n",
    "print(\"\\nTesting Activity Count\")\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "    print(f\"{activity}:{int(testing_distribution[i])} chunks\")\n",
    "print(len(training_data))\n",
    "print(len(testing_data))\n",
    "print(np.array(training_data).shape)\n",
    "print(len(training_data_raw))\n",
    "print(len(testing_data_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fae1da3e-3e2c-40ad-82cb-6d090f2cc7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all data for cross-validation (already done earlier as all_data)\n",
    "all_data = np.array(all_data)  # Convert to numpy array\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Normalize the entire dataset\n",
    "scaler = StandardScaler()\n",
    "X_all = scaler.fit_transform(all_data.reshape(-1, all_data.shape[-1])).reshape(all_data.shape)\n",
    "y_all = all_labels\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_all = torch.tensor(X_all, dtype=torch.float32)\n",
    "y_all = torch.tensor(y_all, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ec68f8d-85ff-48e1-b219-a547e9393c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedCNNModel(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=3, seq_length=int(CHUNK_SIZE * 100)):\n",
    "        super(OptimizedCNNModel, self).__init__()\n",
    "\n",
    "        def depthwise_separable_conv(in_channels, out_channels, kernel_size=3, padding=1):\n",
    "            \"\"\"Depthwise Separable Convolution for efficiency.\"\"\"\n",
    "            return nn.Sequential(\n",
    "                nn.Conv1d(in_channels, in_channels, kernel_size, groups=in_channels, padding=padding, bias=False),\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "                nn.GroupNorm(8, out_channels),  # More stable than BatchNorm for small batches\n",
    "                nn.SiLU()  # Swish activation (better than ReLU)\n",
    "            )\n",
    "\n",
    "        self.conv1 = depthwise_separable_conv(input_channels, 8)\n",
    "        self.conv2 = depthwise_separable_conv(8, 16)\n",
    "        self.conv3 = depthwise_separable_conv(16, 32)\n",
    "\n",
    "        # Adaptive pooling to ensure flexible sequence length compatibility\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        self.fc1 = nn.Linear(32, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, num_classes)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "        self.apply(self.initialize_weights)\n",
    "\n",
    "    def initialize_weights(self, m):\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "\n",
    "        # Global Average Pooling to reduce to fixed size\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = F.silu(self.fc1(x))  # Swish activation\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fea2d82-2721-4226-87c8-d0386cb60e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_PATH = \"best_model.pth\"\n",
    "BEST_METADATA_PATH = \"best_model.json\"\n",
    "\n",
    "def save_best_model(epoch, model, optimizer, test_loss, train_losses, test_losses):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'test_loss': test_loss,\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses\n",
    "    }, BEST_MODEL_PATH)\n",
    "\n",
    "    # Save metadata\n",
    "    with open(BEST_METADATA_PATH, \"w\") as f:\n",
    "        json.dump({\"epoch\": epoch, \"test_loss\": test_loss}, f)\n",
    "\n",
    "def load_best_model(model, optimizer):\n",
    "    if os.path.exists(BEST_MODEL_PATH) and os.path.exists(BEST_METADATA_PATH):\n",
    "        checkpoint = torch.load(BEST_MODEL_PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_loss = checkpoint['test_loss']\n",
    "        train_losses = checkpoint.get('train_losses', [])\n",
    "        test_losses = checkpoint.get('test_losses', [])\n",
    "\n",
    "        print(f\"Loaded best model from epoch {start_epoch} with Avg Loss: {best_loss:.4f}\")\n",
    "        return start_epoch, best_loss, train_losses, test_losses\n",
    "    return 0, float('inf'), 0.0, [], []  # Start fresh if no saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afb6cc02-f6d5-441e-8ea1-736efc276305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(loader, model, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # disable gradient calc\n",
    "        for data, targets in loader:\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # accuracy\n",
    "            _, predicted = torch.max(outputs, 1)  # get predicted class (max value)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc68f1f4-225f-455c-b6d0-80a76a3ce598",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OptimizedCNNModel(num_classes=NUM_CLASSES, input_channels=3, seq_length=int(CHUNK_SIZE * 100))\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Weight decay helps reduce overfitting\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-4)\n",
    "\n",
    "# Load best model if exists\n",
    "#start_epoch, best_loss, best_avg_accuracy, train_losses, test_losses = load_best_model(model, optimizer)\n",
    "start_epoch, best_loss, best_avg_accuracy, train_losses, test_losses =  0, float('inf'), 0.0, [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd628c08-376b-468a-ab5f-6c1a88c7587c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Fold 1 Epoch [1/300], Train Loss: 2.2035, Train Acc: 3.04%, Test Loss: 2.2774, Test Acc: 1.72%\n",
      "Fold 1 Epoch [2/300], Train Loss: 2.0547, Train Acc: 31.30%, Test Loss: 2.1149, Test Acc: 24.14%\n",
      "Fold 1 Epoch [3/300], Train Loss: 1.9242, Train Acc: 34.35%, Test Loss: 1.9783, Test Acc: 29.31%\n",
      "Fold 1 Epoch [4/300], Train Loss: 1.8258, Train Acc: 34.35%, Test Loss: 1.8621, Test Acc: 29.31%\n",
      "Fold 1 Epoch [5/300], Train Loss: 1.7308, Train Acc: 34.35%, Test Loss: 1.7648, Test Acc: 29.31%\n",
      "Fold 1 Epoch [6/300], Train Loss: 1.6602, Train Acc: 34.35%, Test Loss: 1.6816, Test Acc: 29.31%\n",
      "Fold 1 Epoch [7/300], Train Loss: 1.5842, Train Acc: 34.35%, Test Loss: 1.6102, Test Acc: 29.31%\n",
      "Fold 1 Epoch [8/300], Train Loss: 1.5283, Train Acc: 34.35%, Test Loss: 1.5446, Test Acc: 29.31%\n",
      "Fold 1 Epoch [9/300], Train Loss: 1.4701, Train Acc: 34.35%, Test Loss: 1.4859, Test Acc: 29.31%\n",
      "Fold 1 Epoch [10/300], Train Loss: 1.4211, Train Acc: 34.35%, Test Loss: 1.4330, Test Acc: 29.31%\n",
      "Fold 1 Epoch [11/300], Train Loss: 1.3784, Train Acc: 34.35%, Test Loss: 1.3870, Test Acc: 29.31%\n",
      "Fold 1 Epoch [12/300], Train Loss: 1.3377, Train Acc: 34.78%, Test Loss: 1.3438, Test Acc: 29.31%\n",
      "Fold 1 Epoch [13/300], Train Loss: 1.2973, Train Acc: 39.13%, Test Loss: 1.3049, Test Acc: 32.76%\n",
      "Fold 1 Epoch [14/300], Train Loss: 1.2667, Train Acc: 45.65%, Test Loss: 1.2681, Test Acc: 39.66%\n",
      "Fold 1 Epoch [15/300], Train Loss: 1.2353, Train Acc: 46.96%, Test Loss: 1.2369, Test Acc: 41.38%\n",
      "Fold 1 Epoch [16/300], Train Loss: 1.2039, Train Acc: 50.43%, Test Loss: 1.2072, Test Acc: 43.10%\n",
      "Fold 1 Epoch [17/300], Train Loss: 1.1754, Train Acc: 60.00%, Test Loss: 1.1781, Test Acc: 53.45%\n",
      "Fold 1 Epoch [18/300], Train Loss: 1.1447, Train Acc: 62.17%, Test Loss: 1.1505, Test Acc: 55.17%\n",
      "Fold 1 Epoch [19/300], Train Loss: 1.1129, Train Acc: 65.65%, Test Loss: 1.1209, Test Acc: 56.90%\n",
      "Fold 1 Epoch [20/300], Train Loss: 1.0760, Train Acc: 65.22%, Test Loss: 1.0900, Test Acc: 58.62%\n",
      "Fold 1 Epoch [21/300], Train Loss: 1.0427, Train Acc: 66.96%, Test Loss: 1.0550, Test Acc: 60.34%\n",
      "Fold 1 Epoch [22/300], Train Loss: 1.0076, Train Acc: 66.52%, Test Loss: 1.0190, Test Acc: 62.07%\n",
      "Fold 1 Epoch [23/300], Train Loss: 0.9651, Train Acc: 66.52%, Test Loss: 0.9822, Test Acc: 62.07%\n",
      "Fold 1 Epoch [24/300], Train Loss: 0.9255, Train Acc: 66.09%, Test Loss: 0.9478, Test Acc: 62.07%\n",
      "Fold 1 Epoch [25/300], Train Loss: 0.8921, Train Acc: 66.09%, Test Loss: 0.9125, Test Acc: 60.34%\n",
      "Fold 1 Epoch [26/300], Train Loss: 0.8599, Train Acc: 66.96%, Test Loss: 0.8803, Test Acc: 60.34%\n",
      "Fold 1 Epoch [27/300], Train Loss: 0.8123, Train Acc: 67.39%, Test Loss: 0.8476, Test Acc: 60.34%\n",
      "Fold 1 Epoch [28/300], Train Loss: 0.7814, Train Acc: 67.39%, Test Loss: 0.8149, Test Acc: 60.34%\n",
      "Fold 1 Epoch [29/300], Train Loss: 0.7419, Train Acc: 67.39%, Test Loss: 0.7838, Test Acc: 60.34%\n",
      "Fold 1 Epoch [30/300], Train Loss: 0.7121, Train Acc: 67.39%, Test Loss: 0.7571, Test Acc: 60.34%\n",
      "Fold 1 Epoch [31/300], Train Loss: 0.6875, Train Acc: 67.39%, Test Loss: 0.7330, Test Acc: 60.34%\n",
      "Fold 1 Epoch [32/300], Train Loss: 0.6655, Train Acc: 67.39%, Test Loss: 0.7130, Test Acc: 60.34%\n",
      "Fold 1 Epoch [33/300], Train Loss: 0.6513, Train Acc: 67.39%, Test Loss: 0.6955, Test Acc: 60.34%\n",
      "Fold 1 Epoch [34/300], Train Loss: 0.6209, Train Acc: 67.39%, Test Loss: 0.6789, Test Acc: 60.34%\n",
      "Fold 1 Epoch [35/300], Train Loss: 0.6035, Train Acc: 67.83%, Test Loss: 0.6648, Test Acc: 60.34%\n",
      "Fold 1 Epoch [36/300], Train Loss: 0.5862, Train Acc: 67.83%, Test Loss: 0.6512, Test Acc: 60.34%\n",
      "Fold 1 Epoch [37/300], Train Loss: 0.5785, Train Acc: 67.83%, Test Loss: 0.6408, Test Acc: 62.07%\n",
      "Fold 1 Epoch [38/300], Train Loss: 0.5697, Train Acc: 68.26%, Test Loss: 0.6332, Test Acc: 62.07%\n",
      "Fold 1 Epoch [39/300], Train Loss: 0.5461, Train Acc: 67.83%, Test Loss: 0.6240, Test Acc: 63.79%\n",
      "Fold 1 Epoch [40/300], Train Loss: 0.5494, Train Acc: 66.52%, Test Loss: 0.6146, Test Acc: 63.79%\n",
      "Fold 1 Epoch [41/300], Train Loss: 0.5366, Train Acc: 66.96%, Test Loss: 0.6108, Test Acc: 62.07%\n",
      "Fold 1 Epoch [42/300], Train Loss: 0.5356, Train Acc: 65.65%, Test Loss: 0.6063, Test Acc: 62.07%\n",
      "Fold 1 Epoch [43/300], Train Loss: 0.5273, Train Acc: 66.52%, Test Loss: 0.6049, Test Acc: 62.07%\n",
      "Fold 1 Epoch [44/300], Train Loss: 0.5262, Train Acc: 65.65%, Test Loss: 0.6024, Test Acc: 62.07%\n",
      "Fold 1 Epoch [45/300], Train Loss: 0.5087, Train Acc: 66.52%, Test Loss: 0.6020, Test Acc: 62.07%\n",
      "Fold 1 Epoch [46/300], Train Loss: 0.5033, Train Acc: 66.96%, Test Loss: 0.6015, Test Acc: 62.07%\n",
      "Fold 1 Epoch [47/300], Train Loss: 0.5068, Train Acc: 67.39%, Test Loss: 0.6008, Test Acc: 60.34%\n",
      "Fold 1 Epoch [48/300], Train Loss: 0.4980, Train Acc: 67.39%, Test Loss: 0.6021, Test Acc: 60.34%\n",
      "Fold 1 Epoch [49/300], Train Loss: 0.5035, Train Acc: 66.96%, Test Loss: 0.6051, Test Acc: 58.62%\n",
      "Fold 1 Epoch [50/300], Train Loss: 0.4948, Train Acc: 67.39%, Test Loss: 0.6056, Test Acc: 56.90%\n",
      "Fold 1 Epoch [51/300], Train Loss: 0.4859, Train Acc: 67.39%, Test Loss: 0.6017, Test Acc: 60.34%\n",
      "Fold 1 Epoch [52/300], Train Loss: 0.4800, Train Acc: 69.57%, Test Loss: 0.5967, Test Acc: 62.07%\n",
      "Fold 1 Epoch [53/300], Train Loss: 0.4882, Train Acc: 73.04%, Test Loss: 0.5934, Test Acc: 65.52%\n",
      "Fold 1 Epoch [54/300], Train Loss: 0.4788, Train Acc: 75.22%, Test Loss: 0.5922, Test Acc: 65.52%\n",
      "Fold 1 Epoch [55/300], Train Loss: 0.4839, Train Acc: 77.83%, Test Loss: 0.5901, Test Acc: 68.97%\n",
      "Fold 1 Epoch [56/300], Train Loss: 0.4637, Train Acc: 77.83%, Test Loss: 0.5889, Test Acc: 67.24%\n",
      "Fold 1 Epoch [57/300], Train Loss: 0.4699, Train Acc: 78.26%, Test Loss: 0.5910, Test Acc: 68.97%\n",
      "Fold 1 Epoch [58/300], Train Loss: 0.4704, Train Acc: 76.96%, Test Loss: 0.5955, Test Acc: 68.97%\n",
      "Fold 1 Epoch [59/300], Train Loss: 0.4730, Train Acc: 76.96%, Test Loss: 0.5997, Test Acc: 68.97%\n",
      "Fold 1 Epoch [60/300], Train Loss: 0.4713, Train Acc: 76.96%, Test Loss: 0.5991, Test Acc: 68.97%\n",
      "Fold 1 Epoch [61/300], Train Loss: 0.4705, Train Acc: 76.96%, Test Loss: 0.5975, Test Acc: 65.52%\n",
      "Fold 1 Epoch [62/300], Train Loss: 0.4591, Train Acc: 78.26%, Test Loss: 0.5974, Test Acc: 65.52%\n",
      "Fold 1 Epoch [63/300], Train Loss: 0.4606, Train Acc: 79.57%, Test Loss: 0.5977, Test Acc: 68.97%\n",
      "Fold 1 Epoch [64/300], Train Loss: 0.4525, Train Acc: 79.13%, Test Loss: 0.6009, Test Acc: 70.69%\n",
      "Fold 1 Epoch [65/300], Train Loss: 0.4564, Train Acc: 79.13%, Test Loss: 0.6089, Test Acc: 68.97%\n",
      "Fold 1 Epoch [66/300], Train Loss: 0.4652, Train Acc: 78.26%, Test Loss: 0.6149, Test Acc: 67.24%\n",
      "Fold 1 Epoch [67/300], Train Loss: 0.4555, Train Acc: 76.52%, Test Loss: 0.6139, Test Acc: 67.24%\n",
      "Fold 1 Epoch [68/300], Train Loss: 0.4569, Train Acc: 75.65%, Test Loss: 0.6127, Test Acc: 65.52%\n",
      "Fold 1 Epoch [69/300], Train Loss: 0.4516, Train Acc: 77.83%, Test Loss: 0.6123, Test Acc: 65.52%\n",
      "Fold 1 Epoch [70/300], Train Loss: 0.4484, Train Acc: 80.43%, Test Loss: 0.6126, Test Acc: 67.24%\n",
      "Fold 1 Epoch [71/300], Train Loss: 0.4503, Train Acc: 80.43%, Test Loss: 0.6134, Test Acc: 70.69%\n",
      "Fold 1 Epoch [72/300], Train Loss: 0.4477, Train Acc: 80.87%, Test Loss: 0.6123, Test Acc: 72.41%\n",
      "Fold 1 Epoch [73/300], Train Loss: 0.4471, Train Acc: 81.74%, Test Loss: 0.6149, Test Acc: 77.59%\n",
      "Fold 1 Epoch [74/300], Train Loss: 0.4405, Train Acc: 81.74%, Test Loss: 0.6225, Test Acc: 77.59%\n",
      "Fold 1 Epoch [75/300], Train Loss: 0.4455, Train Acc: 81.30%, Test Loss: 0.6219, Test Acc: 74.14%\n",
      "Fold 1 Epoch [76/300], Train Loss: 0.4439, Train Acc: 80.00%, Test Loss: 0.6206, Test Acc: 70.69%\n",
      "Fold 1 Epoch [77/300], Train Loss: 0.4382, Train Acc: 79.57%, Test Loss: 0.6228, Test Acc: 70.69%\n",
      "Fold 1 Epoch [78/300], Train Loss: 0.4379, Train Acc: 79.57%, Test Loss: 0.6238, Test Acc: 70.69%\n",
      "Fold 1 Epoch [79/300], Train Loss: 0.4395, Train Acc: 80.00%, Test Loss: 0.6249, Test Acc: 70.69%\n",
      "Fold 1 Epoch [80/300], Train Loss: 0.4308, Train Acc: 80.00%, Test Loss: 0.6279, Test Acc: 72.41%\n",
      "Fold 1 Epoch [81/300], Train Loss: 0.4295, Train Acc: 80.00%, Test Loss: 0.6302, Test Acc: 72.41%\n",
      "Fold 1 Epoch [82/300], Train Loss: 0.4345, Train Acc: 80.87%, Test Loss: 0.6291, Test Acc: 72.41%\n",
      "Fold 1 Epoch [83/300], Train Loss: 0.4412, Train Acc: 81.74%, Test Loss: 0.6279, Test Acc: 75.86%\n",
      "Fold 1 Epoch [84/300], Train Loss: 0.4260, Train Acc: 80.87%, Test Loss: 0.6299, Test Acc: 74.14%\n",
      "Fold 1 Epoch [85/300], Train Loss: 0.4319, Train Acc: 82.61%, Test Loss: 0.6270, Test Acc: 79.31%\n",
      "Fold 1 Epoch [86/300], Train Loss: 0.4266, Train Acc: 84.35%, Test Loss: 0.6237, Test Acc: 79.31%\n",
      "Fold 1 Epoch [87/300], Train Loss: 0.4255, Train Acc: 83.48%, Test Loss: 0.6265, Test Acc: 79.31%\n",
      "Fold 1 Epoch [88/300], Train Loss: 0.4158, Train Acc: 81.30%, Test Loss: 0.6325, Test Acc: 75.86%\n",
      "Fold 1 Epoch [89/300], Train Loss: 0.4223, Train Acc: 81.30%, Test Loss: 0.6351, Test Acc: 74.14%\n",
      "Fold 1 Epoch [90/300], Train Loss: 0.4278, Train Acc: 81.30%, Test Loss: 0.6352, Test Acc: 75.86%\n",
      "Fold 1 Epoch [91/300], Train Loss: 0.4169, Train Acc: 81.30%, Test Loss: 0.6338, Test Acc: 72.41%\n",
      "Fold 1 Epoch [92/300], Train Loss: 0.4249, Train Acc: 80.87%, Test Loss: 0.6354, Test Acc: 72.41%\n",
      "Fold 1 Epoch [93/300], Train Loss: 0.4206, Train Acc: 81.30%, Test Loss: 0.6338, Test Acc: 72.41%\n",
      "Fold 1 Epoch [94/300], Train Loss: 0.4082, Train Acc: 83.48%, Test Loss: 0.6315, Test Acc: 81.03%\n",
      "Fold 1 Epoch [95/300], Train Loss: 0.4184, Train Acc: 83.91%, Test Loss: 0.6332, Test Acc: 81.03%\n",
      "Fold 1 Epoch [96/300], Train Loss: 0.4107, Train Acc: 82.17%, Test Loss: 0.6361, Test Acc: 75.86%\n",
      "Fold 1 Epoch [97/300], Train Loss: 0.4184, Train Acc: 81.74%, Test Loss: 0.6399, Test Acc: 72.41%\n",
      "Fold 1 Epoch [98/300], Train Loss: 0.4170, Train Acc: 81.74%, Test Loss: 0.6423, Test Acc: 72.41%\n",
      "Fold 1 Epoch [99/300], Train Loss: 0.4154, Train Acc: 81.74%, Test Loss: 0.6438, Test Acc: 72.41%\n",
      "Fold 1 Epoch [100/300], Train Loss: 0.4028, Train Acc: 82.61%, Test Loss: 0.6392, Test Acc: 75.86%\n",
      "Fold 1 Epoch [101/300], Train Loss: 0.4103, Train Acc: 83.04%, Test Loss: 0.6343, Test Acc: 77.59%\n",
      "Fold 1 Epoch [102/300], Train Loss: 0.4184, Train Acc: 83.91%, Test Loss: 0.6355, Test Acc: 77.59%\n",
      "Fold 1 Epoch [103/300], Train Loss: 0.3984, Train Acc: 83.48%, Test Loss: 0.6341, Test Acc: 77.59%\n",
      "Fold 1 Epoch [104/300], Train Loss: 0.4009, Train Acc: 83.91%, Test Loss: 0.6315, Test Acc: 79.31%\n",
      "Fold 1 Epoch [105/300], Train Loss: 0.3977, Train Acc: 83.48%, Test Loss: 0.6364, Test Acc: 74.14%\n",
      "Fold 1 Epoch [106/300], Train Loss: 0.3938, Train Acc: 82.61%, Test Loss: 0.6413, Test Acc: 74.14%\n",
      "Fold 1 Epoch [107/300], Train Loss: 0.3965, Train Acc: 83.91%, Test Loss: 0.6405, Test Acc: 75.86%\n",
      "Fold 1 Epoch [108/300], Train Loss: 0.3912, Train Acc: 84.35%, Test Loss: 0.6345, Test Acc: 77.59%\n",
      "Fold 1 Epoch [109/300], Train Loss: 0.3879, Train Acc: 84.35%, Test Loss: 0.6313, Test Acc: 77.59%\n",
      "Fold 1 Epoch [110/300], Train Loss: 0.3800, Train Acc: 83.91%, Test Loss: 0.6313, Test Acc: 79.31%\n",
      "Fold 1 Epoch [111/300], Train Loss: 0.3899, Train Acc: 85.65%, Test Loss: 0.6247, Test Acc: 77.59%\n",
      "Fold 1 Epoch [112/300], Train Loss: 0.3771, Train Acc: 85.22%, Test Loss: 0.6242, Test Acc: 79.31%\n",
      "Fold 1 Epoch [113/300], Train Loss: 0.3743, Train Acc: 86.09%, Test Loss: 0.6228, Test Acc: 77.59%\n",
      "Fold 1 Epoch [114/300], Train Loss: 0.3746, Train Acc: 86.09%, Test Loss: 0.6221, Test Acc: 77.59%\n",
      "Fold 1 Epoch [115/300], Train Loss: 0.3786, Train Acc: 86.09%, Test Loss: 0.6250, Test Acc: 75.86%\n",
      "Fold 1 Epoch [116/300], Train Loss: 0.3721, Train Acc: 84.35%, Test Loss: 0.6296, Test Acc: 79.31%\n",
      "Fold 1 Epoch [117/300], Train Loss: 0.3727, Train Acc: 86.09%, Test Loss: 0.6226, Test Acc: 79.31%\n",
      "Fold 1 Epoch [118/300], Train Loss: 0.3677, Train Acc: 86.09%, Test Loss: 0.6077, Test Acc: 79.31%\n",
      "Fold 1 Epoch [119/300], Train Loss: 0.3602, Train Acc: 86.09%, Test Loss: 0.6091, Test Acc: 79.31%\n",
      "Fold 1 Epoch [120/300], Train Loss: 0.3630, Train Acc: 85.65%, Test Loss: 0.6161, Test Acc: 75.86%\n",
      "Fold 1 Epoch [121/300], Train Loss: 0.3671, Train Acc: 84.35%, Test Loss: 0.6294, Test Acc: 77.59%\n",
      "Fold 1 Epoch [122/300], Train Loss: 0.3731, Train Acc: 84.35%, Test Loss: 0.6352, Test Acc: 77.59%\n",
      "Fold 1 Epoch [123/300], Train Loss: 0.3488, Train Acc: 86.96%, Test Loss: 0.6273, Test Acc: 79.31%\n",
      "Fold 1 Epoch [124/300], Train Loss: 0.3485, Train Acc: 86.96%, Test Loss: 0.6135, Test Acc: 77.59%\n",
      "Fold 1 Epoch [125/300], Train Loss: 0.3526, Train Acc: 86.96%, Test Loss: 0.6108, Test Acc: 77.59%\n",
      "Fold 1 Epoch [126/300], Train Loss: 0.3470, Train Acc: 86.96%, Test Loss: 0.6091, Test Acc: 77.59%\n",
      "Fold 1 Epoch [127/300], Train Loss: 0.3457, Train Acc: 86.52%, Test Loss: 0.6192, Test Acc: 77.59%\n",
      "Fold 1 Epoch [128/300], Train Loss: 0.3327, Train Acc: 86.09%, Test Loss: 0.6213, Test Acc: 77.59%\n",
      "Fold 1 Epoch [129/300], Train Loss: 0.3366, Train Acc: 86.52%, Test Loss: 0.6114, Test Acc: 77.59%\n",
      "Fold 1 Epoch [130/300], Train Loss: 0.3424, Train Acc: 88.26%, Test Loss: 0.5979, Test Acc: 75.86%\n",
      "Fold 1 Epoch [131/300], Train Loss: 0.3405, Train Acc: 87.83%, Test Loss: 0.5909, Test Acc: 81.03%\n",
      "Fold 1 Epoch [132/300], Train Loss: 0.3300, Train Acc: 87.83%, Test Loss: 0.5974, Test Acc: 75.86%\n",
      "Fold 1 Epoch [133/300], Train Loss: 0.3392, Train Acc: 86.96%, Test Loss: 0.6018, Test Acc: 77.59%\n",
      "Fold 1 Epoch [134/300], Train Loss: 0.3312, Train Acc: 86.09%, Test Loss: 0.6066, Test Acc: 77.59%\n",
      "Fold 1 Epoch [135/300], Train Loss: 0.3298, Train Acc: 87.83%, Test Loss: 0.6013, Test Acc: 77.59%\n",
      "Fold 1 Epoch [136/300], Train Loss: 0.3349, Train Acc: 88.70%, Test Loss: 0.5857, Test Acc: 81.03%\n",
      "Fold 1 Epoch [137/300], Train Loss: 0.3274, Train Acc: 88.70%, Test Loss: 0.5940, Test Acc: 75.86%\n",
      "Fold 1 Epoch [138/300], Train Loss: 0.3252, Train Acc: 87.39%, Test Loss: 0.6033, Test Acc: 77.59%\n",
      "Fold 1 Epoch [139/300], Train Loss: 0.3269, Train Acc: 85.65%, Test Loss: 0.6225, Test Acc: 75.86%\n",
      "Fold 1 Epoch [140/300], Train Loss: 0.3219, Train Acc: 86.96%, Test Loss: 0.6101, Test Acc: 77.59%\n",
      "Fold 1 Epoch [141/300], Train Loss: 0.3074, Train Acc: 89.13%, Test Loss: 0.5858, Test Acc: 79.31%\n",
      "Fold 1 Epoch [142/300], Train Loss: 0.3104, Train Acc: 88.70%, Test Loss: 0.5795, Test Acc: 79.31%\n",
      "Fold 1 Epoch [143/300], Train Loss: 0.3082, Train Acc: 90.00%, Test Loss: 0.5888, Test Acc: 75.86%\n",
      "Fold 1 Epoch [144/300], Train Loss: 0.2978, Train Acc: 88.26%, Test Loss: 0.5931, Test Acc: 75.86%\n",
      "Fold 1 Epoch [145/300], Train Loss: 0.2996, Train Acc: 89.57%, Test Loss: 0.5752, Test Acc: 77.59%\n",
      "Fold 1 Epoch [146/300], Train Loss: 0.3057, Train Acc: 89.57%, Test Loss: 0.5662, Test Acc: 79.31%\n",
      "Fold 1 Epoch [147/300], Train Loss: 0.2948, Train Acc: 90.00%, Test Loss: 0.5674, Test Acc: 81.03%\n",
      "Fold 1 Epoch [148/300], Train Loss: 0.2946, Train Acc: 86.52%, Test Loss: 0.6006, Test Acc: 77.59%\n",
      "Fold 1 Epoch [149/300], Train Loss: 0.2882, Train Acc: 88.26%, Test Loss: 0.5933, Test Acc: 77.59%\n",
      "Fold 1 Epoch [150/300], Train Loss: 0.2785, Train Acc: 89.57%, Test Loss: 0.5721, Test Acc: 79.31%\n",
      "Fold 1 Epoch [151/300], Train Loss: 0.2838, Train Acc: 89.57%, Test Loss: 0.5813, Test Acc: 77.59%\n",
      "Fold 1 Epoch [152/300], Train Loss: 0.2737, Train Acc: 88.26%, Test Loss: 0.5850, Test Acc: 77.59%\n",
      "Fold 1 Epoch [153/300], Train Loss: 0.2808, Train Acc: 89.57%, Test Loss: 0.5564, Test Acc: 82.76%\n",
      "Fold 1 Epoch [154/300], Train Loss: 0.2778, Train Acc: 89.13%, Test Loss: 0.5683, Test Acc: 82.76%\n",
      "Fold 1 Epoch [155/300], Train Loss: 0.2716, Train Acc: 90.00%, Test Loss: 0.5735, Test Acc: 81.03%\n",
      "Fold 1 Epoch [156/300], Train Loss: 0.2654, Train Acc: 90.43%, Test Loss: 0.5619, Test Acc: 81.03%\n",
      "Fold 1 Epoch [157/300], Train Loss: 0.2602, Train Acc: 90.87%, Test Loss: 0.5640, Test Acc: 81.03%\n",
      "Fold 1 Epoch [158/300], Train Loss: 0.2657, Train Acc: 87.83%, Test Loss: 0.5970, Test Acc: 77.59%\n",
      "Fold 1 Epoch [159/300], Train Loss: 0.2544, Train Acc: 91.30%, Test Loss: 0.5677, Test Acc: 81.03%\n",
      "Fold 1 Epoch [160/300], Train Loss: 0.2460, Train Acc: 90.87%, Test Loss: 0.5479, Test Acc: 81.03%\n",
      "Fold 1 Epoch [161/300], Train Loss: 0.2543, Train Acc: 91.74%, Test Loss: 0.5704, Test Acc: 79.31%\n",
      "Fold 1 Epoch [162/300], Train Loss: 0.2489, Train Acc: 91.74%, Test Loss: 0.5686, Test Acc: 79.31%\n",
      "Fold 1 Epoch [163/300], Train Loss: 0.2330, Train Acc: 90.87%, Test Loss: 0.5389, Test Acc: 79.31%\n",
      "Fold 1 Epoch [164/300], Train Loss: 0.2409, Train Acc: 90.43%, Test Loss: 0.5627, Test Acc: 81.03%\n",
      "Fold 1 Epoch [165/300], Train Loss: 0.2316, Train Acc: 91.74%, Test Loss: 0.5733, Test Acc: 79.31%\n",
      "Fold 1 Epoch [166/300], Train Loss: 0.2297, Train Acc: 91.30%, Test Loss: 0.5425, Test Acc: 79.31%\n",
      "Fold 1 Epoch [167/300], Train Loss: 0.2235, Train Acc: 92.17%, Test Loss: 0.5452, Test Acc: 79.31%\n",
      "Fold 1 Epoch [168/300], Train Loss: 0.2238, Train Acc: 92.61%, Test Loss: 0.5701, Test Acc: 81.03%\n",
      "Fold 1 Epoch [169/300], Train Loss: 0.2123, Train Acc: 92.61%, Test Loss: 0.5673, Test Acc: 81.03%\n",
      "Fold 1 Epoch [170/300], Train Loss: 0.2195, Train Acc: 92.17%, Test Loss: 0.5417, Test Acc: 79.31%\n",
      "Fold 1 Epoch [171/300], Train Loss: 0.2223, Train Acc: 94.78%, Test Loss: 0.5635, Test Acc: 84.48%\n",
      "Fold 1 Epoch [172/300], Train Loss: 0.2113, Train Acc: 95.22%, Test Loss: 0.5630, Test Acc: 86.21%\n",
      "Fold 1 Epoch [173/300], Train Loss: 0.2060, Train Acc: 93.48%, Test Loss: 0.5348, Test Acc: 81.03%\n",
      "Fold 1 Epoch [174/300], Train Loss: 0.2096, Train Acc: 93.48%, Test Loss: 0.5336, Test Acc: 82.76%\n",
      "Fold 1 Epoch [175/300], Train Loss: 0.2012, Train Acc: 95.65%, Test Loss: 0.5565, Test Acc: 84.48%\n",
      "Fold 1 Epoch [176/300], Train Loss: 0.1975, Train Acc: 95.65%, Test Loss: 0.5705, Test Acc: 81.03%\n",
      "Fold 1 Epoch [177/300], Train Loss: 0.2006, Train Acc: 96.09%, Test Loss: 0.5763, Test Acc: 84.48%\n",
      "Fold 1 Epoch [178/300], Train Loss: 0.1951, Train Acc: 93.48%, Test Loss: 0.5663, Test Acc: 81.03%\n",
      "Fold 1 Epoch [179/300], Train Loss: 0.1875, Train Acc: 96.52%, Test Loss: 0.5884, Test Acc: 84.48%\n",
      "Fold 1 Epoch [180/300], Train Loss: 0.1952, Train Acc: 96.09%, Test Loss: 0.5732, Test Acc: 84.48%\n",
      "Fold 1 Epoch [181/300], Train Loss: 0.1861, Train Acc: 93.91%, Test Loss: 0.5427, Test Acc: 82.76%\n",
      "Fold 1 Epoch [182/300], Train Loss: 0.1805, Train Acc: 96.09%, Test Loss: 0.5706, Test Acc: 84.48%\n",
      "Fold 1 Epoch [183/300], Train Loss: 0.1750, Train Acc: 95.22%, Test Loss: 0.5681, Test Acc: 84.48%\n",
      "Fold 1 Epoch [184/300], Train Loss: 0.1799, Train Acc: 95.22%, Test Loss: 0.5508, Test Acc: 82.76%\n",
      "Fold 1 Epoch [185/300], Train Loss: 0.1678, Train Acc: 96.52%, Test Loss: 0.5842, Test Acc: 84.48%\n",
      "Fold 1 Epoch [186/300], Train Loss: 0.1703, Train Acc: 96.96%, Test Loss: 0.5864, Test Acc: 86.21%\n",
      "Fold 1 Epoch [187/300], Train Loss: 0.1752, Train Acc: 93.04%, Test Loss: 0.5624, Test Acc: 82.76%\n",
      "Fold 1 Epoch [188/300], Train Loss: 0.1618, Train Acc: 96.52%, Test Loss: 0.5618, Test Acc: 84.48%\n",
      "Fold 1 Epoch [189/300], Train Loss: 0.1599, Train Acc: 96.09%, Test Loss: 0.5900, Test Acc: 82.76%\n",
      "Fold 1 Epoch [190/300], Train Loss: 0.1637, Train Acc: 95.65%, Test Loss: 0.5624, Test Acc: 82.76%\n",
      "Fold 1 Epoch [191/300], Train Loss: 0.1573, Train Acc: 96.09%, Test Loss: 0.5769, Test Acc: 82.76%\n",
      "Fold 1 Epoch [192/300], Train Loss: 0.1487, Train Acc: 96.09%, Test Loss: 0.5947, Test Acc: 82.76%\n",
      "Fold 1 Epoch [193/300], Train Loss: 0.1549, Train Acc: 96.52%, Test Loss: 0.5854, Test Acc: 84.48%\n",
      "Fold 1 Epoch [194/300], Train Loss: 0.1513, Train Acc: 95.65%, Test Loss: 0.6335, Test Acc: 82.76%\n",
      "Fold 1 Epoch [195/300], Train Loss: 0.1505, Train Acc: 96.52%, Test Loss: 0.5681, Test Acc: 82.76%\n",
      "Fold 1 Epoch [196/300], Train Loss: 0.1470, Train Acc: 95.65%, Test Loss: 0.5652, Test Acc: 81.03%\n",
      "Fold 1 Epoch [197/300], Train Loss: 0.1609, Train Acc: 95.22%, Test Loss: 0.6297, Test Acc: 77.59%\n",
      "Fold 1 Epoch [198/300], Train Loss: 0.1330, Train Acc: 97.39%, Test Loss: 0.5831, Test Acc: 81.03%\n",
      "Fold 1 Epoch [199/300], Train Loss: 0.1502, Train Acc: 93.48%, Test Loss: 0.5661, Test Acc: 82.76%\n",
      "Fold 1 Epoch [200/300], Train Loss: 0.1356, Train Acc: 96.52%, Test Loss: 0.6241, Test Acc: 82.76%\n",
      "Fold 1 Epoch [201/300], Train Loss: 0.1382, Train Acc: 96.52%, Test Loss: 0.6296, Test Acc: 81.03%\n",
      "Fold 1 Epoch [202/300], Train Loss: 0.1434, Train Acc: 96.52%, Test Loss: 0.5970, Test Acc: 81.03%\n",
      "Fold 1 Epoch [203/300], Train Loss: 0.1295, Train Acc: 96.96%, Test Loss: 0.5930, Test Acc: 82.76%\n",
      "Fold 1 Epoch [204/300], Train Loss: 0.1285, Train Acc: 97.39%, Test Loss: 0.6046, Test Acc: 84.48%\n",
      "Fold 1 Epoch [205/300], Train Loss: 0.1294, Train Acc: 96.52%, Test Loss: 0.6308, Test Acc: 79.31%\n",
      "Fold 1 Epoch [206/300], Train Loss: 0.1225, Train Acc: 96.52%, Test Loss: 0.6204, Test Acc: 84.48%\n",
      "Fold 1 Epoch [207/300], Train Loss: 0.1284, Train Acc: 96.52%, Test Loss: 0.5851, Test Acc: 81.03%\n",
      "Fold 1 Epoch [208/300], Train Loss: 0.1201, Train Acc: 97.83%, Test Loss: 0.6117, Test Acc: 81.03%\n",
      "Fold 1 Epoch [209/300], Train Loss: 0.1195, Train Acc: 97.83%, Test Loss: 0.6171, Test Acc: 82.76%\n",
      "Fold 1 Epoch [210/300], Train Loss: 0.1168, Train Acc: 97.39%, Test Loss: 0.6205, Test Acc: 84.48%\n",
      "Fold 1 Epoch [211/300], Train Loss: 0.1162, Train Acc: 96.09%, Test Loss: 0.6133, Test Acc: 82.76%\n",
      "Fold 1 Epoch [212/300], Train Loss: 0.1152, Train Acc: 98.26%, Test Loss: 0.6396, Test Acc: 82.76%\n",
      "Fold 1 Epoch [213/300], Train Loss: 0.1156, Train Acc: 97.83%, Test Loss: 0.6291, Test Acc: 81.03%\n",
      "Fold 1 Epoch [214/300], Train Loss: 0.1125, Train Acc: 96.52%, Test Loss: 0.5998, Test Acc: 82.76%\n",
      "Fold 1 Epoch [215/300], Train Loss: 0.1221, Train Acc: 97.83%, Test Loss: 0.6269, Test Acc: 79.31%\n",
      "Fold 1 Epoch [216/300], Train Loss: 0.1099, Train Acc: 97.83%, Test Loss: 0.6302, Test Acc: 81.03%\n",
      "Fold 1 Epoch [217/300], Train Loss: 0.1068, Train Acc: 97.39%, Test Loss: 0.6251, Test Acc: 81.03%\n",
      "Fold 1 Epoch [218/300], Train Loss: 0.1037, Train Acc: 97.39%, Test Loss: 0.6271, Test Acc: 82.76%\n",
      "Fold 1 Epoch [219/300], Train Loss: 0.1050, Train Acc: 96.52%, Test Loss: 0.6212, Test Acc: 82.76%\n",
      "Fold 1 Epoch [220/300], Train Loss: 0.1088, Train Acc: 97.83%, Test Loss: 0.6158, Test Acc: 82.76%\n",
      "Fold 1 Epoch [221/300], Train Loss: 0.1023, Train Acc: 98.26%, Test Loss: 0.6237, Test Acc: 82.76%\n",
      "Fold 1 Epoch [222/300], Train Loss: 0.1113, Train Acc: 96.96%, Test Loss: 0.6062, Test Acc: 81.03%\n",
      "Fold 1 Epoch [223/300], Train Loss: 0.1046, Train Acc: 97.83%, Test Loss: 0.6491, Test Acc: 81.03%\n",
      "Fold 1 Epoch [224/300], Train Loss: 0.0968, Train Acc: 97.39%, Test Loss: 0.6199, Test Acc: 81.03%\n",
      "Fold 1 Epoch [225/300], Train Loss: 0.0964, Train Acc: 97.39%, Test Loss: 0.6391, Test Acc: 81.03%\n",
      "Fold 1 Epoch [226/300], Train Loss: 0.0985, Train Acc: 97.39%, Test Loss: 0.6316, Test Acc: 82.76%\n",
      "Fold 1 Epoch [227/300], Train Loss: 0.0885, Train Acc: 97.83%, Test Loss: 0.6335, Test Acc: 82.76%\n",
      "Fold 1 Epoch [228/300], Train Loss: 0.0891, Train Acc: 98.26%, Test Loss: 0.6289, Test Acc: 82.76%\n",
      "Fold 1 Epoch [229/300], Train Loss: 0.0972, Train Acc: 97.39%, Test Loss: 0.6096, Test Acc: 81.03%\n",
      "Fold 1 Epoch [230/300], Train Loss: 0.0932, Train Acc: 98.26%, Test Loss: 0.6348, Test Acc: 82.76%\n",
      "Fold 1 Epoch [231/300], Train Loss: 0.0885, Train Acc: 98.26%, Test Loss: 0.6646, Test Acc: 81.03%\n",
      "Fold 1 Epoch [232/300], Train Loss: 0.0914, Train Acc: 96.52%, Test Loss: 0.6277, Test Acc: 81.03%\n",
      "Fold 1 Epoch [233/300], Train Loss: 0.0881, Train Acc: 98.26%, Test Loss: 0.6562, Test Acc: 79.31%\n",
      "Fold 1 Epoch [234/300], Train Loss: 0.0816, Train Acc: 98.70%, Test Loss: 0.6433, Test Acc: 81.03%\n",
      "Fold 1 Epoch [235/300], Train Loss: 0.0797, Train Acc: 98.70%, Test Loss: 0.6385, Test Acc: 81.03%\n",
      "Fold 1 Epoch [236/300], Train Loss: 0.0816, Train Acc: 98.70%, Test Loss: 0.6532, Test Acc: 81.03%\n",
      "Fold 1 Epoch [237/300], Train Loss: 0.0779, Train Acc: 98.70%, Test Loss: 0.6514, Test Acc: 82.76%\n",
      "Fold 1 Epoch [238/300], Train Loss: 0.0781, Train Acc: 98.70%, Test Loss: 0.6401, Test Acc: 81.03%\n",
      "Fold 1 Epoch [239/300], Train Loss: 0.0779, Train Acc: 98.26%, Test Loss: 0.6644, Test Acc: 82.76%\n",
      "Fold 1 Epoch [240/300], Train Loss: 0.0816, Train Acc: 98.70%, Test Loss: 0.6555, Test Acc: 82.76%\n",
      "Fold 1 Epoch [241/300], Train Loss: 0.0851, Train Acc: 96.96%, Test Loss: 0.6506, Test Acc: 82.76%\n",
      "Fold 1 Epoch [242/300], Train Loss: 0.0830, Train Acc: 98.70%, Test Loss: 0.6593, Test Acc: 81.03%\n",
      "Fold 1 Epoch [243/300], Train Loss: 0.0815, Train Acc: 98.26%, Test Loss: 0.6885, Test Acc: 81.03%\n",
      "Fold 1 Epoch [244/300], Train Loss: 0.0814, Train Acc: 98.26%, Test Loss: 0.6527, Test Acc: 79.31%\n",
      "Fold 1 Epoch [245/300], Train Loss: 0.0788, Train Acc: 98.70%, Test Loss: 0.7058, Test Acc: 81.03%\n",
      "Fold 1 Epoch [246/300], Train Loss: 0.0766, Train Acc: 98.70%, Test Loss: 0.7193, Test Acc: 79.31%\n",
      "Fold 1 Epoch [247/300], Train Loss: 0.0757, Train Acc: 97.39%, Test Loss: 0.6701, Test Acc: 84.48%\n",
      "Fold 1 Epoch [248/300], Train Loss: 0.0703, Train Acc: 98.70%, Test Loss: 0.7111, Test Acc: 79.31%\n",
      "Fold 1 Epoch [249/300], Train Loss: 0.0719, Train Acc: 98.70%, Test Loss: 0.6817, Test Acc: 82.76%\n",
      "Fold 1 Epoch [250/300], Train Loss: 0.0661, Train Acc: 98.70%, Test Loss: 0.6824, Test Acc: 82.76%\n",
      "Fold 1 Epoch [251/300], Train Loss: 0.0743, Train Acc: 98.70%, Test Loss: 0.7075, Test Acc: 79.31%\n",
      "Fold 1 Epoch [252/300], Train Loss: 0.0711, Train Acc: 98.26%, Test Loss: 0.6702, Test Acc: 84.48%\n",
      "Fold 1 Epoch [253/300], Train Loss: 0.0671, Train Acc: 98.70%, Test Loss: 0.6726, Test Acc: 82.76%\n",
      "Fold 1 Epoch [254/300], Train Loss: 0.0666, Train Acc: 98.70%, Test Loss: 0.6768, Test Acc: 79.31%\n",
      "Fold 1 Epoch [255/300], Train Loss: 0.0639, Train Acc: 99.13%, Test Loss: 0.6614, Test Acc: 82.76%\n",
      "Fold 1 Epoch [256/300], Train Loss: 0.0629, Train Acc: 98.70%, Test Loss: 0.6803, Test Acc: 79.31%\n",
      "Fold 1 Epoch [257/300], Train Loss: 0.0652, Train Acc: 98.70%, Test Loss: 0.6830, Test Acc: 79.31%\n",
      "Fold 1 Epoch [258/300], Train Loss: 0.0624, Train Acc: 98.70%, Test Loss: 0.6808, Test Acc: 82.76%\n",
      "Fold 1 Epoch [259/300], Train Loss: 0.0703, Train Acc: 98.70%, Test Loss: 0.7242, Test Acc: 79.31%\n",
      "Fold 1 Epoch [260/300], Train Loss: 0.0653, Train Acc: 98.70%, Test Loss: 0.6693, Test Acc: 82.76%\n",
      "Fold 1 Epoch [261/300], Train Loss: 0.0707, Train Acc: 98.70%, Test Loss: 0.6884, Test Acc: 79.31%\n",
      "Fold 1 Epoch [262/300], Train Loss: 0.0629, Train Acc: 99.13%, Test Loss: 0.6646, Test Acc: 81.03%\n",
      "Fold 1 Epoch [263/300], Train Loss: 0.0680, Train Acc: 99.13%, Test Loss: 0.6591, Test Acc: 82.76%\n",
      "Fold 1 Epoch [264/300], Train Loss: 0.0675, Train Acc: 98.70%, Test Loss: 0.7077, Test Acc: 81.03%\n",
      "Fold 1 Epoch [265/300], Train Loss: 0.0648, Train Acc: 99.13%, Test Loss: 0.6978, Test Acc: 82.76%\n",
      "Fold 1 Epoch [266/300], Train Loss: 0.0687, Train Acc: 99.13%, Test Loss: 0.7202, Test Acc: 82.76%\n",
      "Fold 1 Epoch [267/300], Train Loss: 0.0645, Train Acc: 98.70%, Test Loss: 0.7028, Test Acc: 79.31%\n",
      "Fold 1 Epoch [268/300], Train Loss: 0.0719, Train Acc: 98.26%, Test Loss: 0.6597, Test Acc: 82.76%\n",
      "Fold 1 Epoch [269/300], Train Loss: 0.0686, Train Acc: 98.70%, Test Loss: 0.6856, Test Acc: 81.03%\n",
      "Fold 1 Epoch [270/300], Train Loss: 0.0593, Train Acc: 98.70%, Test Loss: 0.7004, Test Acc: 81.03%\n",
      "Fold 1 Epoch [271/300], Train Loss: 0.0662, Train Acc: 99.13%, Test Loss: 0.6863, Test Acc: 84.48%\n",
      "Fold 1 Epoch [272/300], Train Loss: 0.0643, Train Acc: 98.70%, Test Loss: 0.7274, Test Acc: 81.03%\n",
      "Fold 1 Epoch [273/300], Train Loss: 0.0554, Train Acc: 99.13%, Test Loss: 0.7009, Test Acc: 84.48%\n",
      "Fold 1 Epoch [274/300], Train Loss: 0.0565, Train Acc: 99.13%, Test Loss: 0.6987, Test Acc: 84.48%\n",
      "Fold 1 Epoch [275/300], Train Loss: 0.0560, Train Acc: 99.13%, Test Loss: 0.7091, Test Acc: 81.03%\n",
      "Fold 1 Epoch [276/300], Train Loss: 0.0578, Train Acc: 99.13%, Test Loss: 0.6835, Test Acc: 84.48%\n",
      "Fold 1 Epoch [277/300], Train Loss: 0.0520, Train Acc: 99.13%, Test Loss: 0.6815, Test Acc: 82.76%\n",
      "Fold 1 Epoch [278/300], Train Loss: 0.0545, Train Acc: 98.70%, Test Loss: 0.7123, Test Acc: 79.31%\n",
      "Fold 1 Epoch [279/300], Train Loss: 0.0491, Train Acc: 98.70%, Test Loss: 0.6903, Test Acc: 81.03%\n",
      "Fold 1 Epoch [280/300], Train Loss: 0.0516, Train Acc: 98.70%, Test Loss: 0.7121, Test Acc: 79.31%\n",
      "Fold 1 Epoch [281/300], Train Loss: 0.0542, Train Acc: 99.13%, Test Loss: 0.7010, Test Acc: 84.48%\n",
      "Fold 1 Epoch [282/300], Train Loss: 0.0485, Train Acc: 99.13%, Test Loss: 0.7101, Test Acc: 84.48%\n",
      "Fold 1 Epoch [283/300], Train Loss: 0.0543, Train Acc: 98.70%, Test Loss: 0.7557, Test Acc: 79.31%\n",
      "Fold 1 Epoch [284/300], Train Loss: 0.0464, Train Acc: 99.13%, Test Loss: 0.7093, Test Acc: 84.48%\n",
      "Fold 1 Epoch [285/300], Train Loss: 0.0577, Train Acc: 99.13%, Test Loss: 0.6969, Test Acc: 81.03%\n",
      "Fold 1 Epoch [286/300], Train Loss: 0.0499, Train Acc: 98.70%, Test Loss: 0.7142, Test Acc: 81.03%\n",
      "Fold 1 Epoch [287/300], Train Loss: 0.0470, Train Acc: 98.70%, Test Loss: 0.7230, Test Acc: 81.03%\n",
      "Fold 1 Epoch [288/300], Train Loss: 0.0576, Train Acc: 99.13%, Test Loss: 0.6971, Test Acc: 82.76%\n",
      "Fold 1 Epoch [289/300], Train Loss: 0.0473, Train Acc: 99.13%, Test Loss: 0.7225, Test Acc: 79.31%\n",
      "Fold 1 Epoch [290/300], Train Loss: 0.0478, Train Acc: 98.70%, Test Loss: 0.7380, Test Acc: 79.31%\n",
      "Fold 1 Epoch [291/300], Train Loss: 0.0520, Train Acc: 98.70%, Test Loss: 0.7081, Test Acc: 81.03%\n",
      "Fold 1 Epoch [292/300], Train Loss: 0.0475, Train Acc: 98.70%, Test Loss: 0.7227, Test Acc: 81.03%\n",
      "Fold 1 Epoch [293/300], Train Loss: 0.0457, Train Acc: 99.57%, Test Loss: 0.6980, Test Acc: 82.76%\n",
      "Fold 1 Epoch [294/300], Train Loss: 0.0453, Train Acc: 99.13%, Test Loss: 0.7034, Test Acc: 82.76%\n",
      "Fold 1 Epoch [295/300], Train Loss: 0.0478, Train Acc: 99.57%, Test Loss: 0.7133, Test Acc: 84.48%\n",
      "Fold 1 Epoch [296/300], Train Loss: 0.0422, Train Acc: 99.57%, Test Loss: 0.7211, Test Acc: 84.48%\n",
      "Fold 1 Epoch [297/300], Train Loss: 0.0455, Train Acc: 99.13%, Test Loss: 0.7282, Test Acc: 79.31%\n",
      "Fold 1 Epoch [298/300], Train Loss: 0.0423, Train Acc: 99.57%, Test Loss: 0.7132, Test Acc: 79.31%\n",
      "Fold 1 Epoch [299/300], Train Loss: 0.0466, Train Acc: 99.57%, Test Loss: 0.6983, Test Acc: 81.03%\n",
      "Fold 1 Epoch [300/300], Train Loss: 0.0412, Train Acc: 98.70%, Test Loss: 0.6977, Test Acc: 81.03%\n",
      "Fold 1 Best Accuracy: 86.21%\n",
      "Fold 2/5\n",
      "Fold 2 Epoch [1/300], Train Loss: 2.0527, Train Acc: 0.00%, Test Loss: 2.0696, Test Acc: 0.00%\n",
      "Fold 2 Epoch [2/300], Train Loss: 1.9751, Train Acc: 26.52%, Test Loss: 1.9944, Test Acc: 17.24%\n",
      "Fold 2 Epoch [3/300], Train Loss: 1.9021, Train Acc: 34.78%, Test Loss: 1.9236, Test Acc: 27.59%\n",
      "Fold 2 Epoch [4/300], Train Loss: 1.8323, Train Acc: 34.78%, Test Loss: 1.8563, Test Acc: 27.59%\n",
      "Fold 2 Epoch [5/300], Train Loss: 1.7695, Train Acc: 34.78%, Test Loss: 1.7916, Test Acc: 27.59%\n",
      "Fold 2 Epoch [6/300], Train Loss: 1.6981, Train Acc: 34.78%, Test Loss: 1.7277, Test Acc: 27.59%\n",
      "Fold 2 Epoch [7/300], Train Loss: 1.6381, Train Acc: 34.78%, Test Loss: 1.6652, Test Acc: 27.59%\n",
      "Fold 2 Epoch [8/300], Train Loss: 1.5723, Train Acc: 34.78%, Test Loss: 1.6045, Test Acc: 27.59%\n",
      "Fold 2 Epoch [9/300], Train Loss: 1.5105, Train Acc: 34.78%, Test Loss: 1.5459, Test Acc: 27.59%\n",
      "Fold 2 Epoch [10/300], Train Loss: 1.4615, Train Acc: 34.78%, Test Loss: 1.4901, Test Acc: 27.59%\n",
      "Fold 2 Epoch [11/300], Train Loss: 1.4044, Train Acc: 34.78%, Test Loss: 1.4380, Test Acc: 27.59%\n",
      "Fold 2 Epoch [12/300], Train Loss: 1.3632, Train Acc: 34.78%, Test Loss: 1.3881, Test Acc: 27.59%\n",
      "Fold 2 Epoch [13/300], Train Loss: 1.3136, Train Acc: 34.78%, Test Loss: 1.3399, Test Acc: 27.59%\n",
      "Fold 2 Epoch [14/300], Train Loss: 1.2737, Train Acc: 34.78%, Test Loss: 1.2939, Test Acc: 27.59%\n",
      "Fold 2 Epoch [15/300], Train Loss: 1.2290, Train Acc: 36.96%, Test Loss: 1.2523, Test Acc: 31.03%\n",
      "Fold 2 Epoch [16/300], Train Loss: 1.2016, Train Acc: 43.04%, Test Loss: 1.2166, Test Acc: 37.93%\n",
      "Fold 2 Epoch [17/300], Train Loss: 1.1730, Train Acc: 49.13%, Test Loss: 1.1864, Test Acc: 53.45%\n",
      "Fold 2 Epoch [18/300], Train Loss: 1.1467, Train Acc: 63.48%, Test Loss: 1.1596, Test Acc: 62.07%\n",
      "Fold 2 Epoch [19/300], Train Loss: 1.1251, Train Acc: 66.52%, Test Loss: 1.1371, Test Acc: 72.41%\n",
      "Fold 2 Epoch [20/300], Train Loss: 1.1040, Train Acc: 68.70%, Test Loss: 1.1144, Test Acc: 72.41%\n",
      "Fold 2 Epoch [21/300], Train Loss: 1.0781, Train Acc: 69.13%, Test Loss: 1.0874, Test Acc: 74.14%\n",
      "Fold 2 Epoch [22/300], Train Loss: 1.0558, Train Acc: 69.57%, Test Loss: 1.0619, Test Acc: 68.97%\n",
      "Fold 2 Epoch [23/300], Train Loss: 1.0281, Train Acc: 70.00%, Test Loss: 1.0381, Test Acc: 68.97%\n",
      "Fold 2 Epoch [24/300], Train Loss: 1.0062, Train Acc: 70.00%, Test Loss: 1.0162, Test Acc: 70.69%\n",
      "Fold 2 Epoch [25/300], Train Loss: 0.9813, Train Acc: 69.13%, Test Loss: 0.9901, Test Acc: 67.24%\n",
      "Fold 2 Epoch [26/300], Train Loss: 0.9527, Train Acc: 70.00%, Test Loss: 0.9634, Test Acc: 67.24%\n",
      "Fold 2 Epoch [27/300], Train Loss: 0.9239, Train Acc: 69.57%, Test Loss: 0.9374, Test Acc: 67.24%\n",
      "Fold 2 Epoch [28/300], Train Loss: 0.8987, Train Acc: 69.57%, Test Loss: 0.9081, Test Acc: 67.24%\n",
      "Fold 2 Epoch [29/300], Train Loss: 0.8743, Train Acc: 69.13%, Test Loss: 0.8802, Test Acc: 67.24%\n",
      "Fold 2 Epoch [30/300], Train Loss: 0.8466, Train Acc: 72.17%, Test Loss: 0.8558, Test Acc: 68.97%\n",
      "Fold 2 Epoch [31/300], Train Loss: 0.8197, Train Acc: 72.61%, Test Loss: 0.8346, Test Acc: 70.69%\n",
      "Fold 2 Epoch [32/300], Train Loss: 0.7968, Train Acc: 75.22%, Test Loss: 0.8135, Test Acc: 72.41%\n",
      "Fold 2 Epoch [33/300], Train Loss: 0.7714, Train Acc: 75.22%, Test Loss: 0.7905, Test Acc: 72.41%\n",
      "Fold 2 Epoch [34/300], Train Loss: 0.7453, Train Acc: 74.78%, Test Loss: 0.7688, Test Acc: 68.97%\n",
      "Fold 2 Epoch [35/300], Train Loss: 0.7276, Train Acc: 74.78%, Test Loss: 0.7490, Test Acc: 70.69%\n",
      "Fold 2 Epoch [36/300], Train Loss: 0.7124, Train Acc: 76.96%, Test Loss: 0.7334, Test Acc: 68.97%\n",
      "Fold 2 Epoch [37/300], Train Loss: 0.6949, Train Acc: 75.22%, Test Loss: 0.7191, Test Acc: 70.69%\n",
      "Fold 2 Epoch [38/300], Train Loss: 0.6730, Train Acc: 75.22%, Test Loss: 0.7052, Test Acc: 68.97%\n",
      "Fold 2 Epoch [39/300], Train Loss: 0.6556, Train Acc: 75.65%, Test Loss: 0.6887, Test Acc: 68.97%\n",
      "Fold 2 Epoch [40/300], Train Loss: 0.6269, Train Acc: 74.35%, Test Loss: 0.6740, Test Acc: 67.24%\n",
      "Fold 2 Epoch [41/300], Train Loss: 0.6160, Train Acc: 76.96%, Test Loss: 0.6576, Test Acc: 70.69%\n",
      "Fold 2 Epoch [42/300], Train Loss: 0.6095, Train Acc: 76.52%, Test Loss: 0.6464, Test Acc: 70.69%\n",
      "Fold 2 Epoch [43/300], Train Loss: 0.5887, Train Acc: 76.09%, Test Loss: 0.6402, Test Acc: 70.69%\n",
      "Fold 2 Epoch [44/300], Train Loss: 0.5852, Train Acc: 76.52%, Test Loss: 0.6352, Test Acc: 70.69%\n",
      "Fold 2 Epoch [45/300], Train Loss: 0.5866, Train Acc: 76.09%, Test Loss: 0.6318, Test Acc: 70.69%\n",
      "Fold 2 Epoch [46/300], Train Loss: 0.5644, Train Acc: 76.09%, Test Loss: 0.6265, Test Acc: 63.79%\n",
      "Fold 2 Epoch [47/300], Train Loss: 0.5601, Train Acc: 76.96%, Test Loss: 0.6235, Test Acc: 62.07%\n",
      "Fold 2 Epoch [48/300], Train Loss: 0.5569, Train Acc: 76.09%, Test Loss: 0.6163, Test Acc: 62.07%\n",
      "Fold 2 Epoch [49/300], Train Loss: 0.5575, Train Acc: 76.96%, Test Loss: 0.6067, Test Acc: 62.07%\n",
      "Fold 2 Epoch [50/300], Train Loss: 0.5339, Train Acc: 76.96%, Test Loss: 0.5988, Test Acc: 67.24%\n",
      "Fold 2 Epoch [51/300], Train Loss: 0.5389, Train Acc: 76.96%, Test Loss: 0.5936, Test Acc: 65.52%\n",
      "Fold 2 Epoch [52/300], Train Loss: 0.5251, Train Acc: 76.96%, Test Loss: 0.5908, Test Acc: 62.07%\n",
      "Fold 2 Epoch [53/300], Train Loss: 0.5172, Train Acc: 75.65%, Test Loss: 0.5860, Test Acc: 63.79%\n",
      "Fold 2 Epoch [54/300], Train Loss: 0.5255, Train Acc: 78.70%, Test Loss: 0.5830, Test Acc: 62.07%\n",
      "Fold 2 Epoch [55/300], Train Loss: 0.5175, Train Acc: 75.22%, Test Loss: 0.5827, Test Acc: 62.07%\n",
      "Fold 2 Epoch [56/300], Train Loss: 0.5110, Train Acc: 74.35%, Test Loss: 0.5803, Test Acc: 60.34%\n",
      "Fold 2 Epoch [57/300], Train Loss: 0.5078, Train Acc: 75.22%, Test Loss: 0.5760, Test Acc: 60.34%\n",
      "Fold 2 Epoch [58/300], Train Loss: 0.4957, Train Acc: 75.65%, Test Loss: 0.5706, Test Acc: 60.34%\n",
      "Fold 2 Epoch [59/300], Train Loss: 0.4850, Train Acc: 76.96%, Test Loss: 0.5671, Test Acc: 60.34%\n",
      "Fold 2 Epoch [60/300], Train Loss: 0.4937, Train Acc: 78.70%, Test Loss: 0.5600, Test Acc: 60.34%\n",
      "Fold 2 Epoch [61/300], Train Loss: 0.4737, Train Acc: 78.70%, Test Loss: 0.5531, Test Acc: 62.07%\n",
      "Fold 2 Epoch [62/300], Train Loss: 0.4782, Train Acc: 77.39%, Test Loss: 0.5495, Test Acc: 62.07%\n",
      "Fold 2 Epoch [63/300], Train Loss: 0.4768, Train Acc: 79.13%, Test Loss: 0.5498, Test Acc: 62.07%\n",
      "Fold 2 Epoch [64/300], Train Loss: 0.4765, Train Acc: 79.13%, Test Loss: 0.5556, Test Acc: 58.62%\n",
      "Fold 2 Epoch [65/300], Train Loss: 0.4772, Train Acc: 79.57%, Test Loss: 0.5532, Test Acc: 60.34%\n",
      "Fold 2 Epoch [66/300], Train Loss: 0.4657, Train Acc: 79.57%, Test Loss: 0.5447, Test Acc: 60.34%\n",
      "Fold 2 Epoch [67/300], Train Loss: 0.4559, Train Acc: 80.43%, Test Loss: 0.5375, Test Acc: 58.62%\n",
      "Fold 2 Epoch [68/300], Train Loss: 0.4560, Train Acc: 80.43%, Test Loss: 0.5334, Test Acc: 62.07%\n",
      "Fold 2 Epoch [69/300], Train Loss: 0.4525, Train Acc: 80.87%, Test Loss: 0.5314, Test Acc: 62.07%\n",
      "Fold 2 Epoch [70/300], Train Loss: 0.4349, Train Acc: 79.57%, Test Loss: 0.5265, Test Acc: 63.79%\n",
      "Fold 2 Epoch [71/300], Train Loss: 0.4488, Train Acc: 80.00%, Test Loss: 0.5214, Test Acc: 65.52%\n",
      "Fold 2 Epoch [72/300], Train Loss: 0.4433, Train Acc: 80.43%, Test Loss: 0.5192, Test Acc: 67.24%\n",
      "Fold 2 Epoch [73/300], Train Loss: 0.4273, Train Acc: 80.43%, Test Loss: 0.5172, Test Acc: 67.24%\n",
      "Fold 2 Epoch [74/300], Train Loss: 0.4267, Train Acc: 80.43%, Test Loss: 0.5136, Test Acc: 67.24%\n",
      "Fold 2 Epoch [75/300], Train Loss: 0.4333, Train Acc: 80.87%, Test Loss: 0.5143, Test Acc: 63.79%\n",
      "Fold 2 Epoch [76/300], Train Loss: 0.4264, Train Acc: 80.87%, Test Loss: 0.5112, Test Acc: 62.07%\n",
      "Fold 2 Epoch [77/300], Train Loss: 0.4139, Train Acc: 80.00%, Test Loss: 0.5058, Test Acc: 63.79%\n",
      "Fold 2 Epoch [78/300], Train Loss: 0.4183, Train Acc: 81.30%, Test Loss: 0.4954, Test Acc: 70.69%\n",
      "Fold 2 Epoch [79/300], Train Loss: 0.4127, Train Acc: 80.00%, Test Loss: 0.4887, Test Acc: 77.59%\n",
      "Fold 2 Epoch [80/300], Train Loss: 0.4107, Train Acc: 80.87%, Test Loss: 0.4853, Test Acc: 77.59%\n",
      "Fold 2 Epoch [81/300], Train Loss: 0.4055, Train Acc: 83.04%, Test Loss: 0.4824, Test Acc: 74.14%\n",
      "Fold 2 Epoch [82/300], Train Loss: 0.3958, Train Acc: 81.74%, Test Loss: 0.4831, Test Acc: 70.69%\n",
      "Fold 2 Epoch [83/300], Train Loss: 0.3979, Train Acc: 80.43%, Test Loss: 0.4872, Test Acc: 65.52%\n",
      "Fold 2 Epoch [84/300], Train Loss: 0.3955, Train Acc: 80.87%, Test Loss: 0.4875, Test Acc: 63.79%\n",
      "Fold 2 Epoch [85/300], Train Loss: 0.3910, Train Acc: 81.30%, Test Loss: 0.4766, Test Acc: 68.97%\n",
      "Fold 2 Epoch [86/300], Train Loss: 0.3923, Train Acc: 81.74%, Test Loss: 0.4691, Test Acc: 74.14%\n",
      "Fold 2 Epoch [87/300], Train Loss: 0.3825, Train Acc: 83.04%, Test Loss: 0.4654, Test Acc: 74.14%\n",
      "Fold 2 Epoch [88/300], Train Loss: 0.3779, Train Acc: 83.48%, Test Loss: 0.4618, Test Acc: 74.14%\n",
      "Fold 2 Epoch [89/300], Train Loss: 0.3767, Train Acc: 83.91%, Test Loss: 0.4589, Test Acc: 74.14%\n",
      "Fold 2 Epoch [90/300], Train Loss: 0.3778, Train Acc: 83.91%, Test Loss: 0.4577, Test Acc: 74.14%\n",
      "Fold 2 Epoch [91/300], Train Loss: 0.3764, Train Acc: 84.35%, Test Loss: 0.4556, Test Acc: 74.14%\n",
      "Fold 2 Epoch [92/300], Train Loss: 0.3694, Train Acc: 84.35%, Test Loss: 0.4539, Test Acc: 74.14%\n",
      "Fold 2 Epoch [93/300], Train Loss: 0.3602, Train Acc: 83.48%, Test Loss: 0.4504, Test Acc: 74.14%\n",
      "Fold 2 Epoch [94/300], Train Loss: 0.3498, Train Acc: 85.22%, Test Loss: 0.4494, Test Acc: 74.14%\n",
      "Fold 2 Epoch [95/300], Train Loss: 0.3510, Train Acc: 85.65%, Test Loss: 0.4470, Test Acc: 74.14%\n",
      "Fold 2 Epoch [96/300], Train Loss: 0.3536, Train Acc: 85.22%, Test Loss: 0.4432, Test Acc: 74.14%\n",
      "Fold 2 Epoch [97/300], Train Loss: 0.3457, Train Acc: 84.78%, Test Loss: 0.4436, Test Acc: 74.14%\n",
      "Fold 2 Epoch [98/300], Train Loss: 0.3387, Train Acc: 83.91%, Test Loss: 0.4429, Test Acc: 74.14%\n",
      "Fold 2 Epoch [99/300], Train Loss: 0.3416, Train Acc: 85.22%, Test Loss: 0.4409, Test Acc: 74.14%\n",
      "Fold 2 Epoch [100/300], Train Loss: 0.3390, Train Acc: 84.35%, Test Loss: 0.4401, Test Acc: 74.14%\n",
      "Fold 2 Epoch [101/300], Train Loss: 0.3363, Train Acc: 87.83%, Test Loss: 0.4323, Test Acc: 75.86%\n",
      "Fold 2 Epoch [102/300], Train Loss: 0.3380, Train Acc: 87.83%, Test Loss: 0.4290, Test Acc: 75.86%\n",
      "Fold 2 Epoch [103/300], Train Loss: 0.3247, Train Acc: 87.39%, Test Loss: 0.4234, Test Acc: 77.59%\n",
      "Fold 2 Epoch [104/300], Train Loss: 0.3228, Train Acc: 88.26%, Test Loss: 0.4222, Test Acc: 75.86%\n",
      "Fold 2 Epoch [105/300], Train Loss: 0.3162, Train Acc: 85.22%, Test Loss: 0.4226, Test Acc: 74.14%\n",
      "Fold 2 Epoch [106/300], Train Loss: 0.3234, Train Acc: 87.83%, Test Loss: 0.4173, Test Acc: 74.14%\n",
      "Fold 2 Epoch [107/300], Train Loss: 0.3083, Train Acc: 89.13%, Test Loss: 0.4100, Test Acc: 77.59%\n",
      "Fold 2 Epoch [108/300], Train Loss: 0.3037, Train Acc: 89.13%, Test Loss: 0.4072, Test Acc: 77.59%\n",
      "Fold 2 Epoch [109/300], Train Loss: 0.3070, Train Acc: 88.70%, Test Loss: 0.4061, Test Acc: 77.59%\n",
      "Fold 2 Epoch [110/300], Train Loss: 0.2972, Train Acc: 88.70%, Test Loss: 0.4067, Test Acc: 74.14%\n",
      "Fold 2 Epoch [111/300], Train Loss: 0.2949, Train Acc: 89.13%, Test Loss: 0.4004, Test Acc: 77.59%\n",
      "Fold 2 Epoch [112/300], Train Loss: 0.2808, Train Acc: 89.13%, Test Loss: 0.3973, Test Acc: 75.86%\n",
      "Fold 2 Epoch [113/300], Train Loss: 0.2786, Train Acc: 89.57%, Test Loss: 0.3966, Test Acc: 75.86%\n",
      "Fold 2 Epoch [114/300], Train Loss: 0.2702, Train Acc: 90.87%, Test Loss: 0.3915, Test Acc: 77.59%\n",
      "Fold 2 Epoch [115/300], Train Loss: 0.2717, Train Acc: 89.57%, Test Loss: 0.3991, Test Acc: 75.86%\n",
      "Fold 2 Epoch [116/300], Train Loss: 0.2716, Train Acc: 89.57%, Test Loss: 0.3916, Test Acc: 79.31%\n",
      "Fold 2 Epoch [117/300], Train Loss: 0.2650, Train Acc: 90.43%, Test Loss: 0.3838, Test Acc: 75.86%\n",
      "Fold 2 Epoch [118/300], Train Loss: 0.2603, Train Acc: 90.87%, Test Loss: 0.3844, Test Acc: 77.59%\n",
      "Fold 2 Epoch [119/300], Train Loss: 0.2605, Train Acc: 90.00%, Test Loss: 0.3785, Test Acc: 75.86%\n",
      "Fold 2 Epoch [120/300], Train Loss: 0.2603, Train Acc: 90.00%, Test Loss: 0.3768, Test Acc: 77.59%\n",
      "Fold 2 Epoch [121/300], Train Loss: 0.2533, Train Acc: 90.00%, Test Loss: 0.3818, Test Acc: 75.86%\n",
      "Fold 2 Epoch [122/300], Train Loss: 0.2449, Train Acc: 90.87%, Test Loss: 0.3751, Test Acc: 77.59%\n",
      "Fold 2 Epoch [123/300], Train Loss: 0.2470, Train Acc: 90.43%, Test Loss: 0.3737, Test Acc: 79.31%\n",
      "Fold 2 Epoch [124/300], Train Loss: 0.2428, Train Acc: 90.43%, Test Loss: 0.3718, Test Acc: 79.31%\n",
      "Fold 2 Epoch [125/300], Train Loss: 0.2456, Train Acc: 92.61%, Test Loss: 0.3775, Test Acc: 77.59%\n",
      "Fold 2 Epoch [126/300], Train Loss: 0.2376, Train Acc: 92.17%, Test Loss: 0.3741, Test Acc: 75.86%\n",
      "Fold 2 Epoch [127/300], Train Loss: 0.2314, Train Acc: 91.74%, Test Loss: 0.3699, Test Acc: 79.31%\n",
      "Fold 2 Epoch [128/300], Train Loss: 0.2273, Train Acc: 91.74%, Test Loss: 0.3785, Test Acc: 75.86%\n",
      "Fold 2 Epoch [129/300], Train Loss: 0.2259, Train Acc: 92.17%, Test Loss: 0.3667, Test Acc: 79.31%\n",
      "Fold 2 Epoch [130/300], Train Loss: 0.2401, Train Acc: 91.30%, Test Loss: 0.3635, Test Acc: 81.03%\n",
      "Fold 2 Epoch [131/300], Train Loss: 0.2140, Train Acc: 92.61%, Test Loss: 0.3690, Test Acc: 79.31%\n",
      "Fold 2 Epoch [132/300], Train Loss: 0.2244, Train Acc: 92.61%, Test Loss: 0.3674, Test Acc: 79.31%\n",
      "Fold 2 Epoch [133/300], Train Loss: 0.2176, Train Acc: 93.04%, Test Loss: 0.3672, Test Acc: 79.31%\n",
      "Fold 2 Epoch [134/300], Train Loss: 0.2132, Train Acc: 92.61%, Test Loss: 0.3855, Test Acc: 75.86%\n",
      "Fold 2 Epoch [135/300], Train Loss: 0.2093, Train Acc: 92.17%, Test Loss: 0.3688, Test Acc: 81.03%\n",
      "Fold 2 Epoch [136/300], Train Loss: 0.2082, Train Acc: 92.17%, Test Loss: 0.3753, Test Acc: 77.59%\n",
      "Fold 2 Epoch [137/300], Train Loss: 0.1998, Train Acc: 92.17%, Test Loss: 0.3749, Test Acc: 77.59%\n",
      "Fold 2 Epoch [138/300], Train Loss: 0.2095, Train Acc: 92.17%, Test Loss: 0.3701, Test Acc: 81.03%\n",
      "Fold 2 Epoch [139/300], Train Loss: 0.2069, Train Acc: 92.61%, Test Loss: 0.3724, Test Acc: 81.03%\n",
      "Fold 2 Epoch [140/300], Train Loss: 0.2022, Train Acc: 93.48%, Test Loss: 0.3853, Test Acc: 77.59%\n",
      "Fold 2 Epoch [141/300], Train Loss: 0.1962, Train Acc: 93.04%, Test Loss: 0.3748, Test Acc: 81.03%\n",
      "Fold 2 Epoch [142/300], Train Loss: 0.2003, Train Acc: 91.74%, Test Loss: 0.3706, Test Acc: 81.03%\n",
      "Fold 2 Epoch [143/300], Train Loss: 0.1861, Train Acc: 93.48%, Test Loss: 0.3808, Test Acc: 79.31%\n",
      "Fold 2 Epoch [144/300], Train Loss: 0.1927, Train Acc: 93.48%, Test Loss: 0.3912, Test Acc: 77.59%\n",
      "Fold 2 Epoch [145/300], Train Loss: 0.2024, Train Acc: 90.43%, Test Loss: 0.3734, Test Acc: 81.03%\n",
      "Fold 2 Epoch [146/300], Train Loss: 0.1808, Train Acc: 93.91%, Test Loss: 0.3758, Test Acc: 79.31%\n",
      "Fold 2 Epoch [147/300], Train Loss: 0.1942, Train Acc: 93.04%, Test Loss: 0.3968, Test Acc: 77.59%\n",
      "Fold 2 Epoch [148/300], Train Loss: 0.1844, Train Acc: 93.04%, Test Loss: 0.3816, Test Acc: 79.31%\n",
      "Fold 2 Epoch [149/300], Train Loss: 0.1983, Train Acc: 92.17%, Test Loss: 0.3767, Test Acc: 81.03%\n",
      "Fold 2 Epoch [150/300], Train Loss: 0.1812, Train Acc: 93.48%, Test Loss: 0.3775, Test Acc: 79.31%\n",
      "Fold 2 Epoch [151/300], Train Loss: 0.1909, Train Acc: 92.61%, Test Loss: 0.4045, Test Acc: 77.59%\n",
      "Fold 2 Epoch [152/300], Train Loss: 0.1801, Train Acc: 91.30%, Test Loss: 0.3697, Test Acc: 81.03%\n",
      "Fold 2 Epoch [153/300], Train Loss: 0.1750, Train Acc: 93.04%, Test Loss: 0.3716, Test Acc: 81.03%\n",
      "Fold 2 Epoch [154/300], Train Loss: 0.1788, Train Acc: 93.48%, Test Loss: 0.3995, Test Acc: 79.31%\n",
      "Fold 2 Epoch [155/300], Train Loss: 0.1717, Train Acc: 93.91%, Test Loss: 0.3796, Test Acc: 79.31%\n",
      "Fold 2 Epoch [156/300], Train Loss: 0.1820, Train Acc: 93.91%, Test Loss: 0.3780, Test Acc: 81.03%\n",
      "Fold 2 Epoch [157/300], Train Loss: 0.1582, Train Acc: 93.91%, Test Loss: 0.3799, Test Acc: 79.31%\n",
      "Fold 2 Epoch [158/300], Train Loss: 0.1655, Train Acc: 93.48%, Test Loss: 0.3867, Test Acc: 79.31%\n",
      "Fold 2 Epoch [159/300], Train Loss: 0.1647, Train Acc: 93.48%, Test Loss: 0.3774, Test Acc: 81.03%\n",
      "Fold 2 Epoch [160/300], Train Loss: 0.1600, Train Acc: 94.35%, Test Loss: 0.3865, Test Acc: 79.31%\n",
      "Fold 2 Epoch [161/300], Train Loss: 0.1600, Train Acc: 93.48%, Test Loss: 0.3936, Test Acc: 81.03%\n",
      "Fold 2 Epoch [162/300], Train Loss: 0.1556, Train Acc: 93.48%, Test Loss: 0.3879, Test Acc: 81.03%\n",
      "Fold 2 Epoch [163/300], Train Loss: 0.1575, Train Acc: 94.35%, Test Loss: 0.3945, Test Acc: 79.31%\n",
      "Fold 2 Epoch [164/300], Train Loss: 0.1534, Train Acc: 93.48%, Test Loss: 0.3925, Test Acc: 79.31%\n",
      "Fold 2 Epoch [165/300], Train Loss: 0.1581, Train Acc: 93.91%, Test Loss: 0.3905, Test Acc: 79.31%\n",
      "Fold 2 Epoch [166/300], Train Loss: 0.1444, Train Acc: 94.35%, Test Loss: 0.3939, Test Acc: 79.31%\n",
      "Fold 2 Epoch [167/300], Train Loss: 0.1480, Train Acc: 94.35%, Test Loss: 0.3943, Test Acc: 79.31%\n",
      "Fold 2 Epoch [168/300], Train Loss: 0.1435, Train Acc: 95.22%, Test Loss: 0.3963, Test Acc: 79.31%\n",
      "Fold 2 Epoch [169/300], Train Loss: 0.1469, Train Acc: 94.78%, Test Loss: 0.3905, Test Acc: 79.31%\n",
      "Fold 2 Epoch [170/300], Train Loss: 0.1422, Train Acc: 95.22%, Test Loss: 0.3964, Test Acc: 81.03%\n",
      "Fold 2 Epoch [171/300], Train Loss: 0.1489, Train Acc: 95.22%, Test Loss: 0.3900, Test Acc: 79.31%\n",
      "Fold 2 Epoch [172/300], Train Loss: 0.1630, Train Acc: 94.78%, Test Loss: 0.4126, Test Acc: 81.03%\n",
      "Fold 2 Epoch [173/300], Train Loss: 0.1384, Train Acc: 96.09%, Test Loss: 0.4039, Test Acc: 79.31%\n",
      "Fold 2 Epoch [174/300], Train Loss: 0.1421, Train Acc: 94.78%, Test Loss: 0.4036, Test Acc: 79.31%\n",
      "Fold 2 Epoch [175/300], Train Loss: 0.1347, Train Acc: 95.22%, Test Loss: 0.4144, Test Acc: 81.03%\n",
      "Fold 2 Epoch [176/300], Train Loss: 0.1334, Train Acc: 95.22%, Test Loss: 0.4193, Test Acc: 81.03%\n",
      "Fold 2 Epoch [177/300], Train Loss: 0.1280, Train Acc: 94.35%, Test Loss: 0.4136, Test Acc: 79.31%\n",
      "Fold 2 Epoch [178/300], Train Loss: 0.1383, Train Acc: 94.35%, Test Loss: 0.4151, Test Acc: 79.31%\n",
      "Fold 2 Epoch [179/300], Train Loss: 0.1285, Train Acc: 94.78%, Test Loss: 0.4179, Test Acc: 79.31%\n",
      "Fold 2 Epoch [180/300], Train Loss: 0.1291, Train Acc: 95.22%, Test Loss: 0.4218, Test Acc: 81.03%\n",
      "Fold 2 Epoch [181/300], Train Loss: 0.1302, Train Acc: 95.22%, Test Loss: 0.4134, Test Acc: 79.31%\n",
      "Fold 2 Epoch [182/300], Train Loss: 0.1360, Train Acc: 96.09%, Test Loss: 0.4132, Test Acc: 79.31%\n",
      "Fold 2 Epoch [183/300], Train Loss: 0.1191, Train Acc: 95.65%, Test Loss: 0.4204, Test Acc: 82.76%\n",
      "Fold 2 Epoch [184/300], Train Loss: 0.1252, Train Acc: 96.09%, Test Loss: 0.4120, Test Acc: 82.76%\n",
      "Fold 2 Epoch [185/300], Train Loss: 0.1239, Train Acc: 95.65%, Test Loss: 0.4152, Test Acc: 82.76%\n",
      "Fold 2 Epoch [186/300], Train Loss: 0.1282, Train Acc: 96.09%, Test Loss: 0.4187, Test Acc: 82.76%\n",
      "Fold 2 Epoch [187/300], Train Loss: 0.1193, Train Acc: 95.65%, Test Loss: 0.4088, Test Acc: 79.31%\n",
      "Fold 2 Epoch [188/300], Train Loss: 0.1281, Train Acc: 95.65%, Test Loss: 0.4145, Test Acc: 81.03%\n",
      "Fold 2 Epoch [189/300], Train Loss: 0.1263, Train Acc: 95.65%, Test Loss: 0.4301, Test Acc: 82.76%\n",
      "Fold 2 Epoch [190/300], Train Loss: 0.1166, Train Acc: 95.65%, Test Loss: 0.4169, Test Acc: 81.03%\n",
      "Fold 2 Epoch [191/300], Train Loss: 0.1159, Train Acc: 95.65%, Test Loss: 0.4171, Test Acc: 79.31%\n",
      "Fold 2 Epoch [192/300], Train Loss: 0.1133, Train Acc: 96.09%, Test Loss: 0.4194, Test Acc: 81.03%\n",
      "Fold 2 Epoch [193/300], Train Loss: 0.1110, Train Acc: 96.96%, Test Loss: 0.4290, Test Acc: 82.76%\n",
      "Fold 2 Epoch [194/300], Train Loss: 0.1158, Train Acc: 95.65%, Test Loss: 0.4157, Test Acc: 81.03%\n",
      "Fold 2 Epoch [195/300], Train Loss: 0.1125, Train Acc: 96.09%, Test Loss: 0.4164, Test Acc: 81.03%\n",
      "Fold 2 Epoch [196/300], Train Loss: 0.1200, Train Acc: 96.52%, Test Loss: 0.4271, Test Acc: 82.76%\n",
      "Fold 2 Epoch [197/300], Train Loss: 0.1179, Train Acc: 96.52%, Test Loss: 0.4377, Test Acc: 82.76%\n",
      "Fold 2 Epoch [198/300], Train Loss: 0.1176, Train Acc: 96.52%, Test Loss: 0.4170, Test Acc: 81.03%\n",
      "Fold 2 Epoch [199/300], Train Loss: 0.1051, Train Acc: 97.39%, Test Loss: 0.4201, Test Acc: 81.03%\n",
      "Fold 2 Epoch [200/300], Train Loss: 0.1008, Train Acc: 97.83%, Test Loss: 0.4296, Test Acc: 82.76%\n",
      "Fold 2 Epoch [201/300], Train Loss: 0.1016, Train Acc: 97.39%, Test Loss: 0.4364, Test Acc: 82.76%\n",
      "Fold 2 Epoch [202/300], Train Loss: 0.1132, Train Acc: 96.96%, Test Loss: 0.4272, Test Acc: 81.03%\n",
      "Fold 2 Epoch [203/300], Train Loss: 0.1004, Train Acc: 97.39%, Test Loss: 0.4309, Test Acc: 81.03%\n",
      "Fold 2 Epoch [204/300], Train Loss: 0.0987, Train Acc: 98.26%, Test Loss: 0.4436, Test Acc: 82.76%\n",
      "Fold 2 Epoch [205/300], Train Loss: 0.0991, Train Acc: 97.83%, Test Loss: 0.4476, Test Acc: 82.76%\n",
      "Fold 2 Epoch [206/300], Train Loss: 0.0952, Train Acc: 98.26%, Test Loss: 0.4541, Test Acc: 82.76%\n",
      "Fold 2 Epoch [207/300], Train Loss: 0.0967, Train Acc: 98.70%, Test Loss: 0.4638, Test Acc: 81.03%\n",
      "Fold 2 Epoch [208/300], Train Loss: 0.0953, Train Acc: 98.70%, Test Loss: 0.4700, Test Acc: 81.03%\n",
      "Fold 2 Epoch [209/300], Train Loss: 0.0896, Train Acc: 98.70%, Test Loss: 0.4652, Test Acc: 81.03%\n",
      "Fold 2 Epoch [210/300], Train Loss: 0.1068, Train Acc: 96.96%, Test Loss: 0.4598, Test Acc: 81.03%\n",
      "Fold 2 Epoch [211/300], Train Loss: 0.0928, Train Acc: 97.39%, Test Loss: 0.4819, Test Acc: 81.03%\n",
      "Fold 2 Epoch [212/300], Train Loss: 0.0962, Train Acc: 98.70%, Test Loss: 0.4683, Test Acc: 81.03%\n",
      "Fold 2 Epoch [213/300], Train Loss: 0.0985, Train Acc: 99.13%, Test Loss: 0.4719, Test Acc: 81.03%\n",
      "Fold 2 Epoch [214/300], Train Loss: 0.0840, Train Acc: 98.70%, Test Loss: 0.4726, Test Acc: 82.76%\n",
      "Fold 2 Epoch [215/300], Train Loss: 0.0873, Train Acc: 98.70%, Test Loss: 0.4754, Test Acc: 82.76%\n",
      "Fold 2 Epoch [216/300], Train Loss: 0.0861, Train Acc: 98.26%, Test Loss: 0.4878, Test Acc: 82.76%\n",
      "Fold 2 Epoch [217/300], Train Loss: 0.0861, Train Acc: 98.70%, Test Loss: 0.4869, Test Acc: 82.76%\n",
      "Fold 2 Epoch [218/300], Train Loss: 0.0825, Train Acc: 98.70%, Test Loss: 0.4831, Test Acc: 81.03%\n",
      "Fold 2 Epoch [219/300], Train Loss: 0.0806, Train Acc: 97.83%, Test Loss: 0.4936, Test Acc: 82.76%\n",
      "Fold 2 Epoch [220/300], Train Loss: 0.0793, Train Acc: 99.13%, Test Loss: 0.4818, Test Acc: 82.76%\n",
      "Fold 2 Epoch [221/300], Train Loss: 0.0804, Train Acc: 97.83%, Test Loss: 0.4891, Test Acc: 82.76%\n",
      "Fold 2 Epoch [222/300], Train Loss: 0.0741, Train Acc: 98.70%, Test Loss: 0.4824, Test Acc: 82.76%\n",
      "Fold 2 Epoch [223/300], Train Loss: 0.0725, Train Acc: 99.13%, Test Loss: 0.4833, Test Acc: 84.48%\n",
      "Fold 2 Epoch [224/300], Train Loss: 0.0756, Train Acc: 99.13%, Test Loss: 0.4836, Test Acc: 84.48%\n",
      "Fold 2 Epoch [225/300], Train Loss: 0.0718, Train Acc: 99.57%, Test Loss: 0.4837, Test Acc: 84.48%\n",
      "Fold 2 Epoch [226/300], Train Loss: 0.0796, Train Acc: 99.13%, Test Loss: 0.4932, Test Acc: 82.76%\n",
      "Fold 2 Epoch [227/300], Train Loss: 0.0704, Train Acc: 99.13%, Test Loss: 0.4844, Test Acc: 84.48%\n",
      "Fold 2 Epoch [228/300], Train Loss: 0.0667, Train Acc: 99.57%, Test Loss: 0.4922, Test Acc: 82.76%\n",
      "Fold 2 Epoch [229/300], Train Loss: 0.0687, Train Acc: 99.13%, Test Loss: 0.5011, Test Acc: 82.76%\n",
      "Fold 2 Epoch [230/300], Train Loss: 0.0739, Train Acc: 98.26%, Test Loss: 0.5229, Test Acc: 81.03%\n",
      "Fold 2 Epoch [231/300], Train Loss: 0.0687, Train Acc: 98.70%, Test Loss: 0.5128, Test Acc: 81.03%\n",
      "Fold 2 Epoch [232/300], Train Loss: 0.0793, Train Acc: 98.70%, Test Loss: 0.5109, Test Acc: 81.03%\n",
      "Fold 2 Epoch [233/300], Train Loss: 0.0721, Train Acc: 98.70%, Test Loss: 0.5275, Test Acc: 81.03%\n",
      "Fold 2 Epoch [234/300], Train Loss: 0.0691, Train Acc: 99.13%, Test Loss: 0.5171, Test Acc: 82.76%\n",
      "Fold 2 Epoch [235/300], Train Loss: 0.0693, Train Acc: 98.70%, Test Loss: 0.5161, Test Acc: 84.48%\n",
      "Fold 2 Epoch [236/300], Train Loss: 0.0645, Train Acc: 99.13%, Test Loss: 0.5310, Test Acc: 82.76%\n",
      "Fold 2 Epoch [237/300], Train Loss: 0.0624, Train Acc: 99.13%, Test Loss: 0.5353, Test Acc: 82.76%\n",
      "Fold 2 Epoch [238/300], Train Loss: 0.0713, Train Acc: 98.70%, Test Loss: 0.5299, Test Acc: 82.76%\n",
      "Fold 2 Epoch [239/300], Train Loss: 0.0599, Train Acc: 99.13%, Test Loss: 0.5315, Test Acc: 82.76%\n",
      "Fold 2 Epoch [240/300], Train Loss: 0.0632, Train Acc: 99.13%, Test Loss: 0.5378, Test Acc: 81.03%\n",
      "Fold 2 Epoch [241/300], Train Loss: 0.0603, Train Acc: 99.13%, Test Loss: 0.5284, Test Acc: 84.48%\n",
      "Fold 2 Epoch [242/300], Train Loss: 0.0556, Train Acc: 99.13%, Test Loss: 0.5351, Test Acc: 84.48%\n",
      "Fold 2 Epoch [243/300], Train Loss: 0.0573, Train Acc: 99.13%, Test Loss: 0.5449, Test Acc: 82.76%\n",
      "Fold 2 Epoch [244/300], Train Loss: 0.0564, Train Acc: 99.13%, Test Loss: 0.5406, Test Acc: 82.76%\n",
      "Fold 2 Epoch [245/300], Train Loss: 0.0544, Train Acc: 99.13%, Test Loss: 0.5428, Test Acc: 82.76%\n",
      "Fold 2 Epoch [246/300], Train Loss: 0.0571, Train Acc: 99.57%, Test Loss: 0.5434, Test Acc: 82.76%\n",
      "Fold 2 Epoch [247/300], Train Loss: 0.0528, Train Acc: 99.57%, Test Loss: 0.5441, Test Acc: 82.76%\n",
      "Fold 2 Epoch [248/300], Train Loss: 0.0560, Train Acc: 99.13%, Test Loss: 0.5579, Test Acc: 82.76%\n",
      "Fold 2 Epoch [249/300], Train Loss: 0.0587, Train Acc: 98.70%, Test Loss: 0.5495, Test Acc: 82.76%\n",
      "Fold 2 Epoch [250/300], Train Loss: 0.0633, Train Acc: 99.57%, Test Loss: 0.5440, Test Acc: 82.76%\n",
      "Fold 2 Epoch [251/300], Train Loss: 0.0627, Train Acc: 97.39%, Test Loss: 0.5825, Test Acc: 79.31%\n",
      "Fold 2 Epoch [252/300], Train Loss: 0.0561, Train Acc: 98.70%, Test Loss: 0.5448, Test Acc: 82.76%\n",
      "Fold 2 Epoch [253/300], Train Loss: 0.0483, Train Acc: 99.57%, Test Loss: 0.5486, Test Acc: 82.76%\n",
      "Fold 2 Epoch [254/300], Train Loss: 0.0479, Train Acc: 99.57%, Test Loss: 0.5493, Test Acc: 84.48%\n",
      "Fold 2 Epoch [255/300], Train Loss: 0.0475, Train Acc: 99.57%, Test Loss: 0.5554, Test Acc: 84.48%\n",
      "Fold 2 Epoch [256/300], Train Loss: 0.0515, Train Acc: 99.13%, Test Loss: 0.5712, Test Acc: 82.76%\n",
      "Fold 2 Epoch [257/300], Train Loss: 0.0443, Train Acc: 99.13%, Test Loss: 0.5787, Test Acc: 82.76%\n",
      "Fold 2 Epoch [258/300], Train Loss: 0.0473, Train Acc: 99.57%, Test Loss: 0.5740, Test Acc: 82.76%\n",
      "Fold 2 Epoch [259/300], Train Loss: 0.0475, Train Acc: 99.13%, Test Loss: 0.5896, Test Acc: 82.76%\n",
      "Fold 2 Epoch [260/300], Train Loss: 0.0443, Train Acc: 99.13%, Test Loss: 0.5913, Test Acc: 82.76%\n",
      "Fold 2 Epoch [261/300], Train Loss: 0.0476, Train Acc: 99.13%, Test Loss: 0.5890, Test Acc: 81.03%\n",
      "Fold 2 Epoch [262/300], Train Loss: 0.0403, Train Acc: 99.13%, Test Loss: 0.5902, Test Acc: 82.76%\n",
      "Fold 2 Epoch [263/300], Train Loss: 0.0545, Train Acc: 98.26%, Test Loss: 0.6280, Test Acc: 79.31%\n",
      "Fold 2 Epoch [264/300], Train Loss: 0.0400, Train Acc: 99.57%, Test Loss: 0.6105, Test Acc: 81.03%\n",
      "Fold 2 Epoch [265/300], Train Loss: 0.0424, Train Acc: 99.57%, Test Loss: 0.6152, Test Acc: 81.03%\n",
      "Fold 2 Epoch [266/300], Train Loss: 0.0578, Train Acc: 97.83%, Test Loss: 0.6283, Test Acc: 77.59%\n",
      "Fold 2 Epoch [267/300], Train Loss: 0.0404, Train Acc: 99.13%, Test Loss: 0.5960, Test Acc: 81.03%\n",
      "Fold 2 Epoch [268/300], Train Loss: 0.0473, Train Acc: 99.57%, Test Loss: 0.5947, Test Acc: 81.03%\n",
      "Fold 2 Epoch [269/300], Train Loss: 0.0427, Train Acc: 99.13%, Test Loss: 0.6025, Test Acc: 82.76%\n",
      "Fold 2 Epoch [270/300], Train Loss: 0.0388, Train Acc: 99.57%, Test Loss: 0.5915, Test Acc: 82.76%\n",
      "Fold 2 Epoch [271/300], Train Loss: 0.0463, Train Acc: 99.13%, Test Loss: 0.5946, Test Acc: 81.03%\n",
      "Fold 2 Epoch [272/300], Train Loss: 0.0377, Train Acc: 99.57%, Test Loss: 0.5930, Test Acc: 82.76%\n",
      "Fold 2 Epoch [273/300], Train Loss: 0.0367, Train Acc: 99.57%, Test Loss: 0.5869, Test Acc: 84.48%\n",
      "Fold 2 Epoch [274/300], Train Loss: 0.0339, Train Acc: 99.57%, Test Loss: 0.5835, Test Acc: 82.76%\n",
      "Fold 2 Epoch [275/300], Train Loss: 0.0335, Train Acc: 99.57%, Test Loss: 0.5886, Test Acc: 82.76%\n",
      "Fold 2 Epoch [276/300], Train Loss: 0.0316, Train Acc: 99.57%, Test Loss: 0.6000, Test Acc: 82.76%\n",
      "Fold 2 Epoch [277/300], Train Loss: 0.0342, Train Acc: 99.57%, Test Loss: 0.6223, Test Acc: 79.31%\n",
      "Fold 2 Epoch [278/300], Train Loss: 0.0306, Train Acc: 99.57%, Test Loss: 0.6263, Test Acc: 81.03%\n",
      "Fold 2 Epoch [279/300], Train Loss: 0.0285, Train Acc: 99.57%, Test Loss: 0.6336, Test Acc: 82.76%\n",
      "Fold 2 Epoch [280/300], Train Loss: 0.0300, Train Acc: 99.57%, Test Loss: 0.6483, Test Acc: 79.31%\n",
      "Fold 2 Epoch [281/300], Train Loss: 0.0292, Train Acc: 99.57%, Test Loss: 0.6465, Test Acc: 82.76%\n",
      "Fold 2 Epoch [282/300], Train Loss: 0.0288, Train Acc: 99.57%, Test Loss: 0.6494, Test Acc: 79.31%\n",
      "Fold 2 Epoch [283/300], Train Loss: 0.0322, Train Acc: 99.57%, Test Loss: 0.6436, Test Acc: 81.03%\n",
      "Fold 2 Epoch [284/300], Train Loss: 0.0275, Train Acc: 99.57%, Test Loss: 0.6427, Test Acc: 82.76%\n",
      "Fold 2 Epoch [285/300], Train Loss: 0.0265, Train Acc: 99.57%, Test Loss: 0.6458, Test Acc: 82.76%\n",
      "Fold 2 Epoch [286/300], Train Loss: 0.0266, Train Acc: 99.57%, Test Loss: 0.6474, Test Acc: 82.76%\n",
      "Fold 2 Epoch [287/300], Train Loss: 0.0281, Train Acc: 99.57%, Test Loss: 0.6609, Test Acc: 79.31%\n",
      "Fold 2 Epoch [288/300], Train Loss: 0.0316, Train Acc: 99.57%, Test Loss: 0.6549, Test Acc: 82.76%\n",
      "Fold 2 Epoch [289/300], Train Loss: 0.0258, Train Acc: 99.57%, Test Loss: 0.6602, Test Acc: 82.76%\n",
      "Fold 2 Epoch [290/300], Train Loss: 0.0267, Train Acc: 99.57%, Test Loss: 0.6670, Test Acc: 81.03%\n",
      "Fold 2 Epoch [291/300], Train Loss: 0.0254, Train Acc: 99.57%, Test Loss: 0.6579, Test Acc: 81.03%\n",
      "Fold 2 Epoch [292/300], Train Loss: 0.0242, Train Acc: 99.57%, Test Loss: 0.6480, Test Acc: 82.76%\n",
      "Fold 2 Epoch [293/300], Train Loss: 0.0238, Train Acc: 99.57%, Test Loss: 0.6400, Test Acc: 81.03%\n",
      "Fold 2 Epoch [294/300], Train Loss: 0.0252, Train Acc: 99.57%, Test Loss: 0.6388, Test Acc: 81.03%\n",
      "Fold 2 Epoch [295/300], Train Loss: 0.0257, Train Acc: 99.57%, Test Loss: 0.6595, Test Acc: 81.03%\n",
      "Fold 2 Epoch [296/300], Train Loss: 0.0269, Train Acc: 99.57%, Test Loss: 0.6751, Test Acc: 82.76%\n",
      "Fold 2 Epoch [297/300], Train Loss: 0.0247, Train Acc: 99.57%, Test Loss: 0.6873, Test Acc: 82.76%\n",
      "Fold 2 Epoch [298/300], Train Loss: 0.0237, Train Acc: 99.57%, Test Loss: 0.6927, Test Acc: 79.31%\n",
      "Fold 2 Epoch [299/300], Train Loss: 0.0232, Train Acc: 99.57%, Test Loss: 0.6882, Test Acc: 82.76%\n",
      "Fold 2 Epoch [300/300], Train Loss: 0.0221, Train Acc: 99.57%, Test Loss: 0.6874, Test Acc: 82.76%\n",
      "Fold 2 Best Accuracy: 84.48%\n",
      "Fold 3/5\n",
      "Fold 3 Epoch [1/300], Train Loss: 2.0194, Train Acc: 35.22%, Test Loss: 2.0611, Test Acc: 27.59%\n",
      "Fold 3 Epoch [2/300], Train Loss: 1.9154, Train Acc: 34.78%, Test Loss: 1.9436, Test Acc: 27.59%\n",
      "Fold 3 Epoch [3/300], Train Loss: 1.8274, Train Acc: 34.78%, Test Loss: 1.8386, Test Acc: 27.59%\n",
      "Fold 3 Epoch [4/300], Train Loss: 1.7407, Train Acc: 35.65%, Test Loss: 1.7420, Test Acc: 31.03%\n",
      "Fold 3 Epoch [5/300], Train Loss: 1.6651, Train Acc: 40.43%, Test Loss: 1.6581, Test Acc: 32.76%\n",
      "Fold 3 Epoch [6/300], Train Loss: 1.5971, Train Acc: 45.22%, Test Loss: 1.5855, Test Acc: 44.83%\n",
      "Fold 3 Epoch [7/300], Train Loss: 1.5484, Train Acc: 48.70%, Test Loss: 1.5241, Test Acc: 48.28%\n",
      "Fold 3 Epoch [8/300], Train Loss: 1.4913, Train Acc: 50.00%, Test Loss: 1.4741, Test Acc: 53.45%\n",
      "Fold 3 Epoch [9/300], Train Loss: 1.4320, Train Acc: 49.57%, Test Loss: 1.4337, Test Acc: 50.00%\n",
      "Fold 3 Epoch [10/300], Train Loss: 1.4059, Train Acc: 50.00%, Test Loss: 1.3928, Test Acc: 51.72%\n",
      "Fold 3 Epoch [11/300], Train Loss: 1.3637, Train Acc: 51.74%, Test Loss: 1.3599, Test Acc: 48.28%\n",
      "Fold 3 Epoch [12/300], Train Loss: 1.3299, Train Acc: 51.30%, Test Loss: 1.3300, Test Acc: 46.55%\n",
      "Fold 3 Epoch [13/300], Train Loss: 1.2924, Train Acc: 52.61%, Test Loss: 1.2965, Test Acc: 55.17%\n",
      "Fold 3 Epoch [14/300], Train Loss: 1.2560, Train Acc: 56.52%, Test Loss: 1.2634, Test Acc: 60.34%\n",
      "Fold 3 Epoch [15/300], Train Loss: 1.2182, Train Acc: 55.22%, Test Loss: 1.2356, Test Acc: 55.17%\n",
      "Fold 3 Epoch [16/300], Train Loss: 1.1877, Train Acc: 58.70%, Test Loss: 1.2037, Test Acc: 62.07%\n",
      "Fold 3 Epoch [17/300], Train Loss: 1.1576, Train Acc: 59.13%, Test Loss: 1.1783, Test Acc: 62.07%\n",
      "Fold 3 Epoch [18/300], Train Loss: 1.1296, Train Acc: 60.00%, Test Loss: 1.1506, Test Acc: 63.79%\n",
      "Fold 3 Epoch [19/300], Train Loss: 1.0988, Train Acc: 60.43%, Test Loss: 1.1239, Test Acc: 62.07%\n",
      "Fold 3 Epoch [20/300], Train Loss: 1.0682, Train Acc: 63.04%, Test Loss: 1.0916, Test Acc: 63.79%\n",
      "Fold 3 Epoch [21/300], Train Loss: 1.0398, Train Acc: 66.09%, Test Loss: 1.0601, Test Acc: 68.97%\n",
      "Fold 3 Epoch [22/300], Train Loss: 1.0142, Train Acc: 71.30%, Test Loss: 1.0310, Test Acc: 67.24%\n",
      "Fold 3 Epoch [23/300], Train Loss: 0.9913, Train Acc: 73.04%, Test Loss: 1.0085, Test Acc: 72.41%\n",
      "Fold 3 Epoch [24/300], Train Loss: 0.9633, Train Acc: 72.61%, Test Loss: 0.9902, Test Acc: 72.41%\n",
      "Fold 3 Epoch [25/300], Train Loss: 0.9453, Train Acc: 72.61%, Test Loss: 0.9735, Test Acc: 70.69%\n",
      "Fold 3 Epoch [26/300], Train Loss: 0.9231, Train Acc: 73.04%, Test Loss: 0.9592, Test Acc: 68.97%\n",
      "Fold 3 Epoch [27/300], Train Loss: 0.9032, Train Acc: 78.26%, Test Loss: 0.9363, Test Acc: 70.69%\n",
      "Fold 3 Epoch [28/300], Train Loss: 0.8845, Train Acc: 74.78%, Test Loss: 0.9044, Test Acc: 65.52%\n",
      "Fold 3 Epoch [29/300], Train Loss: 0.8594, Train Acc: 76.52%, Test Loss: 0.8782, Test Acc: 67.24%\n",
      "Fold 3 Epoch [30/300], Train Loss: 0.8286, Train Acc: 75.65%, Test Loss: 0.8575, Test Acc: 67.24%\n",
      "Fold 3 Epoch [31/300], Train Loss: 0.8140, Train Acc: 79.57%, Test Loss: 0.8374, Test Acc: 70.69%\n",
      "Fold 3 Epoch [32/300], Train Loss: 0.7974, Train Acc: 78.70%, Test Loss: 0.8234, Test Acc: 68.97%\n",
      "Fold 3 Epoch [33/300], Train Loss: 0.7695, Train Acc: 76.09%, Test Loss: 0.8022, Test Acc: 68.97%\n",
      "Fold 3 Epoch [34/300], Train Loss: 0.7538, Train Acc: 76.96%, Test Loss: 0.7806, Test Acc: 75.86%\n",
      "Fold 3 Epoch [35/300], Train Loss: 0.7361, Train Acc: 77.83%, Test Loss: 0.7595, Test Acc: 77.59%\n",
      "Fold 3 Epoch [36/300], Train Loss: 0.7222, Train Acc: 78.26%, Test Loss: 0.7412, Test Acc: 77.59%\n",
      "Fold 3 Epoch [37/300], Train Loss: 0.7009, Train Acc: 76.52%, Test Loss: 0.7311, Test Acc: 77.59%\n",
      "Fold 3 Epoch [38/300], Train Loss: 0.6794, Train Acc: 78.70%, Test Loss: 0.7094, Test Acc: 79.31%\n",
      "Fold 3 Epoch [39/300], Train Loss: 0.6571, Train Acc: 80.00%, Test Loss: 0.6819, Test Acc: 77.59%\n",
      "Fold 3 Epoch [40/300], Train Loss: 0.6399, Train Acc: 78.70%, Test Loss: 0.6648, Test Acc: 74.14%\n",
      "Fold 3 Epoch [41/300], Train Loss: 0.6276, Train Acc: 79.57%, Test Loss: 0.6493, Test Acc: 75.86%\n",
      "Fold 3 Epoch [42/300], Train Loss: 0.6099, Train Acc: 79.57%, Test Loss: 0.6362, Test Acc: 75.86%\n",
      "Fold 3 Epoch [43/300], Train Loss: 0.5932, Train Acc: 79.57%, Test Loss: 0.6197, Test Acc: 75.86%\n",
      "Fold 3 Epoch [44/300], Train Loss: 0.5908, Train Acc: 79.57%, Test Loss: 0.6066, Test Acc: 77.59%\n",
      "Fold 3 Epoch [45/300], Train Loss: 0.5722, Train Acc: 79.57%, Test Loss: 0.5941, Test Acc: 75.86%\n",
      "Fold 3 Epoch [46/300], Train Loss: 0.5507, Train Acc: 79.57%, Test Loss: 0.5825, Test Acc: 75.86%\n",
      "Fold 3 Epoch [47/300], Train Loss: 0.5501, Train Acc: 79.13%, Test Loss: 0.5737, Test Acc: 75.86%\n",
      "Fold 3 Epoch [48/300], Train Loss: 0.5325, Train Acc: 78.70%, Test Loss: 0.5650, Test Acc: 75.86%\n",
      "Fold 3 Epoch [49/300], Train Loss: 0.5286, Train Acc: 79.57%, Test Loss: 0.5519, Test Acc: 75.86%\n",
      "Fold 3 Epoch [50/300], Train Loss: 0.5010, Train Acc: 79.13%, Test Loss: 0.5395, Test Acc: 75.86%\n",
      "Fold 3 Epoch [51/300], Train Loss: 0.5006, Train Acc: 80.87%, Test Loss: 0.5323, Test Acc: 74.14%\n",
      "Fold 3 Epoch [52/300], Train Loss: 0.4875, Train Acc: 80.00%, Test Loss: 0.5275, Test Acc: 75.86%\n",
      "Fold 3 Epoch [53/300], Train Loss: 0.4825, Train Acc: 78.26%, Test Loss: 0.5171, Test Acc: 77.59%\n",
      "Fold 3 Epoch [54/300], Train Loss: 0.4721, Train Acc: 79.57%, Test Loss: 0.5110, Test Acc: 77.59%\n",
      "Fold 3 Epoch [55/300], Train Loss: 0.4709, Train Acc: 78.26%, Test Loss: 0.5048, Test Acc: 77.59%\n",
      "Fold 3 Epoch [56/300], Train Loss: 0.4687, Train Acc: 81.74%, Test Loss: 0.5037, Test Acc: 75.86%\n",
      "Fold 3 Epoch [57/300], Train Loss: 0.4600, Train Acc: 81.30%, Test Loss: 0.4985, Test Acc: 74.14%\n",
      "Fold 3 Epoch [58/300], Train Loss: 0.4480, Train Acc: 80.87%, Test Loss: 0.4921, Test Acc: 74.14%\n",
      "Fold 3 Epoch [59/300], Train Loss: 0.4455, Train Acc: 80.43%, Test Loss: 0.4870, Test Acc: 77.59%\n",
      "Fold 3 Epoch [60/300], Train Loss: 0.4355, Train Acc: 79.13%, Test Loss: 0.4857, Test Acc: 75.86%\n",
      "Fold 3 Epoch [61/300], Train Loss: 0.4504, Train Acc: 80.00%, Test Loss: 0.4851, Test Acc: 77.59%\n",
      "Fold 3 Epoch [62/300], Train Loss: 0.4421, Train Acc: 80.43%, Test Loss: 0.4823, Test Acc: 77.59%\n",
      "Fold 3 Epoch [63/300], Train Loss: 0.4385, Train Acc: 80.43%, Test Loss: 0.4798, Test Acc: 77.59%\n",
      "Fold 3 Epoch [64/300], Train Loss: 0.4318, Train Acc: 81.30%, Test Loss: 0.4775, Test Acc: 77.59%\n",
      "Fold 3 Epoch [65/300], Train Loss: 0.4238, Train Acc: 81.74%, Test Loss: 0.4768, Test Acc: 77.59%\n",
      "Fold 3 Epoch [66/300], Train Loss: 0.4344, Train Acc: 81.74%, Test Loss: 0.4746, Test Acc: 77.59%\n",
      "Fold 3 Epoch [67/300], Train Loss: 0.4170, Train Acc: 81.74%, Test Loss: 0.4704, Test Acc: 75.86%\n",
      "Fold 3 Epoch [68/300], Train Loss: 0.4205, Train Acc: 80.87%, Test Loss: 0.4708, Test Acc: 75.86%\n",
      "Fold 3 Epoch [69/300], Train Loss: 0.4078, Train Acc: 81.30%, Test Loss: 0.4696, Test Acc: 75.86%\n",
      "Fold 3 Epoch [70/300], Train Loss: 0.4093, Train Acc: 82.61%, Test Loss: 0.4696, Test Acc: 75.86%\n",
      "Fold 3 Epoch [71/300], Train Loss: 0.4081, Train Acc: 81.74%, Test Loss: 0.4687, Test Acc: 75.86%\n",
      "Fold 3 Epoch [72/300], Train Loss: 0.4043, Train Acc: 81.74%, Test Loss: 0.4661, Test Acc: 75.86%\n",
      "Fold 3 Epoch [73/300], Train Loss: 0.4068, Train Acc: 80.87%, Test Loss: 0.4652, Test Acc: 75.86%\n",
      "Fold 3 Epoch [74/300], Train Loss: 0.3895, Train Acc: 82.61%, Test Loss: 0.4636, Test Acc: 75.86%\n",
      "Fold 3 Epoch [75/300], Train Loss: 0.3973, Train Acc: 82.61%, Test Loss: 0.4640, Test Acc: 74.14%\n",
      "Fold 3 Epoch [76/300], Train Loss: 0.3961, Train Acc: 83.48%, Test Loss: 0.4677, Test Acc: 75.86%\n",
      "Fold 3 Epoch [77/300], Train Loss: 0.3867, Train Acc: 82.61%, Test Loss: 0.4655, Test Acc: 77.59%\n",
      "Fold 3 Epoch [78/300], Train Loss: 0.3905, Train Acc: 83.04%, Test Loss: 0.4627, Test Acc: 75.86%\n",
      "Fold 3 Epoch [79/300], Train Loss: 0.3830, Train Acc: 82.61%, Test Loss: 0.4619, Test Acc: 75.86%\n",
      "Fold 3 Epoch [80/300], Train Loss: 0.3844, Train Acc: 83.04%, Test Loss: 0.4659, Test Acc: 75.86%\n",
      "Fold 3 Epoch [81/300], Train Loss: 0.3807, Train Acc: 83.48%, Test Loss: 0.4714, Test Acc: 75.86%\n",
      "Fold 3 Epoch [82/300], Train Loss: 0.3697, Train Acc: 83.04%, Test Loss: 0.4730, Test Acc: 75.86%\n",
      "Fold 3 Epoch [83/300], Train Loss: 0.3786, Train Acc: 83.04%, Test Loss: 0.4660, Test Acc: 75.86%\n",
      "Fold 3 Epoch [84/300], Train Loss: 0.3697, Train Acc: 83.48%, Test Loss: 0.4616, Test Acc: 77.59%\n",
      "Fold 3 Epoch [85/300], Train Loss: 0.3683, Train Acc: 83.48%, Test Loss: 0.4580, Test Acc: 75.86%\n",
      "Fold 3 Epoch [86/300], Train Loss: 0.3638, Train Acc: 83.91%, Test Loss: 0.4588, Test Acc: 75.86%\n",
      "Fold 3 Epoch [87/300], Train Loss: 0.3796, Train Acc: 83.48%, Test Loss: 0.4661, Test Acc: 75.86%\n",
      "Fold 3 Epoch [88/300], Train Loss: 0.3715, Train Acc: 84.35%, Test Loss: 0.4686, Test Acc: 75.86%\n",
      "Fold 3 Epoch [89/300], Train Loss: 0.3792, Train Acc: 83.48%, Test Loss: 0.4670, Test Acc: 77.59%\n",
      "Fold 3 Epoch [90/300], Train Loss: 0.3614, Train Acc: 83.48%, Test Loss: 0.4652, Test Acc: 77.59%\n",
      "Fold 3 Epoch [91/300], Train Loss: 0.3528, Train Acc: 84.78%, Test Loss: 0.4613, Test Acc: 74.14%\n",
      "Fold 3 Epoch [92/300], Train Loss: 0.3632, Train Acc: 83.91%, Test Loss: 0.4678, Test Acc: 77.59%\n",
      "Fold 3 Epoch [93/300], Train Loss: 0.3538, Train Acc: 84.78%, Test Loss: 0.4683, Test Acc: 75.86%\n",
      "Fold 3 Epoch [94/300], Train Loss: 0.3587, Train Acc: 84.35%, Test Loss: 0.4641, Test Acc: 77.59%\n",
      "Fold 3 Epoch [95/300], Train Loss: 0.3552, Train Acc: 83.91%, Test Loss: 0.4647, Test Acc: 77.59%\n",
      "Fold 3 Epoch [96/300], Train Loss: 0.3517, Train Acc: 84.78%, Test Loss: 0.4656, Test Acc: 75.86%\n",
      "Fold 3 Epoch [97/300], Train Loss: 0.3502, Train Acc: 84.35%, Test Loss: 0.4694, Test Acc: 79.31%\n",
      "Fold 3 Epoch [98/300], Train Loss: 0.3496, Train Acc: 84.78%, Test Loss: 0.4729, Test Acc: 77.59%\n",
      "Fold 3 Epoch [99/300], Train Loss: 0.3460, Train Acc: 86.09%, Test Loss: 0.4718, Test Acc: 75.86%\n",
      "Fold 3 Epoch [100/300], Train Loss: 0.3438, Train Acc: 85.65%, Test Loss: 0.4714, Test Acc: 77.59%\n",
      "Fold 3 Epoch [101/300], Train Loss: 0.3351, Train Acc: 85.65%, Test Loss: 0.4694, Test Acc: 75.86%\n",
      "Fold 3 Epoch [102/300], Train Loss: 0.3392, Train Acc: 84.78%, Test Loss: 0.4678, Test Acc: 77.59%\n",
      "Fold 3 Epoch [103/300], Train Loss: 0.3356, Train Acc: 83.91%, Test Loss: 0.4696, Test Acc: 79.31%\n",
      "Fold 3 Epoch [104/300], Train Loss: 0.3404, Train Acc: 85.65%, Test Loss: 0.4702, Test Acc: 79.31%\n",
      "Fold 3 Epoch [105/300], Train Loss: 0.3346, Train Acc: 86.09%, Test Loss: 0.4766, Test Acc: 77.59%\n",
      "Fold 3 Epoch [106/300], Train Loss: 0.3322, Train Acc: 86.09%, Test Loss: 0.4781, Test Acc: 77.59%\n",
      "Fold 3 Epoch [107/300], Train Loss: 0.3189, Train Acc: 85.65%, Test Loss: 0.4771, Test Acc: 75.86%\n",
      "Fold 3 Epoch [108/300], Train Loss: 0.3341, Train Acc: 85.65%, Test Loss: 0.4731, Test Acc: 75.86%\n",
      "Fold 3 Epoch [109/300], Train Loss: 0.3281, Train Acc: 85.65%, Test Loss: 0.4681, Test Acc: 77.59%\n",
      "Fold 3 Epoch [110/300], Train Loss: 0.3289, Train Acc: 86.52%, Test Loss: 0.4666, Test Acc: 77.59%\n",
      "Fold 3 Epoch [111/300], Train Loss: 0.3155, Train Acc: 86.52%, Test Loss: 0.4700, Test Acc: 77.59%\n",
      "Fold 3 Epoch [112/300], Train Loss: 0.3169, Train Acc: 86.52%, Test Loss: 0.4733, Test Acc: 77.59%\n",
      "Fold 3 Epoch [113/300], Train Loss: 0.3148, Train Acc: 86.09%, Test Loss: 0.4830, Test Acc: 77.59%\n",
      "Fold 3 Epoch [114/300], Train Loss: 0.3177, Train Acc: 86.52%, Test Loss: 0.4849, Test Acc: 77.59%\n",
      "Fold 3 Epoch [115/300], Train Loss: 0.3131, Train Acc: 86.09%, Test Loss: 0.4839, Test Acc: 79.31%\n",
      "Fold 3 Epoch [116/300], Train Loss: 0.3313, Train Acc: 86.96%, Test Loss: 0.4803, Test Acc: 79.31%\n",
      "Fold 3 Epoch [117/300], Train Loss: 0.3196, Train Acc: 86.96%, Test Loss: 0.4767, Test Acc: 79.31%\n",
      "Fold 3 Epoch [118/300], Train Loss: 0.3039, Train Acc: 86.52%, Test Loss: 0.4822, Test Acc: 75.86%\n",
      "Fold 3 Epoch [119/300], Train Loss: 0.3022, Train Acc: 86.96%, Test Loss: 0.4852, Test Acc: 75.86%\n",
      "Fold 3 Epoch [120/300], Train Loss: 0.3093, Train Acc: 87.39%, Test Loss: 0.4819, Test Acc: 81.03%\n",
      "Fold 3 Epoch [121/300], Train Loss: 0.3073, Train Acc: 86.96%, Test Loss: 0.4819, Test Acc: 79.31%\n",
      "Fold 3 Epoch [122/300], Train Loss: 0.3022, Train Acc: 87.39%, Test Loss: 0.4816, Test Acc: 77.59%\n",
      "Fold 3 Epoch [123/300], Train Loss: 0.3077, Train Acc: 87.39%, Test Loss: 0.4775, Test Acc: 79.31%\n",
      "Fold 3 Epoch [124/300], Train Loss: 0.3001, Train Acc: 87.83%, Test Loss: 0.4763, Test Acc: 79.31%\n",
      "Fold 3 Epoch [125/300], Train Loss: 0.3161, Train Acc: 87.83%, Test Loss: 0.4762, Test Acc: 81.03%\n",
      "Fold 3 Epoch [126/300], Train Loss: 0.3038, Train Acc: 87.83%, Test Loss: 0.4741, Test Acc: 79.31%\n",
      "Fold 3 Epoch [127/300], Train Loss: 0.2957, Train Acc: 88.70%, Test Loss: 0.4736, Test Acc: 79.31%\n",
      "Fold 3 Epoch [128/300], Train Loss: 0.3045, Train Acc: 88.70%, Test Loss: 0.4749, Test Acc: 79.31%\n",
      "Fold 3 Epoch [129/300], Train Loss: 0.3022, Train Acc: 88.26%, Test Loss: 0.4829, Test Acc: 79.31%\n",
      "Fold 3 Epoch [130/300], Train Loss: 0.3038, Train Acc: 88.70%, Test Loss: 0.4881, Test Acc: 79.31%\n",
      "Fold 3 Epoch [131/300], Train Loss: 0.2987, Train Acc: 87.83%, Test Loss: 0.4771, Test Acc: 79.31%\n",
      "Fold 3 Epoch [132/300], Train Loss: 0.2971, Train Acc: 89.13%, Test Loss: 0.4802, Test Acc: 79.31%\n",
      "Fold 3 Epoch [133/300], Train Loss: 0.2898, Train Acc: 88.70%, Test Loss: 0.4931, Test Acc: 81.03%\n",
      "Fold 3 Epoch [134/300], Train Loss: 0.2979, Train Acc: 89.13%, Test Loss: 0.4983, Test Acc: 79.31%\n",
      "Fold 3 Epoch [135/300], Train Loss: 0.2960, Train Acc: 89.13%, Test Loss: 0.4928, Test Acc: 79.31%\n",
      "Fold 3 Epoch [136/300], Train Loss: 0.2889, Train Acc: 88.70%, Test Loss: 0.4822, Test Acc: 79.31%\n",
      "Fold 3 Epoch [137/300], Train Loss: 0.2828, Train Acc: 89.57%, Test Loss: 0.4844, Test Acc: 79.31%\n",
      "Fold 3 Epoch [138/300], Train Loss: 0.2820, Train Acc: 90.00%, Test Loss: 0.4934, Test Acc: 79.31%\n",
      "Fold 3 Epoch [139/300], Train Loss: 0.2958, Train Acc: 86.96%, Test Loss: 0.4916, Test Acc: 79.31%\n",
      "Fold 3 Epoch [140/300], Train Loss: 0.2781, Train Acc: 89.13%, Test Loss: 0.4912, Test Acc: 79.31%\n",
      "Fold 3 Epoch [141/300], Train Loss: 0.2720, Train Acc: 90.87%, Test Loss: 0.4981, Test Acc: 79.31%\n",
      "Fold 3 Epoch [142/300], Train Loss: 0.2868, Train Acc: 89.57%, Test Loss: 0.4904, Test Acc: 79.31%\n",
      "Fold 3 Epoch [143/300], Train Loss: 0.2733, Train Acc: 89.13%, Test Loss: 0.4931, Test Acc: 79.31%\n",
      "Fold 3 Epoch [144/300], Train Loss: 0.2724, Train Acc: 89.13%, Test Loss: 0.4910, Test Acc: 79.31%\n",
      "Fold 3 Epoch [145/300], Train Loss: 0.2667, Train Acc: 91.30%, Test Loss: 0.4915, Test Acc: 79.31%\n",
      "Fold 3 Epoch [146/300], Train Loss: 0.2643, Train Acc: 91.30%, Test Loss: 0.4948, Test Acc: 79.31%\n",
      "Fold 3 Epoch [147/300], Train Loss: 0.2734, Train Acc: 90.00%, Test Loss: 0.5074, Test Acc: 81.03%\n",
      "Fold 3 Epoch [148/300], Train Loss: 0.2722, Train Acc: 90.00%, Test Loss: 0.4949, Test Acc: 79.31%\n",
      "Fold 3 Epoch [149/300], Train Loss: 0.2645, Train Acc: 91.30%, Test Loss: 0.4984, Test Acc: 79.31%\n",
      "Fold 3 Epoch [150/300], Train Loss: 0.2619, Train Acc: 90.87%, Test Loss: 0.4951, Test Acc: 79.31%\n",
      "Fold 3 Epoch [151/300], Train Loss: 0.2579, Train Acc: 91.74%, Test Loss: 0.4973, Test Acc: 79.31%\n",
      "Fold 3 Epoch [152/300], Train Loss: 0.2453, Train Acc: 90.43%, Test Loss: 0.4966, Test Acc: 79.31%\n",
      "Fold 3 Epoch [153/300], Train Loss: 0.2608, Train Acc: 91.74%, Test Loss: 0.5020, Test Acc: 79.31%\n",
      "Fold 3 Epoch [154/300], Train Loss: 0.2465, Train Acc: 91.74%, Test Loss: 0.5044, Test Acc: 79.31%\n",
      "Fold 3 Epoch [155/300], Train Loss: 0.2583, Train Acc: 90.43%, Test Loss: 0.4964, Test Acc: 79.31%\n",
      "Fold 3 Epoch [156/300], Train Loss: 0.2471, Train Acc: 91.74%, Test Loss: 0.5058, Test Acc: 79.31%\n",
      "Fold 3 Epoch [157/300], Train Loss: 0.2536, Train Acc: 90.87%, Test Loss: 0.4959, Test Acc: 79.31%\n",
      "Fold 3 Epoch [158/300], Train Loss: 0.2449, Train Acc: 90.00%, Test Loss: 0.4941, Test Acc: 79.31%\n",
      "Fold 3 Epoch [159/300], Train Loss: 0.2428, Train Acc: 91.74%, Test Loss: 0.5023, Test Acc: 79.31%\n",
      "Fold 3 Epoch [160/300], Train Loss: 0.2555, Train Acc: 92.61%, Test Loss: 0.5377, Test Acc: 81.03%\n",
      "Fold 3 Epoch [161/300], Train Loss: 0.2361, Train Acc: 90.87%, Test Loss: 0.5096, Test Acc: 79.31%\n",
      "Fold 3 Epoch [162/300], Train Loss: 0.2419, Train Acc: 91.30%, Test Loss: 0.5030, Test Acc: 79.31%\n",
      "Fold 3 Epoch [163/300], Train Loss: 0.2305, Train Acc: 92.17%, Test Loss: 0.5018, Test Acc: 79.31%\n",
      "Fold 3 Epoch [164/300], Train Loss: 0.2290, Train Acc: 92.61%, Test Loss: 0.5053, Test Acc: 79.31%\n",
      "Fold 3 Epoch [165/300], Train Loss: 0.2317, Train Acc: 90.87%, Test Loss: 0.4932, Test Acc: 79.31%\n",
      "Fold 3 Epoch [166/300], Train Loss: 0.2315, Train Acc: 90.87%, Test Loss: 0.4911, Test Acc: 79.31%\n",
      "Fold 3 Epoch [167/300], Train Loss: 0.2315, Train Acc: 92.61%, Test Loss: 0.5115, Test Acc: 81.03%\n",
      "Fold 3 Epoch [168/300], Train Loss: 0.2276, Train Acc: 93.04%, Test Loss: 0.5000, Test Acc: 77.59%\n",
      "Fold 3 Epoch [169/300], Train Loss: 0.2315, Train Acc: 90.43%, Test Loss: 0.4934, Test Acc: 82.76%\n",
      "Fold 3 Epoch [170/300], Train Loss: 0.2226, Train Acc: 92.17%, Test Loss: 0.5005, Test Acc: 77.59%\n",
      "Fold 3 Epoch [171/300], Train Loss: 0.2152, Train Acc: 93.48%, Test Loss: 0.5062, Test Acc: 79.31%\n",
      "Fold 3 Epoch [172/300], Train Loss: 0.2252, Train Acc: 92.17%, Test Loss: 0.5047, Test Acc: 77.59%\n",
      "Fold 3 Epoch [173/300], Train Loss: 0.2145, Train Acc: 91.74%, Test Loss: 0.4957, Test Acc: 79.31%\n",
      "Fold 3 Epoch [174/300], Train Loss: 0.2139, Train Acc: 93.48%, Test Loss: 0.5040, Test Acc: 79.31%\n",
      "Fold 3 Epoch [175/300], Train Loss: 0.2116, Train Acc: 93.48%, Test Loss: 0.5103, Test Acc: 81.03%\n",
      "Fold 3 Epoch [176/300], Train Loss: 0.2049, Train Acc: 93.48%, Test Loss: 0.5060, Test Acc: 79.31%\n",
      "Fold 3 Epoch [177/300], Train Loss: 0.2041, Train Acc: 92.61%, Test Loss: 0.4962, Test Acc: 79.31%\n",
      "Fold 3 Epoch [178/300], Train Loss: 0.2052, Train Acc: 92.61%, Test Loss: 0.4885, Test Acc: 77.59%\n",
      "Fold 3 Epoch [179/300], Train Loss: 0.2037, Train Acc: 93.48%, Test Loss: 0.5129, Test Acc: 81.03%\n",
      "Fold 3 Epoch [180/300], Train Loss: 0.2078, Train Acc: 93.48%, Test Loss: 0.4976, Test Acc: 77.59%\n",
      "Fold 3 Epoch [181/300], Train Loss: 0.1977, Train Acc: 92.17%, Test Loss: 0.4997, Test Acc: 81.03%\n",
      "Fold 3 Epoch [182/300], Train Loss: 0.2046, Train Acc: 94.35%, Test Loss: 0.5232, Test Acc: 81.03%\n",
      "Fold 3 Epoch [183/300], Train Loss: 0.1969, Train Acc: 93.48%, Test Loss: 0.5051, Test Acc: 79.31%\n",
      "Fold 3 Epoch [184/300], Train Loss: 0.2017, Train Acc: 91.74%, Test Loss: 0.4884, Test Acc: 84.48%\n",
      "Fold 3 Epoch [185/300], Train Loss: 0.1986, Train Acc: 94.35%, Test Loss: 0.5101, Test Acc: 81.03%\n",
      "Fold 3 Epoch [186/300], Train Loss: 0.1986, Train Acc: 94.78%, Test Loss: 0.5104, Test Acc: 81.03%\n",
      "Fold 3 Epoch [187/300], Train Loss: 0.2038, Train Acc: 90.87%, Test Loss: 0.5053, Test Acc: 82.76%\n",
      "Fold 3 Epoch [188/300], Train Loss: 0.1904, Train Acc: 94.78%, Test Loss: 0.5238, Test Acc: 79.31%\n",
      "Fold 3 Epoch [189/300], Train Loss: 0.1830, Train Acc: 95.22%, Test Loss: 0.5267, Test Acc: 81.03%\n",
      "Fold 3 Epoch [190/300], Train Loss: 0.2013, Train Acc: 92.17%, Test Loss: 0.4938, Test Acc: 82.76%\n",
      "Fold 3 Epoch [191/300], Train Loss: 0.1758, Train Acc: 93.48%, Test Loss: 0.4899, Test Acc: 79.31%\n",
      "Fold 3 Epoch [192/300], Train Loss: 0.1759, Train Acc: 95.22%, Test Loss: 0.4936, Test Acc: 79.31%\n",
      "Fold 3 Epoch [193/300], Train Loss: 0.1801, Train Acc: 93.91%, Test Loss: 0.4870, Test Acc: 82.76%\n",
      "Fold 3 Epoch [194/300], Train Loss: 0.1748, Train Acc: 95.22%, Test Loss: 0.5210, Test Acc: 81.03%\n",
      "Fold 3 Epoch [195/300], Train Loss: 0.1671, Train Acc: 95.22%, Test Loss: 0.5115, Test Acc: 79.31%\n",
      "Fold 3 Epoch [196/300], Train Loss: 0.1765, Train Acc: 94.78%, Test Loss: 0.4981, Test Acc: 82.76%\n",
      "Fold 3 Epoch [197/300], Train Loss: 0.1828, Train Acc: 95.22%, Test Loss: 0.5049, Test Acc: 81.03%\n",
      "Fold 3 Epoch [198/300], Train Loss: 0.1624, Train Acc: 94.78%, Test Loss: 0.4879, Test Acc: 82.76%\n",
      "Fold 3 Epoch [199/300], Train Loss: 0.1646, Train Acc: 95.65%, Test Loss: 0.4959, Test Acc: 81.03%\n",
      "Fold 3 Epoch [200/300], Train Loss: 0.1632, Train Acc: 95.65%, Test Loss: 0.4945, Test Acc: 81.03%\n",
      "Fold 3 Epoch [201/300], Train Loss: 0.1651, Train Acc: 93.91%, Test Loss: 0.4897, Test Acc: 84.48%\n",
      "Fold 3 Epoch [202/300], Train Loss: 0.1630, Train Acc: 95.65%, Test Loss: 0.5007, Test Acc: 84.48%\n",
      "Fold 3 Epoch [203/300], Train Loss: 0.1662, Train Acc: 95.22%, Test Loss: 0.5164, Test Acc: 82.76%\n",
      "Fold 3 Epoch [204/300], Train Loss: 0.1565, Train Acc: 96.52%, Test Loss: 0.4827, Test Acc: 84.48%\n",
      "Fold 3 Epoch [205/300], Train Loss: 0.1563, Train Acc: 96.96%, Test Loss: 0.4784, Test Acc: 84.48%\n",
      "Fold 3 Epoch [206/300], Train Loss: 0.1558, Train Acc: 96.52%, Test Loss: 0.4853, Test Acc: 82.76%\n",
      "Fold 3 Epoch [207/300], Train Loss: 0.1484, Train Acc: 96.52%, Test Loss: 0.4898, Test Acc: 84.48%\n",
      "Fold 3 Epoch [208/300], Train Loss: 0.1555, Train Acc: 95.65%, Test Loss: 0.5206, Test Acc: 82.76%\n",
      "Fold 3 Epoch [209/300], Train Loss: 0.1498, Train Acc: 95.65%, Test Loss: 0.4910, Test Acc: 84.48%\n",
      "Fold 3 Epoch [210/300], Train Loss: 0.1490, Train Acc: 96.52%, Test Loss: 0.4920, Test Acc: 84.48%\n",
      "Fold 3 Epoch [211/300], Train Loss: 0.1462, Train Acc: 96.52%, Test Loss: 0.5177, Test Acc: 82.76%\n",
      "Fold 3 Epoch [212/300], Train Loss: 0.1440, Train Acc: 95.65%, Test Loss: 0.4871, Test Acc: 82.76%\n",
      "Fold 3 Epoch [213/300], Train Loss: 0.1466, Train Acc: 96.52%, Test Loss: 0.5062, Test Acc: 84.48%\n",
      "Fold 3 Epoch [214/300], Train Loss: 0.1512, Train Acc: 95.22%, Test Loss: 0.5427, Test Acc: 82.76%\n",
      "Fold 3 Epoch [215/300], Train Loss: 0.1358, Train Acc: 96.96%, Test Loss: 0.5008, Test Acc: 84.48%\n",
      "Fold 3 Epoch [216/300], Train Loss: 0.1370, Train Acc: 96.96%, Test Loss: 0.4963, Test Acc: 84.48%\n",
      "Fold 3 Epoch [217/300], Train Loss: 0.1309, Train Acc: 96.96%, Test Loss: 0.5147, Test Acc: 84.48%\n",
      "Fold 3 Epoch [218/300], Train Loss: 0.1382, Train Acc: 97.39%, Test Loss: 0.5240, Test Acc: 82.76%\n",
      "Fold 3 Epoch [219/300], Train Loss: 0.1288, Train Acc: 97.39%, Test Loss: 0.5314, Test Acc: 79.31%\n",
      "Fold 3 Epoch [220/300], Train Loss: 0.1366, Train Acc: 96.52%, Test Loss: 0.5050, Test Acc: 84.48%\n",
      "Fold 3 Epoch [221/300], Train Loss: 0.1334, Train Acc: 97.39%, Test Loss: 0.5116, Test Acc: 82.76%\n",
      "Fold 3 Epoch [222/300], Train Loss: 0.1329, Train Acc: 97.39%, Test Loss: 0.4961, Test Acc: 84.48%\n",
      "Fold 3 Epoch [223/300], Train Loss: 0.1319, Train Acc: 97.39%, Test Loss: 0.5063, Test Acc: 81.03%\n",
      "Fold 3 Epoch [224/300], Train Loss: 0.1248, Train Acc: 97.39%, Test Loss: 0.5096, Test Acc: 82.76%\n",
      "Fold 3 Epoch [225/300], Train Loss: 0.1186, Train Acc: 97.39%, Test Loss: 0.5138, Test Acc: 84.48%\n",
      "Fold 3 Epoch [226/300], Train Loss: 0.1303, Train Acc: 97.39%, Test Loss: 0.5130, Test Acc: 84.48%\n",
      "Fold 3 Epoch [227/300], Train Loss: 0.1276, Train Acc: 97.39%, Test Loss: 0.5404, Test Acc: 82.76%\n",
      "Fold 3 Epoch [228/300], Train Loss: 0.1203, Train Acc: 97.39%, Test Loss: 0.5542, Test Acc: 81.03%\n",
      "Fold 3 Epoch [229/300], Train Loss: 0.1279, Train Acc: 96.96%, Test Loss: 0.5090, Test Acc: 82.76%\n",
      "Fold 3 Epoch [230/300], Train Loss: 0.1110, Train Acc: 97.83%, Test Loss: 0.5251, Test Acc: 82.76%\n",
      "Fold 3 Epoch [231/300], Train Loss: 0.1226, Train Acc: 97.39%, Test Loss: 0.5097, Test Acc: 82.76%\n",
      "Fold 3 Epoch [232/300], Train Loss: 0.1101, Train Acc: 98.26%, Test Loss: 0.5231, Test Acc: 82.76%\n",
      "Fold 3 Epoch [233/300], Train Loss: 0.1098, Train Acc: 97.83%, Test Loss: 0.5323, Test Acc: 82.76%\n",
      "Fold 3 Epoch [234/300], Train Loss: 0.1066, Train Acc: 97.83%, Test Loss: 0.5142, Test Acc: 82.76%\n",
      "Fold 3 Epoch [235/300], Train Loss: 0.1055, Train Acc: 98.26%, Test Loss: 0.5191, Test Acc: 82.76%\n",
      "Fold 3 Epoch [236/300], Train Loss: 0.1071, Train Acc: 97.83%, Test Loss: 0.5401, Test Acc: 81.03%\n",
      "Fold 3 Epoch [237/300], Train Loss: 0.1075, Train Acc: 97.39%, Test Loss: 0.5078, Test Acc: 84.48%\n",
      "Fold 3 Epoch [238/300], Train Loss: 0.1077, Train Acc: 98.26%, Test Loss: 0.5425, Test Acc: 81.03%\n",
      "Fold 3 Epoch [239/300], Train Loss: 0.1063, Train Acc: 98.70%, Test Loss: 0.5414, Test Acc: 81.03%\n",
      "Fold 3 Epoch [240/300], Train Loss: 0.1014, Train Acc: 97.83%, Test Loss: 0.5036, Test Acc: 82.76%\n",
      "Fold 3 Epoch [241/300], Train Loss: 0.1041, Train Acc: 98.70%, Test Loss: 0.5108, Test Acc: 82.76%\n",
      "Fold 3 Epoch [242/300], Train Loss: 0.0989, Train Acc: 99.13%, Test Loss: 0.5223, Test Acc: 82.76%\n",
      "Fold 3 Epoch [243/300], Train Loss: 0.0982, Train Acc: 97.83%, Test Loss: 0.5161, Test Acc: 82.76%\n",
      "Fold 3 Epoch [244/300], Train Loss: 0.1016, Train Acc: 98.26%, Test Loss: 0.5220, Test Acc: 82.76%\n",
      "Fold 3 Epoch [245/300], Train Loss: 0.1000, Train Acc: 98.70%, Test Loss: 0.5341, Test Acc: 82.76%\n",
      "Fold 3 Epoch [246/300], Train Loss: 0.1041, Train Acc: 98.70%, Test Loss: 0.5400, Test Acc: 82.76%\n",
      "Fold 3 Epoch [247/300], Train Loss: 0.0984, Train Acc: 97.39%, Test Loss: 0.5169, Test Acc: 81.03%\n",
      "Fold 3 Epoch [248/300], Train Loss: 0.0918, Train Acc: 98.70%, Test Loss: 0.5453, Test Acc: 82.76%\n",
      "Fold 3 Epoch [249/300], Train Loss: 0.0968, Train Acc: 97.39%, Test Loss: 0.5238, Test Acc: 82.76%\n",
      "Fold 3 Epoch [250/300], Train Loss: 0.0895, Train Acc: 99.13%, Test Loss: 0.5565, Test Acc: 82.76%\n",
      "Fold 3 Epoch [251/300], Train Loss: 0.0874, Train Acc: 98.70%, Test Loss: 0.5402, Test Acc: 82.76%\n",
      "Fold 3 Epoch [252/300], Train Loss: 0.0831, Train Acc: 99.13%, Test Loss: 0.5519, Test Acc: 82.76%\n",
      "Fold 3 Epoch [253/300], Train Loss: 0.0844, Train Acc: 99.13%, Test Loss: 0.5375, Test Acc: 82.76%\n",
      "Fold 3 Epoch [254/300], Train Loss: 0.0809, Train Acc: 99.13%, Test Loss: 0.5507, Test Acc: 82.76%\n",
      "Fold 3 Epoch [255/300], Train Loss: 0.0907, Train Acc: 99.13%, Test Loss: 0.5531, Test Acc: 82.76%\n",
      "Fold 3 Epoch [256/300], Train Loss: 0.0840, Train Acc: 99.13%, Test Loss: 0.5365, Test Acc: 82.76%\n",
      "Fold 3 Epoch [257/300], Train Loss: 0.0819, Train Acc: 99.13%, Test Loss: 0.5473, Test Acc: 82.76%\n",
      "Fold 3 Epoch [258/300], Train Loss: 0.0832, Train Acc: 99.13%, Test Loss: 0.5420, Test Acc: 82.76%\n",
      "Fold 3 Epoch [259/300], Train Loss: 0.0785, Train Acc: 99.13%, Test Loss: 0.5269, Test Acc: 82.76%\n",
      "Fold 3 Epoch [260/300], Train Loss: 0.0852, Train Acc: 99.13%, Test Loss: 0.5375, Test Acc: 82.76%\n",
      "Fold 3 Epoch [261/300], Train Loss: 0.0795, Train Acc: 99.13%, Test Loss: 0.5486, Test Acc: 82.76%\n",
      "Fold 3 Epoch [262/300], Train Loss: 0.0811, Train Acc: 99.13%, Test Loss: 0.5623, Test Acc: 82.76%\n",
      "Fold 3 Epoch [263/300], Train Loss: 0.0792, Train Acc: 99.13%, Test Loss: 0.5338, Test Acc: 82.76%\n",
      "Fold 3 Epoch [264/300], Train Loss: 0.0719, Train Acc: 99.13%, Test Loss: 0.5282, Test Acc: 82.76%\n",
      "Fold 3 Epoch [265/300], Train Loss: 0.0840, Train Acc: 98.70%, Test Loss: 0.5729, Test Acc: 82.76%\n",
      "Fold 3 Epoch [266/300], Train Loss: 0.0727, Train Acc: 99.13%, Test Loss: 0.5468, Test Acc: 82.76%\n",
      "Fold 3 Epoch [267/300], Train Loss: 0.0777, Train Acc: 97.83%, Test Loss: 0.5237, Test Acc: 82.76%\n",
      "Fold 3 Epoch [268/300], Train Loss: 0.0793, Train Acc: 99.13%, Test Loss: 0.5699, Test Acc: 82.76%\n",
      "Fold 3 Epoch [269/300], Train Loss: 0.0707, Train Acc: 99.13%, Test Loss: 0.5565, Test Acc: 82.76%\n",
      "Fold 3 Epoch [270/300], Train Loss: 0.0681, Train Acc: 99.13%, Test Loss: 0.5488, Test Acc: 82.76%\n",
      "Fold 3 Epoch [271/300], Train Loss: 0.0716, Train Acc: 99.13%, Test Loss: 0.5649, Test Acc: 82.76%\n",
      "Fold 3 Epoch [272/300], Train Loss: 0.0733, Train Acc: 99.13%, Test Loss: 0.5435, Test Acc: 81.03%\n",
      "Fold 3 Epoch [273/300], Train Loss: 0.0663, Train Acc: 99.13%, Test Loss: 0.5560, Test Acc: 82.76%\n",
      "Fold 3 Epoch [274/300], Train Loss: 0.0671, Train Acc: 99.13%, Test Loss: 0.5499, Test Acc: 82.76%\n",
      "Fold 3 Epoch [275/300], Train Loss: 0.0638, Train Acc: 99.13%, Test Loss: 0.5218, Test Acc: 82.76%\n",
      "Fold 3 Epoch [276/300], Train Loss: 0.0714, Train Acc: 99.13%, Test Loss: 0.5642, Test Acc: 84.48%\n",
      "Fold 3 Epoch [277/300], Train Loss: 0.0661, Train Acc: 99.13%, Test Loss: 0.5158, Test Acc: 82.76%\n",
      "Fold 3 Epoch [278/300], Train Loss: 0.0715, Train Acc: 99.13%, Test Loss: 0.5410, Test Acc: 84.48%\n",
      "Fold 3 Epoch [279/300], Train Loss: 0.0643, Train Acc: 99.13%, Test Loss: 0.5385, Test Acc: 82.76%\n",
      "Fold 3 Epoch [280/300], Train Loss: 0.0661, Train Acc: 99.13%, Test Loss: 0.5639, Test Acc: 84.48%\n",
      "Fold 3 Epoch [281/300], Train Loss: 0.0654, Train Acc: 99.13%, Test Loss: 0.5809, Test Acc: 84.48%\n",
      "Fold 3 Epoch [282/300], Train Loss: 0.0660, Train Acc: 99.13%, Test Loss: 0.5430, Test Acc: 82.76%\n",
      "Fold 3 Epoch [283/300], Train Loss: 0.0661, Train Acc: 98.70%, Test Loss: 0.6000, Test Acc: 82.76%\n",
      "Fold 3 Epoch [284/300], Train Loss: 0.0666, Train Acc: 99.13%, Test Loss: 0.5844, Test Acc: 84.48%\n",
      "Fold 3 Epoch [285/300], Train Loss: 0.0661, Train Acc: 99.13%, Test Loss: 0.5367, Test Acc: 81.03%\n",
      "Fold 3 Epoch [286/300], Train Loss: 0.0672, Train Acc: 98.70%, Test Loss: 0.5856, Test Acc: 82.76%\n",
      "Fold 3 Epoch [287/300], Train Loss: 0.0596, Train Acc: 99.13%, Test Loss: 0.5487, Test Acc: 84.48%\n",
      "Fold 3 Epoch [288/300], Train Loss: 0.0630, Train Acc: 99.13%, Test Loss: 0.5263, Test Acc: 82.76%\n",
      "Fold 3 Epoch [289/300], Train Loss: 0.0609, Train Acc: 99.13%, Test Loss: 0.5451, Test Acc: 82.76%\n",
      "Fold 3 Epoch [290/300], Train Loss: 0.0606, Train Acc: 99.13%, Test Loss: 0.5612, Test Acc: 82.76%\n",
      "Fold 3 Epoch [291/300], Train Loss: 0.0555, Train Acc: 99.13%, Test Loss: 0.5555, Test Acc: 82.76%\n",
      "Fold 3 Epoch [292/300], Train Loss: 0.0529, Train Acc: 99.13%, Test Loss: 0.5817, Test Acc: 82.76%\n",
      "Fold 3 Epoch [293/300], Train Loss: 0.0581, Train Acc: 99.13%, Test Loss: 0.6012, Test Acc: 84.48%\n",
      "Fold 3 Epoch [294/300], Train Loss: 0.0553, Train Acc: 99.13%, Test Loss: 0.5795, Test Acc: 82.76%\n",
      "Fold 3 Epoch [295/300], Train Loss: 0.0480, Train Acc: 99.13%, Test Loss: 0.5760, Test Acc: 82.76%\n",
      "Fold 3 Epoch [296/300], Train Loss: 0.0564, Train Acc: 98.70%, Test Loss: 0.6347, Test Acc: 82.76%\n",
      "Fold 3 Epoch [297/300], Train Loss: 0.0486, Train Acc: 99.13%, Test Loss: 0.5705, Test Acc: 82.76%\n",
      "Fold 3 Epoch [298/300], Train Loss: 0.0563, Train Acc: 99.13%, Test Loss: 0.5606, Test Acc: 81.03%\n",
      "Fold 3 Epoch [299/300], Train Loss: 0.0542, Train Acc: 98.70%, Test Loss: 0.6341, Test Acc: 82.76%\n",
      "Fold 3 Epoch [300/300], Train Loss: 0.0486, Train Acc: 99.13%, Test Loss: 0.5699, Test Acc: 82.76%\n",
      "Fold 3 Best Accuracy: 84.48%\n",
      "Fold 4/5\n",
      "Fold 4 Epoch [1/300], Train Loss: 2.0222, Train Acc: 29.44%, Test Loss: 2.0270, Test Acc: 26.32%\n",
      "Fold 4 Epoch [2/300], Train Loss: 1.9425, Train Acc: 32.47%, Test Loss: 1.9496, Test Acc: 28.07%\n",
      "Fold 4 Epoch [3/300], Train Loss: 1.8665, Train Acc: 36.80%, Test Loss: 1.8763, Test Acc: 31.58%\n",
      "Fold 4 Epoch [4/300], Train Loss: 1.7946, Train Acc: 38.96%, Test Loss: 1.8068, Test Acc: 35.09%\n",
      "Fold 4 Epoch [5/300], Train Loss: 1.7268, Train Acc: 45.89%, Test Loss: 1.7401, Test Acc: 43.86%\n",
      "Fold 4 Epoch [6/300], Train Loss: 1.6588, Train Acc: 50.22%, Test Loss: 1.6750, Test Acc: 43.86%\n",
      "Fold 4 Epoch [7/300], Train Loss: 1.5891, Train Acc: 51.95%, Test Loss: 1.6121, Test Acc: 47.37%\n",
      "Fold 4 Epoch [8/300], Train Loss: 1.5342, Train Acc: 58.01%, Test Loss: 1.5523, Test Acc: 50.88%\n",
      "Fold 4 Epoch [9/300], Train Loss: 1.4677, Train Acc: 60.17%, Test Loss: 1.4957, Test Acc: 52.63%\n",
      "Fold 4 Epoch [10/300], Train Loss: 1.4201, Train Acc: 61.47%, Test Loss: 1.4414, Test Acc: 54.39%\n",
      "Fold 4 Epoch [11/300], Train Loss: 1.3713, Train Acc: 62.34%, Test Loss: 1.3899, Test Acc: 54.39%\n",
      "Fold 4 Epoch [12/300], Train Loss: 1.3150, Train Acc: 62.77%, Test Loss: 1.3389, Test Acc: 50.88%\n",
      "Fold 4 Epoch [13/300], Train Loss: 1.2732, Train Acc: 62.77%, Test Loss: 1.2918, Test Acc: 56.14%\n",
      "Fold 4 Epoch [14/300], Train Loss: 1.2270, Train Acc: 63.64%, Test Loss: 1.2482, Test Acc: 59.65%\n",
      "Fold 4 Epoch [15/300], Train Loss: 1.1892, Train Acc: 67.10%, Test Loss: 1.2062, Test Acc: 57.89%\n",
      "Fold 4 Epoch [16/300], Train Loss: 1.1424, Train Acc: 70.56%, Test Loss: 1.1660, Test Acc: 66.67%\n",
      "Fold 4 Epoch [17/300], Train Loss: 1.1027, Train Acc: 71.86%, Test Loss: 1.1269, Test Acc: 71.93%\n",
      "Fold 4 Epoch [18/300], Train Loss: 1.0614, Train Acc: 71.43%, Test Loss: 1.0891, Test Acc: 71.93%\n",
      "Fold 4 Epoch [19/300], Train Loss: 1.0236, Train Acc: 74.89%, Test Loss: 1.0526, Test Acc: 75.44%\n",
      "Fold 4 Epoch [20/300], Train Loss: 0.9844, Train Acc: 75.32%, Test Loss: 1.0156, Test Acc: 77.19%\n",
      "Fold 4 Epoch [21/300], Train Loss: 0.9458, Train Acc: 74.46%, Test Loss: 0.9800, Test Acc: 77.19%\n",
      "Fold 4 Epoch [22/300], Train Loss: 0.9075, Train Acc: 74.46%, Test Loss: 0.9465, Test Acc: 75.44%\n",
      "Fold 4 Epoch [23/300], Train Loss: 0.8801, Train Acc: 74.89%, Test Loss: 0.9129, Test Acc: 75.44%\n",
      "Fold 4 Epoch [24/300], Train Loss: 0.8419, Train Acc: 75.32%, Test Loss: 0.8799, Test Acc: 78.95%\n",
      "Fold 4 Epoch [25/300], Train Loss: 0.8062, Train Acc: 73.16%, Test Loss: 0.8451, Test Acc: 80.70%\n",
      "Fold 4 Epoch [26/300], Train Loss: 0.7745, Train Acc: 73.59%, Test Loss: 0.8132, Test Acc: 80.70%\n",
      "Fold 4 Epoch [27/300], Train Loss: 0.7508, Train Acc: 73.16%, Test Loss: 0.7824, Test Acc: 82.46%\n",
      "Fold 4 Epoch [28/300], Train Loss: 0.7211, Train Acc: 73.59%, Test Loss: 0.7539, Test Acc: 82.46%\n",
      "Fold 4 Epoch [29/300], Train Loss: 0.7001, Train Acc: 73.59%, Test Loss: 0.7283, Test Acc: 80.70%\n",
      "Fold 4 Epoch [30/300], Train Loss: 0.6758, Train Acc: 74.03%, Test Loss: 0.7056, Test Acc: 82.46%\n",
      "Fold 4 Epoch [31/300], Train Loss: 0.6598, Train Acc: 74.46%, Test Loss: 0.6841, Test Acc: 82.46%\n",
      "Fold 4 Epoch [32/300], Train Loss: 0.6421, Train Acc: 75.32%, Test Loss: 0.6634, Test Acc: 80.70%\n",
      "Fold 4 Epoch [33/300], Train Loss: 0.6200, Train Acc: 74.46%, Test Loss: 0.6475, Test Acc: 84.21%\n",
      "Fold 4 Epoch [34/300], Train Loss: 0.6019, Train Acc: 75.32%, Test Loss: 0.6325, Test Acc: 84.21%\n",
      "Fold 4 Epoch [35/300], Train Loss: 0.5900, Train Acc: 75.76%, Test Loss: 0.6179, Test Acc: 85.96%\n",
      "Fold 4 Epoch [36/300], Train Loss: 0.5711, Train Acc: 75.76%, Test Loss: 0.6056, Test Acc: 80.70%\n",
      "Fold 4 Epoch [37/300], Train Loss: 0.5604, Train Acc: 75.76%, Test Loss: 0.5934, Test Acc: 84.21%\n",
      "Fold 4 Epoch [38/300], Train Loss: 0.5450, Train Acc: 76.19%, Test Loss: 0.5818, Test Acc: 80.70%\n",
      "Fold 4 Epoch [39/300], Train Loss: 0.5371, Train Acc: 75.76%, Test Loss: 0.5700, Test Acc: 78.95%\n",
      "Fold 4 Epoch [40/300], Train Loss: 0.5319, Train Acc: 76.62%, Test Loss: 0.5577, Test Acc: 80.70%\n",
      "Fold 4 Epoch [41/300], Train Loss: 0.5219, Train Acc: 76.62%, Test Loss: 0.5488, Test Acc: 84.21%\n",
      "Fold 4 Epoch [42/300], Train Loss: 0.5190, Train Acc: 76.62%, Test Loss: 0.5407, Test Acc: 84.21%\n",
      "Fold 4 Epoch [43/300], Train Loss: 0.5046, Train Acc: 76.62%, Test Loss: 0.5340, Test Acc: 80.70%\n",
      "Fold 4 Epoch [44/300], Train Loss: 0.4952, Train Acc: 78.79%, Test Loss: 0.5268, Test Acc: 84.21%\n",
      "Fold 4 Epoch [45/300], Train Loss: 0.4920, Train Acc: 79.65%, Test Loss: 0.5189, Test Acc: 85.96%\n",
      "Fold 4 Epoch [46/300], Train Loss: 0.4861, Train Acc: 80.09%, Test Loss: 0.5117, Test Acc: 87.72%\n",
      "Fold 4 Epoch [47/300], Train Loss: 0.4790, Train Acc: 81.39%, Test Loss: 0.5061, Test Acc: 84.21%\n",
      "Fold 4 Epoch [48/300], Train Loss: 0.4748, Train Acc: 81.82%, Test Loss: 0.5016, Test Acc: 87.72%\n",
      "Fold 4 Epoch [49/300], Train Loss: 0.4674, Train Acc: 81.82%, Test Loss: 0.4970, Test Acc: 87.72%\n",
      "Fold 4 Epoch [50/300], Train Loss: 0.4630, Train Acc: 81.39%, Test Loss: 0.4973, Test Acc: 87.72%\n",
      "Fold 4 Epoch [51/300], Train Loss: 0.4543, Train Acc: 82.25%, Test Loss: 0.4898, Test Acc: 87.72%\n",
      "Fold 4 Epoch [52/300], Train Loss: 0.4550, Train Acc: 78.35%, Test Loss: 0.4850, Test Acc: 82.46%\n",
      "Fold 4 Epoch [53/300], Train Loss: 0.4476, Train Acc: 83.12%, Test Loss: 0.4841, Test Acc: 85.96%\n",
      "Fold 4 Epoch [54/300], Train Loss: 0.4412, Train Acc: 83.12%, Test Loss: 0.4850, Test Acc: 85.96%\n",
      "Fold 4 Epoch [55/300], Train Loss: 0.4352, Train Acc: 81.39%, Test Loss: 0.4668, Test Acc: 84.21%\n",
      "Fold 4 Epoch [56/300], Train Loss: 0.4344, Train Acc: 79.65%, Test Loss: 0.4591, Test Acc: 82.46%\n",
      "Fold 4 Epoch [57/300], Train Loss: 0.4278, Train Acc: 82.68%, Test Loss: 0.4538, Test Acc: 87.72%\n",
      "Fold 4 Epoch [58/300], Train Loss: 0.4319, Train Acc: 83.55%, Test Loss: 0.4578, Test Acc: 84.21%\n",
      "Fold 4 Epoch [59/300], Train Loss: 0.4115, Train Acc: 83.98%, Test Loss: 0.4576, Test Acc: 82.46%\n",
      "Fold 4 Epoch [60/300], Train Loss: 0.4130, Train Acc: 82.25%, Test Loss: 0.4496, Test Acc: 85.96%\n",
      "Fold 4 Epoch [61/300], Train Loss: 0.4130, Train Acc: 80.09%, Test Loss: 0.4456, Test Acc: 84.21%\n",
      "Fold 4 Epoch [62/300], Train Loss: 0.4117, Train Acc: 82.68%, Test Loss: 0.4415, Test Acc: 85.96%\n",
      "Fold 4 Epoch [63/300], Train Loss: 0.4026, Train Acc: 84.85%, Test Loss: 0.4429, Test Acc: 82.46%\n",
      "Fold 4 Epoch [64/300], Train Loss: 0.3970, Train Acc: 84.85%, Test Loss: 0.4449, Test Acc: 80.70%\n",
      "Fold 4 Epoch [65/300], Train Loss: 0.3994, Train Acc: 84.85%, Test Loss: 0.4332, Test Acc: 80.70%\n",
      "Fold 4 Epoch [66/300], Train Loss: 0.3986, Train Acc: 83.98%, Test Loss: 0.4300, Test Acc: 89.47%\n",
      "Fold 4 Epoch [67/300], Train Loss: 0.3941, Train Acc: 85.71%, Test Loss: 0.4384, Test Acc: 78.95%\n",
      "Fold 4 Epoch [68/300], Train Loss: 0.3907, Train Acc: 86.15%, Test Loss: 0.4359, Test Acc: 78.95%\n",
      "Fold 4 Epoch [69/300], Train Loss: 0.3882, Train Acc: 85.71%, Test Loss: 0.4237, Test Acc: 82.46%\n",
      "Fold 4 Epoch [70/300], Train Loss: 0.3839, Train Acc: 85.71%, Test Loss: 0.4191, Test Acc: 82.46%\n",
      "Fold 4 Epoch [71/300], Train Loss: 0.3808, Train Acc: 85.71%, Test Loss: 0.4239, Test Acc: 80.70%\n",
      "Fold 4 Epoch [72/300], Train Loss: 0.3814, Train Acc: 85.71%, Test Loss: 0.4214, Test Acc: 84.21%\n",
      "Fold 4 Epoch [73/300], Train Loss: 0.3815, Train Acc: 86.15%, Test Loss: 0.4218, Test Acc: 82.46%\n",
      "Fold 4 Epoch [74/300], Train Loss: 0.3679, Train Acc: 87.01%, Test Loss: 0.4230, Test Acc: 78.95%\n",
      "Fold 4 Epoch [75/300], Train Loss: 0.3583, Train Acc: 84.85%, Test Loss: 0.4214, Test Acc: 85.96%\n",
      "Fold 4 Epoch [76/300], Train Loss: 0.3617, Train Acc: 83.98%, Test Loss: 0.4216, Test Acc: 85.96%\n",
      "Fold 4 Epoch [77/300], Train Loss: 0.3626, Train Acc: 86.58%, Test Loss: 0.4293, Test Acc: 82.46%\n",
      "Fold 4 Epoch [78/300], Train Loss: 0.3631, Train Acc: 87.01%, Test Loss: 0.4280, Test Acc: 78.95%\n",
      "Fold 4 Epoch [79/300], Train Loss: 0.3631, Train Acc: 85.28%, Test Loss: 0.4145, Test Acc: 82.46%\n",
      "Fold 4 Epoch [80/300], Train Loss: 0.3696, Train Acc: 85.71%, Test Loss: 0.4115, Test Acc: 85.96%\n",
      "Fold 4 Epoch [81/300], Train Loss: 0.3568, Train Acc: 86.15%, Test Loss: 0.4132, Test Acc: 85.96%\n",
      "Fold 4 Epoch [82/300], Train Loss: 0.3475, Train Acc: 86.58%, Test Loss: 0.4223, Test Acc: 80.70%\n",
      "Fold 4 Epoch [83/300], Train Loss: 0.3350, Train Acc: 85.28%, Test Loss: 0.4195, Test Acc: 80.70%\n",
      "Fold 4 Epoch [84/300], Train Loss: 0.3412, Train Acc: 87.01%, Test Loss: 0.4072, Test Acc: 85.96%\n",
      "Fold 4 Epoch [85/300], Train Loss: 0.3417, Train Acc: 86.15%, Test Loss: 0.4063, Test Acc: 84.21%\n",
      "Fold 4 Epoch [86/300], Train Loss: 0.3332, Train Acc: 86.58%, Test Loss: 0.4047, Test Acc: 82.46%\n",
      "Fold 4 Epoch [87/300], Train Loss: 0.3330, Train Acc: 86.58%, Test Loss: 0.3967, Test Acc: 85.96%\n",
      "Fold 4 Epoch [88/300], Train Loss: 0.3213, Train Acc: 87.45%, Test Loss: 0.3949, Test Acc: 84.21%\n",
      "Fold 4 Epoch [89/300], Train Loss: 0.3262, Train Acc: 86.58%, Test Loss: 0.3893, Test Acc: 85.96%\n",
      "Fold 4 Epoch [90/300], Train Loss: 0.3251, Train Acc: 86.15%, Test Loss: 0.3894, Test Acc: 85.96%\n",
      "Fold 4 Epoch [91/300], Train Loss: 0.3158, Train Acc: 87.01%, Test Loss: 0.3894, Test Acc: 84.21%\n",
      "Fold 4 Epoch [92/300], Train Loss: 0.3104, Train Acc: 86.15%, Test Loss: 0.3916, Test Acc: 84.21%\n",
      "Fold 4 Epoch [93/300], Train Loss: 0.3164, Train Acc: 87.88%, Test Loss: 0.4060, Test Acc: 78.95%\n",
      "Fold 4 Epoch [94/300], Train Loss: 0.3074, Train Acc: 86.58%, Test Loss: 0.3928, Test Acc: 84.21%\n",
      "Fold 4 Epoch [95/300], Train Loss: 0.3221, Train Acc: 87.45%, Test Loss: 0.3911, Test Acc: 85.96%\n",
      "Fold 4 Epoch [96/300], Train Loss: 0.2988, Train Acc: 87.01%, Test Loss: 0.3860, Test Acc: 84.21%\n",
      "Fold 4 Epoch [97/300], Train Loss: 0.2948, Train Acc: 87.88%, Test Loss: 0.3969, Test Acc: 80.70%\n",
      "Fold 4 Epoch [98/300], Train Loss: 0.2918, Train Acc: 87.45%, Test Loss: 0.3918, Test Acc: 84.21%\n",
      "Fold 4 Epoch [99/300], Train Loss: 0.2915, Train Acc: 89.18%, Test Loss: 0.3935, Test Acc: 82.46%\n",
      "Fold 4 Epoch [100/300], Train Loss: 0.2785, Train Acc: 88.31%, Test Loss: 0.3953, Test Acc: 82.46%\n",
      "Fold 4 Epoch [101/300], Train Loss: 0.2796, Train Acc: 89.18%, Test Loss: 0.3901, Test Acc: 84.21%\n",
      "Fold 4 Epoch [102/300], Train Loss: 0.2727, Train Acc: 89.61%, Test Loss: 0.3868, Test Acc: 85.96%\n",
      "Fold 4 Epoch [103/300], Train Loss: 0.2731, Train Acc: 88.74%, Test Loss: 0.3901, Test Acc: 85.96%\n",
      "Fold 4 Epoch [104/300], Train Loss: 0.2726, Train Acc: 90.04%, Test Loss: 0.3930, Test Acc: 84.21%\n",
      "Fold 4 Epoch [105/300], Train Loss: 0.2624, Train Acc: 90.91%, Test Loss: 0.3969, Test Acc: 82.46%\n",
      "Fold 4 Epoch [106/300], Train Loss: 0.2641, Train Acc: 92.21%, Test Loss: 0.4004, Test Acc: 80.70%\n",
      "Fold 4 Epoch [107/300], Train Loss: 0.2544, Train Acc: 89.61%, Test Loss: 0.3973, Test Acc: 84.21%\n",
      "Fold 4 Epoch [108/300], Train Loss: 0.2593, Train Acc: 92.21%, Test Loss: 0.3882, Test Acc: 82.46%\n",
      "Fold 4 Epoch [109/300], Train Loss: 0.2587, Train Acc: 91.77%, Test Loss: 0.3874, Test Acc: 82.46%\n",
      "Fold 4 Epoch [110/300], Train Loss: 0.2561, Train Acc: 90.91%, Test Loss: 0.3722, Test Acc: 84.21%\n",
      "Fold 4 Epoch [111/300], Train Loss: 0.2743, Train Acc: 89.61%, Test Loss: 0.3824, Test Acc: 84.21%\n",
      "Fold 4 Epoch [112/300], Train Loss: 0.2528, Train Acc: 92.21%, Test Loss: 0.3791, Test Acc: 84.21%\n",
      "Fold 4 Epoch [113/300], Train Loss: 0.2528, Train Acc: 91.77%, Test Loss: 0.3838, Test Acc: 84.21%\n",
      "Fold 4 Epoch [114/300], Train Loss: 0.2650, Train Acc: 90.91%, Test Loss: 0.3817, Test Acc: 84.21%\n",
      "Fold 4 Epoch [115/300], Train Loss: 0.2422, Train Acc: 92.64%, Test Loss: 0.3760, Test Acc: 82.46%\n",
      "Fold 4 Epoch [116/300], Train Loss: 0.2407, Train Acc: 93.07%, Test Loss: 0.3791, Test Acc: 82.46%\n",
      "Fold 4 Epoch [117/300], Train Loss: 0.2410, Train Acc: 93.07%, Test Loss: 0.3753, Test Acc: 84.21%\n",
      "Fold 4 Epoch [118/300], Train Loss: 0.2408, Train Acc: 93.51%, Test Loss: 0.3730, Test Acc: 85.96%\n",
      "Fold 4 Epoch [119/300], Train Loss: 0.2451, Train Acc: 91.77%, Test Loss: 0.3768, Test Acc: 80.70%\n",
      "Fold 4 Epoch [120/300], Train Loss: 0.2274, Train Acc: 93.51%, Test Loss: 0.3801, Test Acc: 82.46%\n",
      "Fold 4 Epoch [121/300], Train Loss: 0.2175, Train Acc: 93.51%, Test Loss: 0.3883, Test Acc: 84.21%\n",
      "Fold 4 Epoch [122/300], Train Loss: 0.2135, Train Acc: 91.77%, Test Loss: 0.3863, Test Acc: 78.95%\n",
      "Fold 4 Epoch [123/300], Train Loss: 0.2209, Train Acc: 93.51%, Test Loss: 0.3765, Test Acc: 85.96%\n",
      "Fold 4 Epoch [124/300], Train Loss: 0.2108, Train Acc: 94.37%, Test Loss: 0.3782, Test Acc: 82.46%\n",
      "Fold 4 Epoch [125/300], Train Loss: 0.2147, Train Acc: 93.07%, Test Loss: 0.3725, Test Acc: 80.70%\n",
      "Fold 4 Epoch [126/300], Train Loss: 0.2142, Train Acc: 94.81%, Test Loss: 0.3732, Test Acc: 82.46%\n",
      "Fold 4 Epoch [127/300], Train Loss: 0.2010, Train Acc: 95.24%, Test Loss: 0.3761, Test Acc: 82.46%\n",
      "Fold 4 Epoch [128/300], Train Loss: 0.2060, Train Acc: 93.07%, Test Loss: 0.3761, Test Acc: 82.46%\n",
      "Fold 4 Epoch [129/300], Train Loss: 0.1975, Train Acc: 93.07%, Test Loss: 0.3761, Test Acc: 80.70%\n",
      "Fold 4 Epoch [130/300], Train Loss: 0.2037, Train Acc: 94.81%, Test Loss: 0.3871, Test Acc: 80.70%\n",
      "Fold 4 Epoch [131/300], Train Loss: 0.1968, Train Acc: 93.07%, Test Loss: 0.3767, Test Acc: 84.21%\n",
      "Fold 4 Epoch [132/300], Train Loss: 0.1982, Train Acc: 94.37%, Test Loss: 0.3661, Test Acc: 85.96%\n",
      "Fold 4 Epoch [133/300], Train Loss: 0.1931, Train Acc: 95.67%, Test Loss: 0.3624, Test Acc: 84.21%\n",
      "Fold 4 Epoch [134/300], Train Loss: 0.1952, Train Acc: 93.51%, Test Loss: 0.3690, Test Acc: 80.70%\n",
      "Fold 4 Epoch [135/300], Train Loss: 0.1849, Train Acc: 95.67%, Test Loss: 0.3563, Test Acc: 84.21%\n",
      "Fold 4 Epoch [136/300], Train Loss: 0.1903, Train Acc: 95.67%, Test Loss: 0.3527, Test Acc: 84.21%\n",
      "Fold 4 Epoch [137/300], Train Loss: 0.1834, Train Acc: 94.81%, Test Loss: 0.3555, Test Acc: 84.21%\n",
      "Fold 4 Epoch [138/300], Train Loss: 0.1850, Train Acc: 95.67%, Test Loss: 0.3613, Test Acc: 82.46%\n",
      "Fold 4 Epoch [139/300], Train Loss: 0.1798, Train Acc: 94.81%, Test Loss: 0.3621, Test Acc: 82.46%\n",
      "Fold 4 Epoch [140/300], Train Loss: 0.1776, Train Acc: 93.51%, Test Loss: 0.3662, Test Acc: 80.70%\n",
      "Fold 4 Epoch [141/300], Train Loss: 0.1807, Train Acc: 93.94%, Test Loss: 0.3743, Test Acc: 82.46%\n",
      "Fold 4 Epoch [142/300], Train Loss: 0.1835, Train Acc: 95.24%, Test Loss: 0.3665, Test Acc: 84.21%\n",
      "Fold 4 Epoch [143/300], Train Loss: 0.1813, Train Acc: 96.10%, Test Loss: 0.3648, Test Acc: 84.21%\n",
      "Fold 4 Epoch [144/300], Train Loss: 0.1953, Train Acc: 93.94%, Test Loss: 0.3833, Test Acc: 82.46%\n",
      "Fold 4 Epoch [145/300], Train Loss: 0.1642, Train Acc: 95.67%, Test Loss: 0.3765, Test Acc: 82.46%\n",
      "Fold 4 Epoch [146/300], Train Loss: 0.1691, Train Acc: 95.67%, Test Loss: 0.3826, Test Acc: 82.46%\n",
      "Fold 4 Epoch [147/300], Train Loss: 0.1868, Train Acc: 93.51%, Test Loss: 0.4006, Test Acc: 84.21%\n",
      "Fold 4 Epoch [148/300], Train Loss: 0.1669, Train Acc: 94.81%, Test Loss: 0.3737, Test Acc: 82.46%\n",
      "Fold 4 Epoch [149/300], Train Loss: 0.1607, Train Acc: 95.67%, Test Loss: 0.3589, Test Acc: 84.21%\n",
      "Fold 4 Epoch [150/300], Train Loss: 0.2083, Train Acc: 91.77%, Test Loss: 0.3980, Test Acc: 85.96%\n",
      "Fold 4 Epoch [151/300], Train Loss: 0.1513, Train Acc: 96.10%, Test Loss: 0.3517, Test Acc: 84.21%\n",
      "Fold 4 Epoch [152/300], Train Loss: 0.1547, Train Acc: 95.67%, Test Loss: 0.3548, Test Acc: 84.21%\n",
      "Fold 4 Epoch [153/300], Train Loss: 0.1645, Train Acc: 94.37%, Test Loss: 0.3586, Test Acc: 84.21%\n",
      "Fold 4 Epoch [154/300], Train Loss: 0.1520, Train Acc: 95.24%, Test Loss: 0.3542, Test Acc: 85.96%\n",
      "Fold 4 Epoch [155/300], Train Loss: 0.1510, Train Acc: 95.24%, Test Loss: 0.3548, Test Acc: 84.21%\n",
      "Fold 4 Epoch [156/300], Train Loss: 0.1532, Train Acc: 95.24%, Test Loss: 0.3582, Test Acc: 82.46%\n",
      "Fold 4 Epoch [157/300], Train Loss: 0.1514, Train Acc: 95.67%, Test Loss: 0.3555, Test Acc: 85.96%\n",
      "Fold 4 Epoch [158/300], Train Loss: 0.1499, Train Acc: 95.24%, Test Loss: 0.3559, Test Acc: 84.21%\n",
      "Fold 4 Epoch [159/300], Train Loss: 0.1459, Train Acc: 95.67%, Test Loss: 0.3551, Test Acc: 84.21%\n",
      "Fold 4 Epoch [160/300], Train Loss: 0.1398, Train Acc: 95.67%, Test Loss: 0.3555, Test Acc: 85.96%\n",
      "Fold 4 Epoch [161/300], Train Loss: 0.1356, Train Acc: 95.24%, Test Loss: 0.3578, Test Acc: 84.21%\n",
      "Fold 4 Epoch [162/300], Train Loss: 0.1446, Train Acc: 96.54%, Test Loss: 0.3535, Test Acc: 82.46%\n",
      "Fold 4 Epoch [163/300], Train Loss: 0.1365, Train Acc: 94.81%, Test Loss: 0.3571, Test Acc: 85.96%\n",
      "Fold 4 Epoch [164/300], Train Loss: 0.1397, Train Acc: 96.54%, Test Loss: 0.3506, Test Acc: 84.21%\n",
      "Fold 4 Epoch [165/300], Train Loss: 0.1267, Train Acc: 96.10%, Test Loss: 0.3551, Test Acc: 85.96%\n",
      "Fold 4 Epoch [166/300], Train Loss: 0.1368, Train Acc: 95.67%, Test Loss: 0.3668, Test Acc: 84.21%\n",
      "Fold 4 Epoch [167/300], Train Loss: 0.1307, Train Acc: 95.67%, Test Loss: 0.3572, Test Acc: 84.21%\n",
      "Fold 4 Epoch [168/300], Train Loss: 0.1272, Train Acc: 96.54%, Test Loss: 0.3558, Test Acc: 82.46%\n",
      "Fold 4 Epoch [169/300], Train Loss: 0.1454, Train Acc: 93.94%, Test Loss: 0.3807, Test Acc: 85.96%\n",
      "Fold 4 Epoch [170/300], Train Loss: 0.1298, Train Acc: 96.97%, Test Loss: 0.3504, Test Acc: 84.21%\n",
      "Fold 4 Epoch [171/300], Train Loss: 0.1276, Train Acc: 96.54%, Test Loss: 0.3506, Test Acc: 85.96%\n",
      "Fold 4 Epoch [172/300], Train Loss: 0.1272, Train Acc: 96.97%, Test Loss: 0.3706, Test Acc: 82.46%\n",
      "Fold 4 Epoch [173/300], Train Loss: 0.1261, Train Acc: 96.10%, Test Loss: 0.3739, Test Acc: 85.96%\n",
      "Fold 4 Epoch [174/300], Train Loss: 0.1241, Train Acc: 96.10%, Test Loss: 0.3769, Test Acc: 84.21%\n",
      "Fold 4 Epoch [175/300], Train Loss: 0.1168, Train Acc: 96.97%, Test Loss: 0.3709, Test Acc: 84.21%\n",
      "Fold 4 Epoch [176/300], Train Loss: 0.1281, Train Acc: 94.81%, Test Loss: 0.3829, Test Acc: 84.21%\n",
      "Fold 4 Epoch [177/300], Train Loss: 0.1213, Train Acc: 96.97%, Test Loss: 0.3610, Test Acc: 82.46%\n",
      "Fold 4 Epoch [178/300], Train Loss: 0.1149, Train Acc: 96.54%, Test Loss: 0.3626, Test Acc: 85.96%\n",
      "Fold 4 Epoch [179/300], Train Loss: 0.1157, Train Acc: 96.97%, Test Loss: 0.3651, Test Acc: 84.21%\n",
      "Fold 4 Epoch [180/300], Train Loss: 0.1087, Train Acc: 96.54%, Test Loss: 0.3553, Test Acc: 85.96%\n",
      "Fold 4 Epoch [181/300], Train Loss: 0.1187, Train Acc: 96.54%, Test Loss: 0.3681, Test Acc: 85.96%\n",
      "Fold 4 Epoch [182/300], Train Loss: 0.1116, Train Acc: 96.54%, Test Loss: 0.3461, Test Acc: 85.96%\n",
      "Fold 4 Epoch [183/300], Train Loss: 0.1106, Train Acc: 97.40%, Test Loss: 0.3501, Test Acc: 85.96%\n",
      "Fold 4 Epoch [184/300], Train Loss: 0.1181, Train Acc: 95.24%, Test Loss: 0.3606, Test Acc: 84.21%\n",
      "Fold 4 Epoch [185/300], Train Loss: 0.1037, Train Acc: 97.40%, Test Loss: 0.3439, Test Acc: 87.72%\n",
      "Fold 4 Epoch [186/300], Train Loss: 0.1077, Train Acc: 97.40%, Test Loss: 0.3457, Test Acc: 85.96%\n",
      "Fold 4 Epoch [187/300], Train Loss: 0.1064, Train Acc: 96.54%, Test Loss: 0.3665, Test Acc: 84.21%\n",
      "Fold 4 Epoch [188/300], Train Loss: 0.1011, Train Acc: 97.40%, Test Loss: 0.3461, Test Acc: 85.96%\n",
      "Fold 4 Epoch [189/300], Train Loss: 0.0959, Train Acc: 97.84%, Test Loss: 0.3462, Test Acc: 85.96%\n",
      "Fold 4 Epoch [190/300], Train Loss: 0.0954, Train Acc: 96.54%, Test Loss: 0.3604, Test Acc: 85.96%\n",
      "Fold 4 Epoch [191/300], Train Loss: 0.0963, Train Acc: 96.97%, Test Loss: 0.3626, Test Acc: 84.21%\n",
      "Fold 4 Epoch [192/300], Train Loss: 0.0949, Train Acc: 97.84%, Test Loss: 0.3582, Test Acc: 85.96%\n",
      "Fold 4 Epoch [193/300], Train Loss: 0.0899, Train Acc: 97.84%, Test Loss: 0.3589, Test Acc: 85.96%\n",
      "Fold 4 Epoch [194/300], Train Loss: 0.0901, Train Acc: 96.97%, Test Loss: 0.3690, Test Acc: 85.96%\n",
      "Fold 4 Epoch [195/300], Train Loss: 0.0874, Train Acc: 97.84%, Test Loss: 0.3544, Test Acc: 85.96%\n",
      "Fold 4 Epoch [196/300], Train Loss: 0.0857, Train Acc: 97.84%, Test Loss: 0.3520, Test Acc: 87.72%\n",
      "Fold 4 Epoch [197/300], Train Loss: 0.0904, Train Acc: 97.40%, Test Loss: 0.3561, Test Acc: 87.72%\n",
      "Fold 4 Epoch [198/300], Train Loss: 0.0880, Train Acc: 98.27%, Test Loss: 0.3558, Test Acc: 85.96%\n",
      "Fold 4 Epoch [199/300], Train Loss: 0.0927, Train Acc: 97.84%, Test Loss: 0.3790, Test Acc: 87.72%\n",
      "Fold 4 Epoch [200/300], Train Loss: 0.0837, Train Acc: 98.27%, Test Loss: 0.3684, Test Acc: 85.96%\n",
      "Fold 4 Epoch [201/300], Train Loss: 0.0804, Train Acc: 98.27%, Test Loss: 0.3704, Test Acc: 85.96%\n",
      "Fold 4 Epoch [202/300], Train Loss: 0.0792, Train Acc: 98.27%, Test Loss: 0.3698, Test Acc: 84.21%\n",
      "Fold 4 Epoch [203/300], Train Loss: 0.0769, Train Acc: 98.27%, Test Loss: 0.3608, Test Acc: 85.96%\n",
      "Fold 4 Epoch [204/300], Train Loss: 0.0779, Train Acc: 98.70%, Test Loss: 0.3694, Test Acc: 85.96%\n",
      "Fold 4 Epoch [205/300], Train Loss: 0.0825, Train Acc: 97.40%, Test Loss: 0.3607, Test Acc: 85.96%\n",
      "Fold 4 Epoch [206/300], Train Loss: 0.0742, Train Acc: 98.70%, Test Loss: 0.3703, Test Acc: 87.72%\n",
      "Fold 4 Epoch [207/300], Train Loss: 0.0778, Train Acc: 98.70%, Test Loss: 0.3952, Test Acc: 85.96%\n",
      "Fold 4 Epoch [208/300], Train Loss: 0.0954, Train Acc: 97.40%, Test Loss: 0.3852, Test Acc: 85.96%\n",
      "Fold 4 Epoch [209/300], Train Loss: 0.0721, Train Acc: 98.70%, Test Loss: 0.3868, Test Acc: 84.21%\n",
      "Fold 4 Epoch [210/300], Train Loss: 0.0706, Train Acc: 98.70%, Test Loss: 0.3757, Test Acc: 87.72%\n",
      "Fold 4 Epoch [211/300], Train Loss: 0.0784, Train Acc: 98.27%, Test Loss: 0.3652, Test Acc: 85.96%\n",
      "Fold 4 Epoch [212/300], Train Loss: 0.0791, Train Acc: 98.27%, Test Loss: 0.3872, Test Acc: 85.96%\n",
      "Fold 4 Epoch [213/300], Train Loss: 0.0700, Train Acc: 97.84%, Test Loss: 0.3638, Test Acc: 85.96%\n",
      "Fold 4 Epoch [214/300], Train Loss: 0.0721, Train Acc: 98.70%, Test Loss: 0.3615, Test Acc: 85.96%\n",
      "Fold 4 Epoch [215/300], Train Loss: 0.0744, Train Acc: 98.70%, Test Loss: 0.3826, Test Acc: 85.96%\n",
      "Fold 4 Epoch [216/300], Train Loss: 0.0811, Train Acc: 96.97%, Test Loss: 0.3578, Test Acc: 85.96%\n",
      "Fold 4 Epoch [217/300], Train Loss: 0.0733, Train Acc: 98.70%, Test Loss: 0.3877, Test Acc: 84.21%\n",
      "Fold 4 Epoch [218/300], Train Loss: 0.0668, Train Acc: 98.70%, Test Loss: 0.3765, Test Acc: 85.96%\n",
      "Fold 4 Epoch [219/300], Train Loss: 0.0682, Train Acc: 98.27%, Test Loss: 0.3857, Test Acc: 85.96%\n",
      "Fold 4 Epoch [220/300], Train Loss: 0.0707, Train Acc: 98.27%, Test Loss: 0.4218, Test Acc: 87.72%\n",
      "Fold 4 Epoch [221/300], Train Loss: 0.0587, Train Acc: 98.70%, Test Loss: 0.4183, Test Acc: 84.21%\n",
      "Fold 4 Epoch [222/300], Train Loss: 0.0612, Train Acc: 99.57%, Test Loss: 0.3992, Test Acc: 84.21%\n",
      "Fold 4 Epoch [223/300], Train Loss: 0.0645, Train Acc: 98.27%, Test Loss: 0.4192, Test Acc: 85.96%\n",
      "Fold 4 Epoch [224/300], Train Loss: 0.0619, Train Acc: 98.27%, Test Loss: 0.3989, Test Acc: 85.96%\n",
      "Fold 4 Epoch [225/300], Train Loss: 0.0565, Train Acc: 99.13%, Test Loss: 0.4081, Test Acc: 84.21%\n",
      "Fold 4 Epoch [226/300], Train Loss: 0.0553, Train Acc: 99.13%, Test Loss: 0.3998, Test Acc: 84.21%\n",
      "Fold 4 Epoch [227/300], Train Loss: 0.0606, Train Acc: 98.70%, Test Loss: 0.3821, Test Acc: 85.96%\n",
      "Fold 4 Epoch [228/300], Train Loss: 0.0609, Train Acc: 99.13%, Test Loss: 0.4047, Test Acc: 85.96%\n",
      "Fold 4 Epoch [229/300], Train Loss: 0.0560, Train Acc: 99.13%, Test Loss: 0.3956, Test Acc: 85.96%\n",
      "Fold 4 Epoch [230/300], Train Loss: 0.0526, Train Acc: 99.13%, Test Loss: 0.4023, Test Acc: 85.96%\n",
      "Fold 4 Epoch [231/300], Train Loss: 0.0548, Train Acc: 99.57%, Test Loss: 0.4363, Test Acc: 84.21%\n",
      "Fold 4 Epoch [232/300], Train Loss: 0.0562, Train Acc: 99.57%, Test Loss: 0.4377, Test Acc: 84.21%\n",
      "Fold 4 Epoch [233/300], Train Loss: 0.0506, Train Acc: 99.57%, Test Loss: 0.4489, Test Acc: 84.21%\n",
      "Fold 4 Epoch [234/300], Train Loss: 0.0473, Train Acc: 99.57%, Test Loss: 0.4539, Test Acc: 85.96%\n",
      "Fold 4 Epoch [235/300], Train Loss: 0.0471, Train Acc: 99.57%, Test Loss: 0.4400, Test Acc: 84.21%\n",
      "Fold 4 Epoch [236/300], Train Loss: 0.0448, Train Acc: 99.57%, Test Loss: 0.4462, Test Acc: 84.21%\n",
      "Fold 4 Epoch [237/300], Train Loss: 0.0477, Train Acc: 98.70%, Test Loss: 0.4632, Test Acc: 85.96%\n",
      "Fold 4 Epoch [238/300], Train Loss: 0.0450, Train Acc: 99.57%, Test Loss: 0.4424, Test Acc: 84.21%\n",
      "Fold 4 Epoch [239/300], Train Loss: 0.0472, Train Acc: 99.57%, Test Loss: 0.4370, Test Acc: 85.96%\n",
      "Fold 4 Epoch [240/300], Train Loss: 0.0446, Train Acc: 100.00%, Test Loss: 0.4142, Test Acc: 84.21%\n",
      "Fold 4 Epoch [241/300], Train Loss: 0.0586, Train Acc: 98.70%, Test Loss: 0.4671, Test Acc: 85.96%\n",
      "Fold 4 Epoch [242/300], Train Loss: 0.0474, Train Acc: 99.57%, Test Loss: 0.4403, Test Acc: 84.21%\n",
      "Fold 4 Epoch [243/300], Train Loss: 0.0455, Train Acc: 99.57%, Test Loss: 0.4594, Test Acc: 85.96%\n",
      "Fold 4 Epoch [244/300], Train Loss: 0.0407, Train Acc: 99.57%, Test Loss: 0.4604, Test Acc: 85.96%\n",
      "Fold 4 Epoch [245/300], Train Loss: 0.0380, Train Acc: 100.00%, Test Loss: 0.4532, Test Acc: 85.96%\n",
      "Fold 4 Epoch [246/300], Train Loss: 0.0391, Train Acc: 99.57%, Test Loss: 0.4571, Test Acc: 84.21%\n",
      "Fold 4 Epoch [247/300], Train Loss: 0.0401, Train Acc: 99.57%, Test Loss: 0.4644, Test Acc: 87.72%\n",
      "Fold 4 Epoch [248/300], Train Loss: 0.0366, Train Acc: 100.00%, Test Loss: 0.4406, Test Acc: 85.96%\n",
      "Fold 4 Epoch [249/300], Train Loss: 0.0360, Train Acc: 100.00%, Test Loss: 0.4284, Test Acc: 84.21%\n",
      "Fold 4 Epoch [250/300], Train Loss: 0.0446, Train Acc: 99.57%, Test Loss: 0.4787, Test Acc: 87.72%\n",
      "Fold 4 Epoch [251/300], Train Loss: 0.0410, Train Acc: 100.00%, Test Loss: 0.4533, Test Acc: 84.21%\n",
      "Fold 4 Epoch [252/300], Train Loss: 0.0345, Train Acc: 100.00%, Test Loss: 0.4783, Test Acc: 84.21%\n",
      "Fold 4 Epoch [253/300], Train Loss: 0.0347, Train Acc: 100.00%, Test Loss: 0.4795, Test Acc: 84.21%\n",
      "Fold 4 Epoch [254/300], Train Loss: 0.0323, Train Acc: 100.00%, Test Loss: 0.4693, Test Acc: 84.21%\n",
      "Fold 4 Epoch [255/300], Train Loss: 0.0366, Train Acc: 100.00%, Test Loss: 0.4639, Test Acc: 87.72%\n",
      "Fold 4 Epoch [256/300], Train Loss: 0.0341, Train Acc: 100.00%, Test Loss: 0.4461, Test Acc: 85.96%\n",
      "Fold 4 Epoch [257/300], Train Loss: 0.0333, Train Acc: 100.00%, Test Loss: 0.4732, Test Acc: 85.96%\n",
      "Fold 4 Epoch [258/300], Train Loss: 0.0346, Train Acc: 100.00%, Test Loss: 0.4937, Test Acc: 85.96%\n",
      "Fold 4 Epoch [259/300], Train Loss: 0.0328, Train Acc: 100.00%, Test Loss: 0.4907, Test Acc: 85.96%\n",
      "Fold 4 Epoch [260/300], Train Loss: 0.0300, Train Acc: 100.00%, Test Loss: 0.4792, Test Acc: 85.96%\n",
      "Fold 4 Epoch [261/300], Train Loss: 0.0313, Train Acc: 100.00%, Test Loss: 0.4697, Test Acc: 84.21%\n",
      "Fold 4 Epoch [262/300], Train Loss: 0.0330, Train Acc: 100.00%, Test Loss: 0.4998, Test Acc: 87.72%\n",
      "Fold 4 Epoch [263/300], Train Loss: 0.0337, Train Acc: 100.00%, Test Loss: 0.4732, Test Acc: 84.21%\n",
      "Fold 4 Epoch [264/300], Train Loss: 0.0304, Train Acc: 100.00%, Test Loss: 0.4638, Test Acc: 84.21%\n",
      "Fold 4 Epoch [265/300], Train Loss: 0.0316, Train Acc: 100.00%, Test Loss: 0.4803, Test Acc: 87.72%\n",
      "Fold 4 Epoch [266/300], Train Loss: 0.0281, Train Acc: 100.00%, Test Loss: 0.4571, Test Acc: 84.21%\n",
      "Fold 4 Epoch [267/300], Train Loss: 0.0287, Train Acc: 100.00%, Test Loss: 0.4524, Test Acc: 84.21%\n",
      "Fold 4 Epoch [268/300], Train Loss: 0.0290, Train Acc: 100.00%, Test Loss: 0.4645, Test Acc: 85.96%\n",
      "Fold 4 Epoch [269/300], Train Loss: 0.0303, Train Acc: 100.00%, Test Loss: 0.4710, Test Acc: 87.72%\n",
      "Fold 4 Epoch [270/300], Train Loss: 0.0281, Train Acc: 100.00%, Test Loss: 0.4542, Test Acc: 85.96%\n",
      "Fold 4 Epoch [271/300], Train Loss: 0.0269, Train Acc: 100.00%, Test Loss: 0.4651, Test Acc: 85.96%\n",
      "Fold 4 Epoch [272/300], Train Loss: 0.0265, Train Acc: 100.00%, Test Loss: 0.4889, Test Acc: 87.72%\n",
      "Fold 4 Epoch [273/300], Train Loss: 0.0270, Train Acc: 100.00%, Test Loss: 0.4981, Test Acc: 87.72%\n",
      "Fold 4 Epoch [274/300], Train Loss: 0.0240, Train Acc: 100.00%, Test Loss: 0.4851, Test Acc: 84.21%\n",
      "Fold 4 Epoch [275/300], Train Loss: 0.0257, Train Acc: 100.00%, Test Loss: 0.4976, Test Acc: 87.72%\n",
      "Fold 4 Epoch [276/300], Train Loss: 0.0251, Train Acc: 100.00%, Test Loss: 0.5075, Test Acc: 85.96%\n",
      "Fold 4 Epoch [277/300], Train Loss: 0.0237, Train Acc: 100.00%, Test Loss: 0.5127, Test Acc: 85.96%\n",
      "Fold 4 Epoch [278/300], Train Loss: 0.0254, Train Acc: 100.00%, Test Loss: 0.5293, Test Acc: 85.96%\n",
      "Fold 4 Epoch [279/300], Train Loss: 0.0243, Train Acc: 100.00%, Test Loss: 0.5319, Test Acc: 85.96%\n",
      "Fold 4 Epoch [280/300], Train Loss: 0.0236, Train Acc: 100.00%, Test Loss: 0.5212, Test Acc: 85.96%\n",
      "Fold 4 Epoch [281/300], Train Loss: 0.0236, Train Acc: 100.00%, Test Loss: 0.5114, Test Acc: 85.96%\n",
      "Fold 4 Epoch [282/300], Train Loss: 0.0228, Train Acc: 100.00%, Test Loss: 0.5038, Test Acc: 85.96%\n",
      "Fold 4 Epoch [283/300], Train Loss: 0.0240, Train Acc: 100.00%, Test Loss: 0.5320, Test Acc: 87.72%\n",
      "Fold 4 Epoch [284/300], Train Loss: 0.0222, Train Acc: 100.00%, Test Loss: 0.5189, Test Acc: 85.96%\n",
      "Fold 4 Epoch [285/300], Train Loss: 0.0217, Train Acc: 100.00%, Test Loss: 0.5233, Test Acc: 87.72%\n",
      "Fold 4 Epoch [286/300], Train Loss: 0.0224, Train Acc: 100.00%, Test Loss: 0.4925, Test Acc: 84.21%\n",
      "Fold 4 Epoch [287/300], Train Loss: 0.0213, Train Acc: 100.00%, Test Loss: 0.5132, Test Acc: 87.72%\n",
      "Fold 4 Epoch [288/300], Train Loss: 0.0198, Train Acc: 100.00%, Test Loss: 0.5131, Test Acc: 84.21%\n",
      "Fold 4 Epoch [289/300], Train Loss: 0.0199, Train Acc: 100.00%, Test Loss: 0.5206, Test Acc: 85.96%\n",
      "Fold 4 Epoch [290/300], Train Loss: 0.0198, Train Acc: 100.00%, Test Loss: 0.5329, Test Acc: 84.21%\n",
      "Fold 4 Epoch [291/300], Train Loss: 0.0219, Train Acc: 100.00%, Test Loss: 0.5296, Test Acc: 84.21%\n",
      "Fold 4 Epoch [292/300], Train Loss: 0.0278, Train Acc: 99.57%, Test Loss: 0.5934, Test Acc: 87.72%\n",
      "Fold 4 Epoch [293/300], Train Loss: 0.0192, Train Acc: 100.00%, Test Loss: 0.5429, Test Acc: 85.96%\n",
      "Fold 4 Epoch [294/300], Train Loss: 0.0193, Train Acc: 100.00%, Test Loss: 0.5544, Test Acc: 85.96%\n",
      "Fold 4 Epoch [295/300], Train Loss: 0.0176, Train Acc: 100.00%, Test Loss: 0.5697, Test Acc: 87.72%\n",
      "Fold 4 Epoch [296/300], Train Loss: 0.0168, Train Acc: 100.00%, Test Loss: 0.5445, Test Acc: 87.72%\n",
      "Fold 4 Epoch [297/300], Train Loss: 0.0193, Train Acc: 100.00%, Test Loss: 0.5315, Test Acc: 84.21%\n",
      "Fold 4 Epoch [298/300], Train Loss: 0.0199, Train Acc: 100.00%, Test Loss: 0.5643, Test Acc: 87.72%\n",
      "Fold 4 Epoch [299/300], Train Loss: 0.0186, Train Acc: 100.00%, Test Loss: 0.5492, Test Acc: 85.96%\n",
      "Fold 4 Epoch [300/300], Train Loss: 0.0170, Train Acc: 100.00%, Test Loss: 0.5270, Test Acc: 84.21%\n",
      "Fold 4 Best Accuracy: 89.47%\n",
      "Fold 5/5\n",
      "Fold 5 Epoch [1/300], Train Loss: 2.2980, Train Acc: 0.00%, Test Loss: 2.2990, Test Acc: 0.00%\n",
      "Fold 5 Epoch [2/300], Train Loss: 2.1501, Train Acc: 0.00%, Test Loss: 2.1535, Test Acc: 0.00%\n",
      "Fold 5 Epoch [3/300], Train Loss: 2.0252, Train Acc: 3.90%, Test Loss: 2.0316, Test Acc: 1.75%\n",
      "Fold 5 Epoch [4/300], Train Loss: 1.9147, Train Acc: 26.84%, Test Loss: 1.9273, Test Acc: 24.56%\n",
      "Fold 5 Epoch [5/300], Train Loss: 1.8296, Train Acc: 32.90%, Test Loss: 1.8409, Test Acc: 35.09%\n",
      "Fold 5 Epoch [6/300], Train Loss: 1.7583, Train Acc: 33.77%, Test Loss: 1.7682, Test Acc: 35.09%\n",
      "Fold 5 Epoch [7/300], Train Loss: 1.6955, Train Acc: 33.33%, Test Loss: 1.7048, Test Acc: 33.33%\n",
      "Fold 5 Epoch [8/300], Train Loss: 1.6356, Train Acc: 32.90%, Test Loss: 1.6482, Test Acc: 33.33%\n",
      "Fold 5 Epoch [9/300], Train Loss: 1.5825, Train Acc: 34.63%, Test Loss: 1.5972, Test Acc: 28.07%\n",
      "Fold 5 Epoch [10/300], Train Loss: 1.5384, Train Acc: 35.93%, Test Loss: 1.5505, Test Acc: 22.81%\n",
      "Fold 5 Epoch [11/300], Train Loss: 1.4907, Train Acc: 36.80%, Test Loss: 1.5085, Test Acc: 26.32%\n",
      "Fold 5 Epoch [12/300], Train Loss: 1.4507, Train Acc: 34.63%, Test Loss: 1.4693, Test Acc: 29.82%\n",
      "Fold 5 Epoch [13/300], Train Loss: 1.4154, Train Acc: 36.80%, Test Loss: 1.4322, Test Acc: 33.33%\n",
      "Fold 5 Epoch [14/300], Train Loss: 1.3790, Train Acc: 39.39%, Test Loss: 1.3972, Test Acc: 36.84%\n",
      "Fold 5 Epoch [15/300], Train Loss: 1.3540, Train Acc: 41.56%, Test Loss: 1.3650, Test Acc: 38.60%\n",
      "Fold 5 Epoch [16/300], Train Loss: 1.3220, Train Acc: 44.59%, Test Loss: 1.3352, Test Acc: 38.60%\n",
      "Fold 5 Epoch [17/300], Train Loss: 1.2953, Train Acc: 46.32%, Test Loss: 1.3077, Test Acc: 38.60%\n",
      "Fold 5 Epoch [18/300], Train Loss: 1.2723, Train Acc: 50.22%, Test Loss: 1.2807, Test Acc: 45.61%\n",
      "Fold 5 Epoch [19/300], Train Loss: 1.2453, Train Acc: 53.25%, Test Loss: 1.2546, Test Acc: 49.12%\n",
      "Fold 5 Epoch [20/300], Train Loss: 1.2232, Train Acc: 53.68%, Test Loss: 1.2286, Test Acc: 50.88%\n",
      "Fold 5 Epoch [21/300], Train Loss: 1.1997, Train Acc: 56.71%, Test Loss: 1.2032, Test Acc: 49.12%\n",
      "Fold 5 Epoch [22/300], Train Loss: 1.1789, Train Acc: 56.71%, Test Loss: 1.1774, Test Acc: 50.88%\n",
      "Fold 5 Epoch [23/300], Train Loss: 1.1547, Train Acc: 57.14%, Test Loss: 1.1509, Test Acc: 52.63%\n",
      "Fold 5 Epoch [24/300], Train Loss: 1.1301, Train Acc: 58.01%, Test Loss: 1.1245, Test Acc: 54.39%\n",
      "Fold 5 Epoch [25/300], Train Loss: 1.1021, Train Acc: 57.14%, Test Loss: 1.0976, Test Acc: 54.39%\n",
      "Fold 5 Epoch [26/300], Train Loss: 1.0789, Train Acc: 58.44%, Test Loss: 1.0711, Test Acc: 52.63%\n",
      "Fold 5 Epoch [27/300], Train Loss: 1.0580, Train Acc: 61.47%, Test Loss: 1.0445, Test Acc: 56.14%\n",
      "Fold 5 Epoch [28/300], Train Loss: 1.0293, Train Acc: 65.37%, Test Loss: 1.0161, Test Acc: 63.16%\n",
      "Fold 5 Epoch [29/300], Train Loss: 0.9990, Train Acc: 66.23%, Test Loss: 0.9865, Test Acc: 64.91%\n",
      "Fold 5 Epoch [30/300], Train Loss: 0.9767, Train Acc: 67.53%, Test Loss: 0.9559, Test Acc: 63.16%\n",
      "Fold 5 Epoch [31/300], Train Loss: 0.9445, Train Acc: 69.26%, Test Loss: 0.9259, Test Acc: 64.91%\n",
      "Fold 5 Epoch [32/300], Train Loss: 0.9188, Train Acc: 69.70%, Test Loss: 0.8961, Test Acc: 63.16%\n",
      "Fold 5 Epoch [33/300], Train Loss: 0.8932, Train Acc: 70.13%, Test Loss: 0.8646, Test Acc: 66.67%\n",
      "Fold 5 Epoch [34/300], Train Loss: 0.8627, Train Acc: 71.43%, Test Loss: 0.8343, Test Acc: 68.42%\n",
      "Fold 5 Epoch [35/300], Train Loss: 0.8410, Train Acc: 70.13%, Test Loss: 0.8045, Test Acc: 68.42%\n",
      "Fold 5 Epoch [36/300], Train Loss: 0.8126, Train Acc: 71.00%, Test Loss: 0.7768, Test Acc: 68.42%\n",
      "Fold 5 Epoch [37/300], Train Loss: 0.7882, Train Acc: 70.56%, Test Loss: 0.7506, Test Acc: 68.42%\n",
      "Fold 5 Epoch [38/300], Train Loss: 0.7654, Train Acc: 70.56%, Test Loss: 0.7258, Test Acc: 70.18%\n",
      "Fold 5 Epoch [39/300], Train Loss: 0.7406, Train Acc: 71.43%, Test Loss: 0.7029, Test Acc: 73.68%\n",
      "Fold 5 Epoch [40/300], Train Loss: 0.7209, Train Acc: 71.43%, Test Loss: 0.6813, Test Acc: 71.93%\n",
      "Fold 5 Epoch [41/300], Train Loss: 0.7122, Train Acc: 70.13%, Test Loss: 0.6618, Test Acc: 71.93%\n",
      "Fold 5 Epoch [42/300], Train Loss: 0.6869, Train Acc: 71.00%, Test Loss: 0.6442, Test Acc: 71.93%\n",
      "Fold 5 Epoch [43/300], Train Loss: 0.6659, Train Acc: 71.86%, Test Loss: 0.6301, Test Acc: 71.93%\n",
      "Fold 5 Epoch [44/300], Train Loss: 0.6505, Train Acc: 71.86%, Test Loss: 0.6155, Test Acc: 70.18%\n",
      "Fold 5 Epoch [45/300], Train Loss: 0.6378, Train Acc: 72.29%, Test Loss: 0.6010, Test Acc: 68.42%\n",
      "Fold 5 Epoch [46/300], Train Loss: 0.6277, Train Acc: 72.29%, Test Loss: 0.5881, Test Acc: 68.42%\n",
      "Fold 5 Epoch [47/300], Train Loss: 0.6117, Train Acc: 72.29%, Test Loss: 0.5773, Test Acc: 68.42%\n",
      "Fold 5 Epoch [48/300], Train Loss: 0.5979, Train Acc: 72.73%, Test Loss: 0.5662, Test Acc: 68.42%\n",
      "Fold 5 Epoch [49/300], Train Loss: 0.5807, Train Acc: 72.73%, Test Loss: 0.5566, Test Acc: 68.42%\n",
      "Fold 5 Epoch [50/300], Train Loss: 0.5798, Train Acc: 73.16%, Test Loss: 0.5483, Test Acc: 68.42%\n",
      "Fold 5 Epoch [51/300], Train Loss: 0.5691, Train Acc: 73.16%, Test Loss: 0.5420, Test Acc: 70.18%\n",
      "Fold 5 Epoch [52/300], Train Loss: 0.5633, Train Acc: 73.16%, Test Loss: 0.5376, Test Acc: 70.18%\n",
      "Fold 5 Epoch [53/300], Train Loss: 0.5539, Train Acc: 73.59%, Test Loss: 0.5339, Test Acc: 70.18%\n",
      "Fold 5 Epoch [54/300], Train Loss: 0.5451, Train Acc: 73.59%, Test Loss: 0.5291, Test Acc: 68.42%\n",
      "Fold 5 Epoch [55/300], Train Loss: 0.5378, Train Acc: 74.89%, Test Loss: 0.5271, Test Acc: 68.42%\n",
      "Fold 5 Epoch [56/300], Train Loss: 0.5372, Train Acc: 74.89%, Test Loss: 0.5243, Test Acc: 66.67%\n",
      "Fold 5 Epoch [57/300], Train Loss: 0.5326, Train Acc: 76.19%, Test Loss: 0.5191, Test Acc: 66.67%\n",
      "Fold 5 Epoch [58/300], Train Loss: 0.5324, Train Acc: 76.19%, Test Loss: 0.5136, Test Acc: 66.67%\n",
      "Fold 5 Epoch [59/300], Train Loss: 0.5160, Train Acc: 77.06%, Test Loss: 0.5083, Test Acc: 66.67%\n",
      "Fold 5 Epoch [60/300], Train Loss: 0.5211, Train Acc: 76.62%, Test Loss: 0.5045, Test Acc: 68.42%\n",
      "Fold 5 Epoch [61/300], Train Loss: 0.5120, Train Acc: 77.06%, Test Loss: 0.5033, Test Acc: 66.67%\n",
      "Fold 5 Epoch [62/300], Train Loss: 0.4953, Train Acc: 76.19%, Test Loss: 0.5013, Test Acc: 66.67%\n",
      "Fold 5 Epoch [63/300], Train Loss: 0.4962, Train Acc: 77.49%, Test Loss: 0.4987, Test Acc: 66.67%\n",
      "Fold 5 Epoch [64/300], Train Loss: 0.4955, Train Acc: 77.49%, Test Loss: 0.4965, Test Acc: 66.67%\n",
      "Fold 5 Epoch [65/300], Train Loss: 0.4942, Train Acc: 77.06%, Test Loss: 0.4943, Test Acc: 70.18%\n",
      "Fold 5 Epoch [66/300], Train Loss: 0.4831, Train Acc: 77.06%, Test Loss: 0.4916, Test Acc: 70.18%\n",
      "Fold 5 Epoch [67/300], Train Loss: 0.4942, Train Acc: 77.49%, Test Loss: 0.4888, Test Acc: 70.18%\n",
      "Fold 5 Epoch [68/300], Train Loss: 0.4741, Train Acc: 77.49%, Test Loss: 0.4870, Test Acc: 70.18%\n",
      "Fold 5 Epoch [69/300], Train Loss: 0.4693, Train Acc: 77.06%, Test Loss: 0.4876, Test Acc: 71.93%\n",
      "Fold 5 Epoch [70/300], Train Loss: 0.4618, Train Acc: 77.06%, Test Loss: 0.4875, Test Acc: 71.93%\n",
      "Fold 5 Epoch [71/300], Train Loss: 0.4718, Train Acc: 77.49%, Test Loss: 0.4882, Test Acc: 70.18%\n",
      "Fold 5 Epoch [72/300], Train Loss: 0.4734, Train Acc: 77.49%, Test Loss: 0.4865, Test Acc: 71.93%\n",
      "Fold 5 Epoch [73/300], Train Loss: 0.4676, Train Acc: 77.06%, Test Loss: 0.4838, Test Acc: 71.93%\n",
      "Fold 5 Epoch [74/300], Train Loss: 0.4645, Train Acc: 77.06%, Test Loss: 0.4818, Test Acc: 71.93%\n",
      "Fold 5 Epoch [75/300], Train Loss: 0.4628, Train Acc: 76.62%, Test Loss: 0.4811, Test Acc: 73.68%\n",
      "Fold 5 Epoch [76/300], Train Loss: 0.4536, Train Acc: 79.22%, Test Loss: 0.4825, Test Acc: 70.18%\n",
      "Fold 5 Epoch [77/300], Train Loss: 0.4616, Train Acc: 80.95%, Test Loss: 0.4810, Test Acc: 73.68%\n",
      "Fold 5 Epoch [78/300], Train Loss: 0.4464, Train Acc: 80.95%, Test Loss: 0.4767, Test Acc: 73.68%\n",
      "Fold 5 Epoch [79/300], Train Loss: 0.4459, Train Acc: 79.22%, Test Loss: 0.4742, Test Acc: 70.18%\n",
      "Fold 5 Epoch [80/300], Train Loss: 0.4407, Train Acc: 78.79%, Test Loss: 0.4739, Test Acc: 71.93%\n",
      "Fold 5 Epoch [81/300], Train Loss: 0.4467, Train Acc: 78.35%, Test Loss: 0.4756, Test Acc: 71.93%\n",
      "Fold 5 Epoch [82/300], Train Loss: 0.4400, Train Acc: 79.22%, Test Loss: 0.4785, Test Acc: 71.93%\n",
      "Fold 5 Epoch [83/300], Train Loss: 0.4407, Train Acc: 79.65%, Test Loss: 0.4792, Test Acc: 71.93%\n",
      "Fold 5 Epoch [84/300], Train Loss: 0.4332, Train Acc: 79.65%, Test Loss: 0.4780, Test Acc: 71.93%\n",
      "Fold 5 Epoch [85/300], Train Loss: 0.4330, Train Acc: 78.35%, Test Loss: 0.4742, Test Acc: 71.93%\n",
      "Fold 5 Epoch [86/300], Train Loss: 0.4374, Train Acc: 80.09%, Test Loss: 0.4692, Test Acc: 73.68%\n",
      "Fold 5 Epoch [87/300], Train Loss: 0.4390, Train Acc: 80.52%, Test Loss: 0.4685, Test Acc: 73.68%\n",
      "Fold 5 Epoch [88/300], Train Loss: 0.4210, Train Acc: 80.52%, Test Loss: 0.4719, Test Acc: 73.68%\n",
      "Fold 5 Epoch [89/300], Train Loss: 0.4266, Train Acc: 80.52%, Test Loss: 0.4792, Test Acc: 71.93%\n",
      "Fold 5 Epoch [90/300], Train Loss: 0.4198, Train Acc: 80.95%, Test Loss: 0.4858, Test Acc: 73.68%\n",
      "Fold 5 Epoch [91/300], Train Loss: 0.4216, Train Acc: 80.95%, Test Loss: 0.4793, Test Acc: 73.68%\n",
      "Fold 5 Epoch [92/300], Train Loss: 0.4224, Train Acc: 80.52%, Test Loss: 0.4718, Test Acc: 73.68%\n",
      "Fold 5 Epoch [93/300], Train Loss: 0.4174, Train Acc: 80.09%, Test Loss: 0.4656, Test Acc: 71.93%\n",
      "Fold 5 Epoch [94/300], Train Loss: 0.4157, Train Acc: 80.52%, Test Loss: 0.4613, Test Acc: 73.68%\n",
      "Fold 5 Epoch [95/300], Train Loss: 0.4165, Train Acc: 80.52%, Test Loss: 0.4591, Test Acc: 73.68%\n",
      "Fold 5 Epoch [96/300], Train Loss: 0.4177, Train Acc: 80.52%, Test Loss: 0.4596, Test Acc: 70.18%\n",
      "Fold 5 Epoch [97/300], Train Loss: 0.4128, Train Acc: 81.39%, Test Loss: 0.4604, Test Acc: 73.68%\n",
      "Fold 5 Epoch [98/300], Train Loss: 0.4090, Train Acc: 80.95%, Test Loss: 0.4610, Test Acc: 73.68%\n",
      "Fold 5 Epoch [99/300], Train Loss: 0.4054, Train Acc: 80.09%, Test Loss: 0.4573, Test Acc: 70.18%\n",
      "Fold 5 Epoch [100/300], Train Loss: 0.4060, Train Acc: 79.65%, Test Loss: 0.4606, Test Acc: 71.93%\n",
      "Fold 5 Epoch [101/300], Train Loss: 0.3994, Train Acc: 81.39%, Test Loss: 0.4663, Test Acc: 73.68%\n",
      "Fold 5 Epoch [102/300], Train Loss: 0.4012, Train Acc: 81.82%, Test Loss: 0.4689, Test Acc: 73.68%\n",
      "Fold 5 Epoch [103/300], Train Loss: 0.3922, Train Acc: 81.82%, Test Loss: 0.4667, Test Acc: 73.68%\n",
      "Fold 5 Epoch [104/300], Train Loss: 0.3929, Train Acc: 80.95%, Test Loss: 0.4623, Test Acc: 71.93%\n",
      "Fold 5 Epoch [105/300], Train Loss: 0.3966, Train Acc: 80.52%, Test Loss: 0.4570, Test Acc: 71.93%\n",
      "Fold 5 Epoch [106/300], Train Loss: 0.3907, Train Acc: 80.09%, Test Loss: 0.4572, Test Acc: 73.68%\n",
      "Fold 5 Epoch [107/300], Train Loss: 0.3845, Train Acc: 80.95%, Test Loss: 0.4610, Test Acc: 71.93%\n",
      "Fold 5 Epoch [108/300], Train Loss: 0.3989, Train Acc: 82.68%, Test Loss: 0.4650, Test Acc: 71.93%\n",
      "Fold 5 Epoch [109/300], Train Loss: 0.3869, Train Acc: 81.39%, Test Loss: 0.4696, Test Acc: 73.68%\n",
      "Fold 5 Epoch [110/300], Train Loss: 0.3921, Train Acc: 82.68%, Test Loss: 0.4640, Test Acc: 71.93%\n",
      "Fold 5 Epoch [111/300], Train Loss: 0.3811, Train Acc: 80.52%, Test Loss: 0.4544, Test Acc: 73.68%\n",
      "Fold 5 Epoch [112/300], Train Loss: 0.3863, Train Acc: 80.95%, Test Loss: 0.4509, Test Acc: 71.93%\n",
      "Fold 5 Epoch [113/300], Train Loss: 0.3810, Train Acc: 81.39%, Test Loss: 0.4477, Test Acc: 71.93%\n",
      "Fold 5 Epoch [114/300], Train Loss: 0.3779, Train Acc: 80.09%, Test Loss: 0.4441, Test Acc: 73.68%\n",
      "Fold 5 Epoch [115/300], Train Loss: 0.3728, Train Acc: 80.52%, Test Loss: 0.4481, Test Acc: 73.68%\n",
      "Fold 5 Epoch [116/300], Train Loss: 0.3795, Train Acc: 81.39%, Test Loss: 0.4601, Test Acc: 73.68%\n",
      "Fold 5 Epoch [117/300], Train Loss: 0.3610, Train Acc: 82.25%, Test Loss: 0.4658, Test Acc: 73.68%\n",
      "Fold 5 Epoch [118/300], Train Loss: 0.3635, Train Acc: 81.82%, Test Loss: 0.4541, Test Acc: 73.68%\n",
      "Fold 5 Epoch [119/300], Train Loss: 0.3695, Train Acc: 82.25%, Test Loss: 0.4450, Test Acc: 73.68%\n",
      "Fold 5 Epoch [120/300], Train Loss: 0.3624, Train Acc: 81.39%, Test Loss: 0.4408, Test Acc: 73.68%\n",
      "Fold 5 Epoch [121/300], Train Loss: 0.3579, Train Acc: 81.82%, Test Loss: 0.4398, Test Acc: 73.68%\n",
      "Fold 5 Epoch [122/300], Train Loss: 0.3631, Train Acc: 81.39%, Test Loss: 0.4440, Test Acc: 73.68%\n",
      "Fold 5 Epoch [123/300], Train Loss: 0.3589, Train Acc: 81.82%, Test Loss: 0.4492, Test Acc: 70.18%\n",
      "Fold 5 Epoch [124/300], Train Loss: 0.3575, Train Acc: 82.68%, Test Loss: 0.4486, Test Acc: 73.68%\n",
      "Fold 5 Epoch [125/300], Train Loss: 0.3503, Train Acc: 83.98%, Test Loss: 0.4452, Test Acc: 71.93%\n",
      "Fold 5 Epoch [126/300], Train Loss: 0.3611, Train Acc: 83.55%, Test Loss: 0.4535, Test Acc: 71.93%\n",
      "Fold 5 Epoch [127/300], Train Loss: 0.3534, Train Acc: 83.98%, Test Loss: 0.4481, Test Acc: 71.93%\n",
      "Fold 5 Epoch [128/300], Train Loss: 0.3561, Train Acc: 83.98%, Test Loss: 0.4506, Test Acc: 71.93%\n",
      "Fold 5 Epoch [129/300], Train Loss: 0.3363, Train Acc: 83.55%, Test Loss: 0.4495, Test Acc: 71.93%\n",
      "Fold 5 Epoch [130/300], Train Loss: 0.3411, Train Acc: 83.12%, Test Loss: 0.4397, Test Acc: 71.93%\n",
      "Fold 5 Epoch [131/300], Train Loss: 0.3416, Train Acc: 83.55%, Test Loss: 0.4309, Test Acc: 71.93%\n",
      "Fold 5 Epoch [132/300], Train Loss: 0.3502, Train Acc: 83.98%, Test Loss: 0.4372, Test Acc: 71.93%\n",
      "Fold 5 Epoch [133/300], Train Loss: 0.3417, Train Acc: 84.85%, Test Loss: 0.4385, Test Acc: 71.93%\n",
      "Fold 5 Epoch [134/300], Train Loss: 0.3345, Train Acc: 83.55%, Test Loss: 0.4440, Test Acc: 70.18%\n",
      "Fold 5 Epoch [135/300], Train Loss: 0.3240, Train Acc: 84.85%, Test Loss: 0.4484, Test Acc: 71.93%\n",
      "Fold 5 Epoch [136/300], Train Loss: 0.3340, Train Acc: 84.42%, Test Loss: 0.4387, Test Acc: 71.93%\n",
      "Fold 5 Epoch [137/300], Train Loss: 0.3367, Train Acc: 84.85%, Test Loss: 0.4336, Test Acc: 71.93%\n",
      "Fold 5 Epoch [138/300], Train Loss: 0.3263, Train Acc: 84.42%, Test Loss: 0.4295, Test Acc: 71.93%\n",
      "Fold 5 Epoch [139/300], Train Loss: 0.3249, Train Acc: 84.85%, Test Loss: 0.4321, Test Acc: 71.93%\n",
      "Fold 5 Epoch [140/300], Train Loss: 0.3140, Train Acc: 84.85%, Test Loss: 0.4333, Test Acc: 71.93%\n",
      "Fold 5 Epoch [141/300], Train Loss: 0.3177, Train Acc: 86.15%, Test Loss: 0.4399, Test Acc: 71.93%\n",
      "Fold 5 Epoch [142/300], Train Loss: 0.3149, Train Acc: 84.85%, Test Loss: 0.4288, Test Acc: 71.93%\n",
      "Fold 5 Epoch [143/300], Train Loss: 0.3062, Train Acc: 84.85%, Test Loss: 0.4237, Test Acc: 71.93%\n",
      "Fold 5 Epoch [144/300], Train Loss: 0.3153, Train Acc: 85.71%, Test Loss: 0.4231, Test Acc: 71.93%\n",
      "Fold 5 Epoch [145/300], Train Loss: 0.3015, Train Acc: 85.71%, Test Loss: 0.4236, Test Acc: 71.93%\n",
      "Fold 5 Epoch [146/300], Train Loss: 0.2972, Train Acc: 86.15%, Test Loss: 0.4277, Test Acc: 71.93%\n",
      "Fold 5 Epoch [147/300], Train Loss: 0.3002, Train Acc: 86.58%, Test Loss: 0.4298, Test Acc: 71.93%\n",
      "Fold 5 Epoch [148/300], Train Loss: 0.3012, Train Acc: 86.15%, Test Loss: 0.4324, Test Acc: 71.93%\n",
      "Fold 5 Epoch [149/300], Train Loss: 0.2911, Train Acc: 86.58%, Test Loss: 0.4370, Test Acc: 73.68%\n",
      "Fold 5 Epoch [150/300], Train Loss: 0.2982, Train Acc: 87.01%, Test Loss: 0.4345, Test Acc: 73.68%\n",
      "Fold 5 Epoch [151/300], Train Loss: 0.2902, Train Acc: 87.01%, Test Loss: 0.4171, Test Acc: 73.68%\n",
      "Fold 5 Epoch [152/300], Train Loss: 0.2813, Train Acc: 86.58%, Test Loss: 0.4183, Test Acc: 75.44%\n",
      "Fold 5 Epoch [153/300], Train Loss: 0.2851, Train Acc: 87.88%, Test Loss: 0.4261, Test Acc: 75.44%\n",
      "Fold 5 Epoch [154/300], Train Loss: 0.2790, Train Acc: 87.88%, Test Loss: 0.4185, Test Acc: 73.68%\n",
      "Fold 5 Epoch [155/300], Train Loss: 0.2850, Train Acc: 88.31%, Test Loss: 0.4139, Test Acc: 73.68%\n",
      "Fold 5 Epoch [156/300], Train Loss: 0.2805, Train Acc: 88.74%, Test Loss: 0.4130, Test Acc: 75.44%\n",
      "Fold 5 Epoch [157/300], Train Loss: 0.2813, Train Acc: 87.88%, Test Loss: 0.4317, Test Acc: 73.68%\n",
      "Fold 5 Epoch [158/300], Train Loss: 0.2625, Train Acc: 88.31%, Test Loss: 0.4274, Test Acc: 73.68%\n",
      "Fold 5 Epoch [159/300], Train Loss: 0.2638, Train Acc: 89.18%, Test Loss: 0.4292, Test Acc: 75.44%\n",
      "Fold 5 Epoch [160/300], Train Loss: 0.2691, Train Acc: 88.31%, Test Loss: 0.4309, Test Acc: 75.44%\n",
      "Fold 5 Epoch [161/300], Train Loss: 0.2665, Train Acc: 89.18%, Test Loss: 0.4153, Test Acc: 77.19%\n",
      "Fold 5 Epoch [162/300], Train Loss: 0.2570, Train Acc: 89.61%, Test Loss: 0.4110, Test Acc: 77.19%\n",
      "Fold 5 Epoch [163/300], Train Loss: 0.2613, Train Acc: 88.31%, Test Loss: 0.4096, Test Acc: 77.19%\n",
      "Fold 5 Epoch [164/300], Train Loss: 0.2511, Train Acc: 90.04%, Test Loss: 0.4146, Test Acc: 77.19%\n",
      "Fold 5 Epoch [165/300], Train Loss: 0.2507, Train Acc: 89.61%, Test Loss: 0.4256, Test Acc: 75.44%\n",
      "Fold 5 Epoch [166/300], Train Loss: 0.2463, Train Acc: 89.61%, Test Loss: 0.4208, Test Acc: 78.95%\n",
      "Fold 5 Epoch [167/300], Train Loss: 0.2500, Train Acc: 89.18%, Test Loss: 0.4205, Test Acc: 80.70%\n",
      "Fold 5 Epoch [168/300], Train Loss: 0.2406, Train Acc: 88.74%, Test Loss: 0.4292, Test Acc: 77.19%\n",
      "Fold 5 Epoch [169/300], Train Loss: 0.2347, Train Acc: 90.91%, Test Loss: 0.4109, Test Acc: 77.19%\n",
      "Fold 5 Epoch [170/300], Train Loss: 0.2509, Train Acc: 89.61%, Test Loss: 0.4011, Test Acc: 80.70%\n",
      "Fold 5 Epoch [171/300], Train Loss: 0.2338, Train Acc: 91.77%, Test Loss: 0.4011, Test Acc: 82.46%\n",
      "Fold 5 Epoch [172/300], Train Loss: 0.2404, Train Acc: 90.04%, Test Loss: 0.4143, Test Acc: 78.95%\n",
      "Fold 5 Epoch [173/300], Train Loss: 0.2291, Train Acc: 91.34%, Test Loss: 0.4096, Test Acc: 82.46%\n",
      "Fold 5 Epoch [174/300], Train Loss: 0.2243, Train Acc: 92.64%, Test Loss: 0.4123, Test Acc: 80.70%\n",
      "Fold 5 Epoch [175/300], Train Loss: 0.2223, Train Acc: 91.77%, Test Loss: 0.4100, Test Acc: 80.70%\n",
      "Fold 5 Epoch [176/300], Train Loss: 0.2167, Train Acc: 91.77%, Test Loss: 0.4094, Test Acc: 80.70%\n",
      "Fold 5 Epoch [177/300], Train Loss: 0.2084, Train Acc: 92.21%, Test Loss: 0.4090, Test Acc: 82.46%\n",
      "Fold 5 Epoch [178/300], Train Loss: 0.2224, Train Acc: 91.77%, Test Loss: 0.4277, Test Acc: 78.95%\n",
      "Fold 5 Epoch [179/300], Train Loss: 0.2078, Train Acc: 91.34%, Test Loss: 0.4178, Test Acc: 82.46%\n",
      "Fold 5 Epoch [180/300], Train Loss: 0.2106, Train Acc: 91.77%, Test Loss: 0.4116, Test Acc: 82.46%\n",
      "Fold 5 Epoch [181/300], Train Loss: 0.2085, Train Acc: 93.51%, Test Loss: 0.4128, Test Acc: 82.46%\n",
      "Fold 5 Epoch [182/300], Train Loss: 0.1985, Train Acc: 93.51%, Test Loss: 0.4039, Test Acc: 80.70%\n",
      "Fold 5 Epoch [183/300], Train Loss: 0.2050, Train Acc: 91.77%, Test Loss: 0.4074, Test Acc: 80.70%\n",
      "Fold 5 Epoch [184/300], Train Loss: 0.1967, Train Acc: 93.94%, Test Loss: 0.4117, Test Acc: 82.46%\n",
      "Fold 5 Epoch [185/300], Train Loss: 0.1937, Train Acc: 93.07%, Test Loss: 0.4180, Test Acc: 82.46%\n",
      "Fold 5 Epoch [186/300], Train Loss: 0.1998, Train Acc: 91.34%, Test Loss: 0.4088, Test Acc: 82.46%\n",
      "Fold 5 Epoch [187/300], Train Loss: 0.1956, Train Acc: 93.94%, Test Loss: 0.4149, Test Acc: 80.70%\n",
      "Fold 5 Epoch [188/300], Train Loss: 0.1877, Train Acc: 93.51%, Test Loss: 0.4276, Test Acc: 77.19%\n",
      "Fold 5 Epoch [189/300], Train Loss: 0.1939, Train Acc: 92.64%, Test Loss: 0.4190, Test Acc: 82.46%\n",
      "Fold 5 Epoch [190/300], Train Loss: 0.1741, Train Acc: 94.37%, Test Loss: 0.4103, Test Acc: 82.46%\n",
      "Fold 5 Epoch [191/300], Train Loss: 0.1934, Train Acc: 93.07%, Test Loss: 0.4355, Test Acc: 77.19%\n",
      "Fold 5 Epoch [192/300], Train Loss: 0.1849, Train Acc: 93.51%, Test Loss: 0.4086, Test Acc: 82.46%\n",
      "Fold 5 Epoch [193/300], Train Loss: 0.1730, Train Acc: 95.24%, Test Loss: 0.4192, Test Acc: 82.46%\n",
      "Fold 5 Epoch [194/300], Train Loss: 0.1663, Train Acc: 95.67%, Test Loss: 0.4296, Test Acc: 82.46%\n",
      "Fold 5 Epoch [195/300], Train Loss: 0.1686, Train Acc: 96.10%, Test Loss: 0.4129, Test Acc: 82.46%\n",
      "Fold 5 Epoch [196/300], Train Loss: 0.1659, Train Acc: 96.10%, Test Loss: 0.4044, Test Acc: 82.46%\n",
      "Fold 5 Epoch [197/300], Train Loss: 0.1674, Train Acc: 96.10%, Test Loss: 0.4062, Test Acc: 82.46%\n",
      "Fold 5 Epoch [198/300], Train Loss: 0.1553, Train Acc: 96.10%, Test Loss: 0.4161, Test Acc: 82.46%\n",
      "Fold 5 Epoch [199/300], Train Loss: 0.1769, Train Acc: 93.07%, Test Loss: 0.4584, Test Acc: 77.19%\n",
      "Fold 5 Epoch [200/300], Train Loss: 0.1602, Train Acc: 95.24%, Test Loss: 0.4317, Test Acc: 82.46%\n",
      "Fold 5 Epoch [201/300], Train Loss: 0.1560, Train Acc: 94.81%, Test Loss: 0.4253, Test Acc: 82.46%\n",
      "Fold 5 Epoch [202/300], Train Loss: 0.1545, Train Acc: 94.37%, Test Loss: 0.4306, Test Acc: 80.70%\n",
      "Fold 5 Epoch [203/300], Train Loss: 0.1547, Train Acc: 96.10%, Test Loss: 0.4136, Test Acc: 80.70%\n",
      "Fold 5 Epoch [204/300], Train Loss: 0.1452, Train Acc: 96.54%, Test Loss: 0.4180, Test Acc: 82.46%\n",
      "Fold 5 Epoch [205/300], Train Loss: 0.1491, Train Acc: 96.10%, Test Loss: 0.4274, Test Acc: 82.46%\n",
      "Fold 5 Epoch [206/300], Train Loss: 0.1443, Train Acc: 96.54%, Test Loss: 0.4260, Test Acc: 82.46%\n",
      "Fold 5 Epoch [207/300], Train Loss: 0.1462, Train Acc: 96.10%, Test Loss: 0.4211, Test Acc: 84.21%\n",
      "Fold 5 Epoch [208/300], Train Loss: 0.1369, Train Acc: 96.97%, Test Loss: 0.4136, Test Acc: 82.46%\n",
      "Fold 5 Epoch [209/300], Train Loss: 0.1352, Train Acc: 96.54%, Test Loss: 0.4193, Test Acc: 82.46%\n",
      "Fold 5 Epoch [210/300], Train Loss: 0.1374, Train Acc: 96.97%, Test Loss: 0.4217, Test Acc: 82.46%\n",
      "Fold 5 Epoch [211/300], Train Loss: 0.1322, Train Acc: 96.54%, Test Loss: 0.4172, Test Acc: 82.46%\n",
      "Fold 5 Epoch [212/300], Train Loss: 0.1308, Train Acc: 97.40%, Test Loss: 0.4200, Test Acc: 84.21%\n",
      "Fold 5 Epoch [213/300], Train Loss: 0.1262, Train Acc: 96.97%, Test Loss: 0.4301, Test Acc: 84.21%\n",
      "Fold 5 Epoch [214/300], Train Loss: 0.1232, Train Acc: 96.97%, Test Loss: 0.4432, Test Acc: 82.46%\n",
      "Fold 5 Epoch [215/300], Train Loss: 0.1192, Train Acc: 96.97%, Test Loss: 0.4406, Test Acc: 84.21%\n",
      "Fold 5 Epoch [216/300], Train Loss: 0.1236, Train Acc: 96.97%, Test Loss: 0.4404, Test Acc: 84.21%\n",
      "Fold 5 Epoch [217/300], Train Loss: 0.1337, Train Acc: 95.24%, Test Loss: 0.4592, Test Acc: 77.19%\n",
      "Fold 5 Epoch [218/300], Train Loss: 0.1430, Train Acc: 94.37%, Test Loss: 0.4561, Test Acc: 84.21%\n",
      "Fold 5 Epoch [219/300], Train Loss: 0.1123, Train Acc: 97.40%, Test Loss: 0.4362, Test Acc: 84.21%\n",
      "Fold 5 Epoch [220/300], Train Loss: 0.1371, Train Acc: 95.24%, Test Loss: 0.4656, Test Acc: 82.46%\n",
      "Fold 5 Epoch [221/300], Train Loss: 0.1349, Train Acc: 94.81%, Test Loss: 0.4490, Test Acc: 82.46%\n",
      "Fold 5 Epoch [222/300], Train Loss: 0.1225, Train Acc: 96.97%, Test Loss: 0.4343, Test Acc: 80.70%\n",
      "Fold 5 Epoch [223/300], Train Loss: 0.1209, Train Acc: 97.40%, Test Loss: 0.4246, Test Acc: 80.70%\n",
      "Fold 5 Epoch [224/300], Train Loss: 0.1187, Train Acc: 96.97%, Test Loss: 0.4245, Test Acc: 84.21%\n",
      "Fold 5 Epoch [225/300], Train Loss: 0.1160, Train Acc: 98.27%, Test Loss: 0.4288, Test Acc: 84.21%\n",
      "Fold 5 Epoch [226/300], Train Loss: 0.1148, Train Acc: 96.97%, Test Loss: 0.4449, Test Acc: 80.70%\n",
      "Fold 5 Epoch [227/300], Train Loss: 0.1044, Train Acc: 97.40%, Test Loss: 0.4375, Test Acc: 85.96%\n",
      "Fold 5 Epoch [228/300], Train Loss: 0.1061, Train Acc: 98.27%, Test Loss: 0.4415, Test Acc: 85.96%\n",
      "Fold 5 Epoch [229/300], Train Loss: 0.1056, Train Acc: 97.84%, Test Loss: 0.4363, Test Acc: 85.96%\n",
      "Fold 5 Epoch [230/300], Train Loss: 0.0933, Train Acc: 98.70%, Test Loss: 0.4462, Test Acc: 85.96%\n",
      "Fold 5 Epoch [231/300], Train Loss: 0.0980, Train Acc: 98.70%, Test Loss: 0.4510, Test Acc: 85.96%\n",
      "Fold 5 Epoch [232/300], Train Loss: 0.0930, Train Acc: 98.70%, Test Loss: 0.4488, Test Acc: 85.96%\n",
      "Fold 5 Epoch [233/300], Train Loss: 0.0959, Train Acc: 98.27%, Test Loss: 0.4462, Test Acc: 82.46%\n",
      "Fold 5 Epoch [234/300], Train Loss: 0.0940, Train Acc: 99.13%, Test Loss: 0.4421, Test Acc: 84.21%\n",
      "Fold 5 Epoch [235/300], Train Loss: 0.0906, Train Acc: 99.13%, Test Loss: 0.4352, Test Acc: 85.96%\n",
      "Fold 5 Epoch [236/300], Train Loss: 0.0939, Train Acc: 97.84%, Test Loss: 0.4445, Test Acc: 84.21%\n",
      "Fold 5 Epoch [237/300], Train Loss: 0.0854, Train Acc: 99.13%, Test Loss: 0.4558, Test Acc: 85.96%\n",
      "Fold 5 Epoch [238/300], Train Loss: 0.0845, Train Acc: 98.70%, Test Loss: 0.4666, Test Acc: 85.96%\n",
      "Fold 5 Epoch [239/300], Train Loss: 0.0911, Train Acc: 98.70%, Test Loss: 0.4637, Test Acc: 85.96%\n",
      "Fold 5 Epoch [240/300], Train Loss: 0.0823, Train Acc: 98.27%, Test Loss: 0.4525, Test Acc: 85.96%\n",
      "Fold 5 Epoch [241/300], Train Loss: 0.0868, Train Acc: 99.13%, Test Loss: 0.4489, Test Acc: 85.96%\n",
      "Fold 5 Epoch [242/300], Train Loss: 0.0841, Train Acc: 99.13%, Test Loss: 0.4428, Test Acc: 85.96%\n",
      "Fold 5 Epoch [243/300], Train Loss: 0.0831, Train Acc: 99.57%, Test Loss: 0.4463, Test Acc: 85.96%\n",
      "Fold 5 Epoch [244/300], Train Loss: 0.0779, Train Acc: 99.13%, Test Loss: 0.4495, Test Acc: 85.96%\n",
      "Fold 5 Epoch [245/300], Train Loss: 0.0779, Train Acc: 99.13%, Test Loss: 0.4480, Test Acc: 85.96%\n",
      "Fold 5 Epoch [246/300], Train Loss: 0.0789, Train Acc: 99.13%, Test Loss: 0.4448, Test Acc: 85.96%\n",
      "Fold 5 Epoch [247/300], Train Loss: 0.0755, Train Acc: 98.70%, Test Loss: 0.4452, Test Acc: 85.96%\n",
      "Fold 5 Epoch [248/300], Train Loss: 0.0759, Train Acc: 99.13%, Test Loss: 0.4476, Test Acc: 85.96%\n",
      "Fold 5 Epoch [249/300], Train Loss: 0.0747, Train Acc: 99.13%, Test Loss: 0.4480, Test Acc: 85.96%\n",
      "Fold 5 Epoch [250/300], Train Loss: 0.0739, Train Acc: 99.13%, Test Loss: 0.4453, Test Acc: 85.96%\n",
      "Fold 5 Epoch [251/300], Train Loss: 0.0736, Train Acc: 98.70%, Test Loss: 0.4408, Test Acc: 85.96%\n",
      "Fold 5 Epoch [252/300], Train Loss: 0.0799, Train Acc: 99.13%, Test Loss: 0.4478, Test Acc: 85.96%\n",
      "Fold 5 Epoch [253/300], Train Loss: 0.0802, Train Acc: 99.13%, Test Loss: 0.4467, Test Acc: 85.96%\n",
      "Fold 5 Epoch [254/300], Train Loss: 0.0682, Train Acc: 99.13%, Test Loss: 0.4506, Test Acc: 85.96%\n",
      "Fold 5 Epoch [255/300], Train Loss: 0.0679, Train Acc: 99.57%, Test Loss: 0.4446, Test Acc: 85.96%\n",
      "Fold 5 Epoch [256/300], Train Loss: 0.0683, Train Acc: 100.00%, Test Loss: 0.4438, Test Acc: 85.96%\n",
      "Fold 5 Epoch [257/300], Train Loss: 0.0681, Train Acc: 99.57%, Test Loss: 0.4454, Test Acc: 85.96%\n",
      "Fold 5 Epoch [258/300], Train Loss: 0.0655, Train Acc: 99.57%, Test Loss: 0.4536, Test Acc: 85.96%\n",
      "Fold 5 Epoch [259/300], Train Loss: 0.0696, Train Acc: 99.57%, Test Loss: 0.4646, Test Acc: 85.96%\n",
      "Fold 5 Epoch [260/300], Train Loss: 0.0707, Train Acc: 98.70%, Test Loss: 0.4737, Test Acc: 85.96%\n",
      "Fold 5 Epoch [261/300], Train Loss: 0.0633, Train Acc: 99.57%, Test Loss: 0.4696, Test Acc: 85.96%\n",
      "Fold 5 Epoch [262/300], Train Loss: 0.0614, Train Acc: 100.00%, Test Loss: 0.4734, Test Acc: 85.96%\n",
      "Fold 5 Epoch [263/300], Train Loss: 0.0669, Train Acc: 99.13%, Test Loss: 0.4797, Test Acc: 84.21%\n",
      "Fold 5 Epoch [264/300], Train Loss: 0.0779, Train Acc: 98.70%, Test Loss: 0.4811, Test Acc: 85.96%\n",
      "Fold 5 Epoch [265/300], Train Loss: 0.0624, Train Acc: 99.13%, Test Loss: 0.4677, Test Acc: 85.96%\n",
      "Fold 5 Epoch [266/300], Train Loss: 0.0589, Train Acc: 100.00%, Test Loss: 0.4615, Test Acc: 85.96%\n",
      "Fold 5 Epoch [267/300], Train Loss: 0.0598, Train Acc: 99.57%, Test Loss: 0.4617, Test Acc: 85.96%\n",
      "Fold 5 Epoch [268/300], Train Loss: 0.0581, Train Acc: 99.13%, Test Loss: 0.4604, Test Acc: 85.96%\n",
      "Fold 5 Epoch [269/300], Train Loss: 0.0542, Train Acc: 100.00%, Test Loss: 0.4621, Test Acc: 85.96%\n",
      "Fold 5 Epoch [270/300], Train Loss: 0.0572, Train Acc: 99.57%, Test Loss: 0.4709, Test Acc: 85.96%\n",
      "Fold 5 Epoch [271/300], Train Loss: 0.0563, Train Acc: 99.13%, Test Loss: 0.4774, Test Acc: 85.96%\n",
      "Fold 5 Epoch [272/300], Train Loss: 0.0549, Train Acc: 100.00%, Test Loss: 0.4781, Test Acc: 85.96%\n",
      "Fold 5 Epoch [273/300], Train Loss: 0.0549, Train Acc: 99.57%, Test Loss: 0.4810, Test Acc: 85.96%\n",
      "Fold 5 Epoch [274/300], Train Loss: 0.0515, Train Acc: 100.00%, Test Loss: 0.4793, Test Acc: 85.96%\n",
      "Fold 5 Epoch [275/300], Train Loss: 0.0518, Train Acc: 99.57%, Test Loss: 0.4809, Test Acc: 85.96%\n",
      "Fold 5 Epoch [276/300], Train Loss: 0.0557, Train Acc: 99.57%, Test Loss: 0.4825, Test Acc: 85.96%\n",
      "Fold 5 Epoch [277/300], Train Loss: 0.0506, Train Acc: 100.00%, Test Loss: 0.4830, Test Acc: 85.96%\n",
      "Fold 5 Epoch [278/300], Train Loss: 0.0503, Train Acc: 100.00%, Test Loss: 0.4872, Test Acc: 85.96%\n",
      "Fold 5 Epoch [279/300], Train Loss: 0.0507, Train Acc: 99.57%, Test Loss: 0.4966, Test Acc: 85.96%\n",
      "Fold 5 Epoch [280/300], Train Loss: 0.0488, Train Acc: 100.00%, Test Loss: 0.4903, Test Acc: 85.96%\n",
      "Fold 5 Epoch [281/300], Train Loss: 0.0468, Train Acc: 100.00%, Test Loss: 0.4907, Test Acc: 85.96%\n",
      "Fold 5 Epoch [282/300], Train Loss: 0.0515, Train Acc: 99.57%, Test Loss: 0.4919, Test Acc: 85.96%\n",
      "Fold 5 Epoch [283/300], Train Loss: 0.0474, Train Acc: 100.00%, Test Loss: 0.4908, Test Acc: 85.96%\n",
      "Fold 5 Epoch [284/300], Train Loss: 0.0518, Train Acc: 99.57%, Test Loss: 0.4963, Test Acc: 85.96%\n",
      "Fold 5 Epoch [285/300], Train Loss: 0.0490, Train Acc: 100.00%, Test Loss: 0.4906, Test Acc: 85.96%\n",
      "Fold 5 Epoch [286/300], Train Loss: 0.0456, Train Acc: 100.00%, Test Loss: 0.4865, Test Acc: 85.96%\n",
      "Fold 5 Epoch [287/300], Train Loss: 0.0430, Train Acc: 100.00%, Test Loss: 0.4827, Test Acc: 85.96%\n",
      "Fold 5 Epoch [288/300], Train Loss: 0.0475, Train Acc: 99.57%, Test Loss: 0.4792, Test Acc: 85.96%\n",
      "Fold 5 Epoch [289/300], Train Loss: 0.0443, Train Acc: 100.00%, Test Loss: 0.4789, Test Acc: 85.96%\n",
      "Fold 5 Epoch [290/300], Train Loss: 0.0416, Train Acc: 100.00%, Test Loss: 0.4819, Test Acc: 85.96%\n",
      "Fold 5 Epoch [291/300], Train Loss: 0.0483, Train Acc: 99.57%, Test Loss: 0.4930, Test Acc: 84.21%\n",
      "Fold 5 Epoch [292/300], Train Loss: 0.0430, Train Acc: 99.57%, Test Loss: 0.4931, Test Acc: 85.96%\n",
      "Fold 5 Epoch [293/300], Train Loss: 0.0448, Train Acc: 99.57%, Test Loss: 0.5023, Test Acc: 85.96%\n",
      "Fold 5 Epoch [294/300], Train Loss: 0.0429, Train Acc: 100.00%, Test Loss: 0.5078, Test Acc: 85.96%\n",
      "Fold 5 Epoch [295/300], Train Loss: 0.0444, Train Acc: 99.57%, Test Loss: 0.5093, Test Acc: 85.96%\n",
      "Fold 5 Epoch [296/300], Train Loss: 0.0422, Train Acc: 100.00%, Test Loss: 0.5032, Test Acc: 85.96%\n",
      "Fold 5 Epoch [297/300], Train Loss: 0.0439, Train Acc: 100.00%, Test Loss: 0.5075, Test Acc: 85.96%\n",
      "Fold 5 Epoch [298/300], Train Loss: 0.0406, Train Acc: 100.00%, Test Loss: 0.5081, Test Acc: 85.96%\n",
      "Fold 5 Epoch [299/300], Train Loss: 0.0396, Train Acc: 100.00%, Test Loss: 0.5162, Test Acc: 85.96%\n",
      "Fold 5 Epoch [300/300], Train Loss: 0.0385, Train Acc: 100.00%, Test Loss: 0.5228, Test Acc: 85.96%\n",
      "Fold 5 Best Accuracy: 85.96%\n",
      "\n",
      "5-Fold CV Results:\n",
      "Average Accuracy: 86.12% (1.82)\n",
      "Fold Accuracies: ['86.21%', '84.48%', '84.48%', '89.47%', '85.96%']\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "fold_losses = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_all)):\n",
    "    print(f'Fold {fold + 1}/{n_folds}')\n",
    "    \n",
    "    # Split data\n",
    "    X_train_fold = X_all[train_idx]\n",
    "    y_train_fold = y_all[train_idx]\n",
    "    X_test_fold = X_all[val_idx]\n",
    "    y_test_fold = y_all[val_idx]\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_train_fold, y_train_fold)\n",
    "    test_dataset = TensorDataset(X_test_fold, y_test_fold)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    # Initialize model for each fold\n",
    "    model = OptimizedCNNModel(\n",
    "        num_classes=NUM_CLASSES,\n",
    "        input_channels=3,\n",
    "        seq_length=int(CHUNK_SIZE * SAMPLING_RATE)\n",
    "    )\n",
    "    optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-4)\n",
    "    \n",
    "    # Training loop\n",
    "    best_test_loss = float('inf')\n",
    "    best_test_accuracy = 0\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(300): \n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        train_loss, train_accuracy = evaluate_model(train_loader, model, criterion)\n",
    "        test_loss, test_accuracy = evaluate_model(test_loader, model, criterion)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        # Log and print\n",
    "        log_message = (f\"Fold {fold+1} Epoch [{epoch+1}/300], \"\n",
    "                      f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
    "                      f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\")\n",
    "        logging.info(log_message)\n",
    "        print(log_message)\n",
    "        \n",
    "        # Save best model based on validation accuracy\n",
    "        if test_accuracy > best_test_accuracy:\n",
    "            best_test_accuracy = test_accuracy\n",
    "            best_test_loss = test_loss\n",
    "            best_model_state = model.state_dict()  # Save the model state\n",
    "            save_best_model(epoch, model, optimizer, test_loss, train_losses, test_losses)\n",
    "    \n",
    "    # After training, store the best results for this fold\n",
    "    fold_accuracies.append(best_test_accuracy)\n",
    "    fold_losses.append(best_test_loss)\n",
    "    print(f'Fold {fold + 1} Best Accuracy: {best_test_accuracy:.2f}%')\n",
    "    \n",
    "\n",
    "# Print summary\n",
    "mean_accuracy = np.mean(fold_accuracies)\n",
    "std_accuracy = np.std(fold_accuracies)\n",
    "print(f'\\n5-Fold CV Results:')\n",
    "print(f'Average Accuracy: {mean_accuracy:.2f}% ({std_accuracy:.2f})')\n",
    "print(f'Fold Accuracies: {[f\"{acc:.2f}%\" for acc in fold_accuracies]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37f5a0a4-374a-40dd-9d27-81279b433394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot losses\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(test_losses) + 1), test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Testing Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c79cb82-164f-4ed0-ba0b-63dcc6f6bbc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Activities for Confusion matrix\n",
    "ACTIVITIES = ['sit', 'walk','upstair']\n",
    "\n",
    "# Model evaluation (confusion matrix)\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Loop through the test loader to collect predictions and true labels\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Compute the confusion matrix, explicitly specifying the labels\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions, labels=[0, 1, 2]) \n",
    "\n",
    "# Assuming conf_matrix and ACTIVITIES are already defined\n",
    "class_accuracies = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)  # Compute per-class accuracy\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(conf_matrix, display_labels=ACTIVITIES)\n",
    "fig, ax = plt.subplots()\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "\n",
    "# Add per-class accuracy text\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "    acc_text = f\"{class_accuracies[i] * 100:.2f}%\"\n",
    "    ax.text(\n",
    "        len(ACTIVITIES) + 0.3, i, acc_text, \n",
    "        fontsize=12, verticalalignment='center', color='black'\n",
    "    )\n",
    "\n",
    "plt.title(\"Confusion Matrix for Activities with Per-Class Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0f841cd-23c7-425c-a8c0-6e363b2c94f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label: 1, Predicted: 2, Confidence: 0.9483\n",
      "Full Probabilities: [1.1090045518358238e-05, 0.05171306058764458, 0.9482505321502686, 1.6183086017917958e-06, 1.2904809409519657e-05, 7.71257055021124e-06, 3.981357394877705e-07, 2.6850123049371177e-06]\n",
      "True Label: 1, Predicted: 2, Confidence: 0.9532\n",
      "Full Probabilities: [0.021949652582406998, 0.02394545078277588, 0.9531758427619934, 5.9593287005554885e-05, 0.0003994432627223432, 0.0004010896373074502, 1.7361004211124964e-05, 5.1631515816552565e-05]\n",
      "True Label: 1, Predicted: 2, Confidence: 0.6053\n",
      "Full Probabilities: [0.382885605096817, 0.009973566047847271, 0.6053200960159302, 7.666453893762082e-05, 0.0007514233002439141, 0.0009273415780626237, 2.022685657721013e-05, 4.5162469177739695e-05]\n",
      "True Label: 2, Predicted: 1, Confidence: 0.9908\n",
      "Full Probabilities: [4.1226962821383495e-06, 0.990790843963623, 0.009164496324956417, 1.254787775906152e-06, 2.808994395309128e-05, 9.842928193393163e-06, 4.369933037651208e-07, 8.672881222082651e-07]\n",
      "True Label: 1, Predicted: 2, Confidence: 0.9644\n",
      "Full Probabilities: [7.682313025725307e-07, 0.03563167527318001, 0.9643604755401611, 4.773797286361514e-07, 3.930619641323574e-06, 1.8609478047437733e-06, 9.457389893441359e-08, 7.590726340822584e-07]\n",
      "True Label: 2, Predicted: 1, Confidence: 0.8646\n",
      "Full Probabilities: [1.755963785399217e-05, 0.8646454811096191, 0.13529162108898163, 1.7342973706035991e-06, 2.8385922632878646e-05, 1.2828343642468099e-05, 5.505793296833872e-07, 1.9905046428903006e-06]\n",
      "True Label: 2, Predicted: 1, Confidence: 0.8746\n",
      "Full Probabilities: [0.0009713100153021514, 0.8746393918991089, 0.12361717969179153, 4.066759720444679e-05, 0.0004633661883417517, 0.00021397041564341635, 1.7947895685210824e-05, 3.618621121859178e-05]\n",
      "True Label: 2, Predicted: 1, Confidence: 0.9780\n",
      "Full Probabilities: [2.3915151814435376e-06, 0.9780319333076477, 0.0219370536506176, 1.1223181672903593e-06, 1.9500734197208658e-05, 6.256654614844592e-06, 4.1151412233375595e-07, 1.1653668252620264e-06]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "incorrect_samples = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)  # Get raw logits\n",
    "        probabilities = F.softmax(outputs, dim=1)  # Convert to probabilities\n",
    "        predicted_labels = torch.argmax(probabilities, dim=1)  # Get predicted class\n",
    "        confidence_scores = torch.max(probabilities, dim=1).values  # Get max confidence\n",
    "\n",
    "        # Find incorrect predictions\n",
    "        incorrect_indices = (predicted_labels != batch_y)\n",
    "        for i in range(len(batch_y)):\n",
    "            if incorrect_indices[i]:\n",
    "                incorrect_samples.append({\n",
    "                    \"True Label\": batch_y[i].item(),\n",
    "                    \"Predicted Label\": predicted_labels[i].item(),\n",
    "                    \"Confidence\": confidence_scores[i].item(),\n",
    "                    \"Probabilities\": probabilities[i].tolist()\n",
    "                })\n",
    "\n",
    "for sample in incorrect_samples:\n",
    "    print(f\"True Label: {sample['True Label']}, Predicted: {sample['Predicted Label']}, Confidence: {sample['Confidence']:.4f}\")\n",
    "    print(f\"Full Probabilities: {sample['Probabilities']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ec9986-bae2-4e5f-9a83-0fa0902dc6b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing chunk size: 0.25 seconds\n",
      "Chunk size 0.25s - Test Accuracy: 81.19%\n",
      "\n",
      "Testing chunk size: 0.5 seconds\n",
      "Chunk size 0.5s - Test Accuracy: 77.27%\n",
      "\n",
      "Testing chunk size: 1.0 seconds\n",
      "Chunk size 1.0s - Test Accuracy: 72.22%\n",
      "\n",
      "Testing chunk size: 1.375 seconds\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "chunk_sizes = [\n",
    "    0.25, 0.5, 1.0, 1.375, 2.0, 2.75, 3.0, 3.5, 4.0, 5, 5.5\n",
    "]\n",
    "accuracies = []\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    print(f\"\\nTesting chunk size: {chunk_size} seconds\")\n",
    "    \n",
    "    training_data, training_labels, _ = chunk_data(training_data_raw, training_docs, chunk_size, ACTIVITIES, SAMPLING_RATE)\n",
    "    testing_data, testing_labels, _ = chunk_data(testing_data_raw, testing_docs, chunk_size, ACTIVITIES, SAMPLING_RATE)\n",
    "    \n",
    "    training_data = np.array(training_data)\n",
    "    testing_data = np.array(testing_data)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(training_data.reshape(-1, training_data.shape[-1])).reshape(training_data.shape)\n",
    "    X_test = scaler.transform(testing_data.reshape(-1, testing_data.shape[-1])).reshape(testing_data.shape)\n",
    "    \n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(training_labels, dtype=torch.long)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(testing_labels, dtype=torch.long)\n",
    "    \n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    test_data = TensorDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True, pin_memory=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=64, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    model = OptimizedCNNModel(num_classes=NUM_CLASSES, input_channels=3, seq_length=int(chunk_size * 100))\n",
    "    optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    test_loss, test_accuracy = evaluate_model(test_loader, model, criterion)\n",
    "    accuracies.append(test_accuracy)\n",
    "    print(f\"Chunk size {chunk_size}s - Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "best_chunk_size = chunk_sizes[np.argmax(accuracies)]\n",
    "print(f\"\\nBest Chunk Size: {best_chunk_size} with Test Accuracy: {max(accuracies):.2f}%\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(chunk_sizes, accuracies, marker='o', linestyle='-')\n",
    "plt.xlabel('Chunk Size (seconds)')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.title('Chunk Size vs Test Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
