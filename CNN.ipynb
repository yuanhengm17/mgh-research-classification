{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e998e0-f6cd-4d3d-82f0-08637032af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from firebase_admin import credentials, firestore\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Firebase Initialization\n",
    "cred = credentials.Certificate(\"adminkey.json\")\n",
    "firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename='training_log.txt', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "INCLUDE_ONLY = ['Stephen', 'Lillian', 'Ren', 'Yuanheng', 'Ethan']\n",
    "ACTIVITIES = ['sit', 'walk', 'upstair','downstair']\n",
    "CHUNK_SIZE = 0.5  # in seconds (can be a decimal)\n",
    "SAMPLING_RATE = 100  # Hz\n",
    "START_FROM, END_TO = 200, 300\n",
    "NUM_CLASSES = 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "287ad27b-8007-4ccd-b113-f8016c2aaec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Structures\n",
    "def fetch_data(collection_name, activities, include_only, time_start=500, time_end=8000):\n",
    "    \"\"\"Fetch and preprocess data from Firestore.\"\"\"\n",
    "    data, docs = [], []\n",
    "    for person in db.collection(collection_name).stream():\n",
    "        person_name = str(person.to_dict().get('name', ''))\n",
    "        if person_name not in include_only:\n",
    "            continue\n",
    "\n",
    "        for activity in activities:\n",
    "            for recording in db.collection(collection_name).document(person_name).collection(activity).stream():\n",
    "                record = recording.to_dict()\n",
    "                if 'acceleration' not in record:\n",
    "                    continue\n",
    "\n",
    "                docs.append(record)\n",
    "                df = pd.DataFrame(record['acceleration'])\n",
    "                \n",
    "                if 'time' in df.columns:\n",
    "                    filtered_df = df[(df['time'] >= time_start) & (df['time'] <= time_end)]\n",
    "                    data.append(filtered_df)\n",
    "                else:\n",
    "                    raise ValueError(\"The 'acceleration' field must include a 'time' column.\")\n",
    "    return data, docs\n",
    "\n",
    "# Fetch and preprocess training/testing data\n",
    "training_data_raw, training_docs = fetch_data(\"training\", ACTIVITIES, INCLUDE_ONLY)\n",
    "testing_data_raw, testing_docs = fetch_data(\"testing\", ACTIVITIES, INCLUDE_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1284a61-2522-4348-abbf-1d4e31e36bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to chunk data into smaller segments for training/testing\n",
    "def chunk_data(data_raw, docs, chunk_size, activities, sampling_rate):\n",
    "    \"\"\"Split data into chunks and assign labels.\"\"\"\n",
    "    data, labels = [], []\n",
    "    activity_distribution = np.zeros(len(activities))\n",
    "    chunk_samples = int(chunk_size * sampling_rate)  # Convert time to sample count\n",
    "\n",
    "    for i in range(len(data_raw)):\n",
    "        num_chunks = len(data_raw[i]) // chunk_samples  # Number of full chunks\n",
    "        for j in range(num_chunks):\n",
    "            start = j * chunk_samples\n",
    "            end = start + chunk_samples\n",
    "            x = list(data_raw[i][\"x\"])[start:end]\n",
    "            y = list(data_raw[i][\"y\"])[start:end]\n",
    "            z = list(data_raw[i][\"z\"])[start:end]\n",
    "            activity = docs[i]['activity']\n",
    "            label = activities.index(activity)\n",
    "\n",
    "            activity_distribution[label] += 1\n",
    "            data.append([x, y, z])\n",
    "            labels.append(label)\n",
    "\n",
    "    return data, labels, activity_distribution\n",
    "\n",
    "# Chunk the data\n",
    "training_data, training_labels, training_distribution = chunk_data(training_data_raw, training_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE)\n",
    "testing_data, testing_labels, testing_distribution = chunk_data(testing_data_raw, testing_docs, CHUNK_SIZE, ACTIVITIES, SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c103939a-52ee-4f57-87df-be129b11ad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save function\n",
    "def save_model(model, optimizer, filename=\"model.pth\"):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Model saved to {filename}\")\n",
    "# Load function\n",
    "def load_model(model, optimizer, filename=\"model.pth\"):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    print(f\"Model loaded from {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fae1da3e-3e2c-40ad-82cb-6d090f2cc7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to NumPy arrays\n",
    "training_data = np.array(training_data)\n",
    "testing_data = np.array(testing_data)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(training_data.reshape(-1, training_data.shape[-1])).reshape(training_data.shape)\n",
    "X_test = scaler.transform(testing_data.reshape(-1, testing_data.shape[-1])).reshape(testing_data.shape)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)  # Shape: (num_samples, 3, chunk_size*100)\n",
    "y_train = torch.tensor(training_labels, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(testing_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset and DataLoader for batching\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ec68f8d-85ff-48e1-b219-a547e9393c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedCNNModel(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=3, seq_length=int(CHUNK_SIZE * 100)):\n",
    "        super(OptimizedCNNModel, self).__init__()\n",
    "\n",
    "        def depthwise_separable_conv(in_channels, out_channels, kernel_size=3, padding=1):\n",
    "            \"\"\"Depthwise Separable Convolution for efficiency.\"\"\"\n",
    "            return nn.Sequential(\n",
    "                nn.Conv1d(in_channels, in_channels, kernel_size, groups=in_channels, padding=padding, bias=False),\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "                nn.GroupNorm(8, out_channels),  # More stable than BatchNorm for small batches\n",
    "                nn.SiLU()  # Swish activation (better than ReLU)\n",
    "            )\n",
    "\n",
    "        self.conv1 = depthwise_separable_conv(input_channels, 16)\n",
    "        self.conv2 = depthwise_separable_conv(16, 32)\n",
    "        self.conv3 = depthwise_separable_conv(32, 64)\n",
    "\n",
    "        # Adaptive pooling to ensure flexible sequence length compatibility\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, num_classes)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "        # Weight initialization\n",
    "        self.apply(self.initialize_weights)\n",
    "\n",
    "    def initialize_weights(self, m):\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "\n",
    "        # Global Average Pooling to reduce to fixed size\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.silu(self.fc1(x))  # Swish activation\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc68f1f4-225f-455c-b6d0-80a76a3ce598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer with weight decay, and loss function\n",
    "model = OptimizedCNNModel(num_classes=NUM_CLASSES, input_channels=3, seq_length=int(CHUNK_SIZE * 100))\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Weight decay helps reduce overfitting\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd628c08-376b-468a-ab5f-6c1a88c7587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(loader, model, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # disable gradient calc\n",
    "        for batch_X, batch_y in loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)  # get predicted class (max value)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "696123a5-acaa-402b-bb78-04b48daa8a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 2.3964, Train Acc: 36.67%, Test Loss: 2.4439, Test Acc: 38.23%\n",
      "Epoch [2/100], Train Loss: 2.0691, Train Acc: 42.81%, Test Loss: 2.1196, Test Acc: 43.69%\n",
      "Epoch [3/100], Train Loss: 1.7916, Train Acc: 43.62%, Test Loss: 1.8611, Test Acc: 43.86%\n",
      "Epoch [4/100], Train Loss: 1.5724, Train Acc: 45.07%, Test Loss: 1.6593, Test Acc: 44.54%\n",
      "Epoch [5/100], Train Loss: 1.3980, Train Acc: 46.53%, Test Loss: 1.5015, Test Acc: 44.88%\n",
      "Epoch [6/100], Train Loss: 1.2661, Train Acc: 49.11%, Test Loss: 1.3782, Test Acc: 45.73%\n",
      "Epoch [7/100], Train Loss: 1.1608, Train Acc: 54.93%, Test Loss: 1.2718, Test Acc: 51.88%\n",
      "Epoch [8/100], Train Loss: 1.0861, Train Acc: 61.23%, Test Loss: 1.1892, Test Acc: 61.09%\n",
      "Epoch [9/100], Train Loss: 1.0192, Train Acc: 64.94%, Test Loss: 1.1336, Test Acc: 64.85%\n",
      "Epoch [10/100], Train Loss: 0.9654, Train Acc: 66.72%, Test Loss: 1.1039, Test Acc: 65.36%\n",
      "Epoch [11/100], Train Loss: 0.9285, Train Acc: 66.88%, Test Loss: 1.0695, Test Acc: 66.04%\n",
      "Epoch [12/100], Train Loss: 0.8972, Train Acc: 67.53%, Test Loss: 1.0386, Test Acc: 67.41%\n",
      "Epoch [13/100], Train Loss: 0.8709, Train Acc: 67.53%, Test Loss: 1.0169, Test Acc: 67.92%\n",
      "Epoch [14/100], Train Loss: 0.8470, Train Acc: 68.17%, Test Loss: 0.9972, Test Acc: 67.75%\n",
      "Epoch [15/100], Train Loss: 0.8198, Train Acc: 68.34%, Test Loss: 0.9978, Test Acc: 65.02%\n",
      "Epoch [16/100], Train Loss: 0.8010, Train Acc: 68.66%, Test Loss: 0.9790, Test Acc: 63.99%\n",
      "Epoch [17/100], Train Loss: 0.7767, Train Acc: 68.01%, Test Loss: 0.9674, Test Acc: 64.16%\n",
      "Epoch [18/100], Train Loss: 0.7686, Train Acc: 67.53%, Test Loss: 0.9511, Test Acc: 64.51%\n",
      "Epoch [19/100], Train Loss: 0.7533, Train Acc: 67.37%, Test Loss: 0.9479, Test Acc: 63.82%\n",
      "Epoch [20/100], Train Loss: 0.7472, Train Acc: 68.34%, Test Loss: 0.9659, Test Acc: 63.31%\n",
      "Epoch [21/100], Train Loss: 0.7425, Train Acc: 68.66%, Test Loss: 0.9715, Test Acc: 64.33%\n",
      "Epoch [22/100], Train Loss: 0.7339, Train Acc: 68.34%, Test Loss: 0.9556, Test Acc: 64.33%\n",
      "Epoch [23/100], Train Loss: 0.7226, Train Acc: 68.66%, Test Loss: 0.9628, Test Acc: 64.33%\n",
      "Epoch [24/100], Train Loss: 0.7201, Train Acc: 68.50%, Test Loss: 0.9641, Test Acc: 64.68%\n",
      "Epoch [25/100], Train Loss: 0.7223, Train Acc: 68.50%, Test Loss: 0.9521, Test Acc: 64.33%\n",
      "Epoch [26/100], Train Loss: 0.7070, Train Acc: 68.98%, Test Loss: 0.9494, Test Acc: 64.16%\n",
      "Epoch [27/100], Train Loss: 0.7078, Train Acc: 68.98%, Test Loss: 0.9803, Test Acc: 64.33%\n",
      "Epoch [28/100], Train Loss: 0.7000, Train Acc: 69.31%, Test Loss: 0.9740, Test Acc: 64.68%\n",
      "Epoch [29/100], Train Loss: 0.7063, Train Acc: 69.47%, Test Loss: 0.9696, Test Acc: 64.51%\n",
      "Epoch [30/100], Train Loss: 0.6948, Train Acc: 69.47%, Test Loss: 0.9886, Test Acc: 64.68%\n",
      "Epoch [31/100], Train Loss: 0.7006, Train Acc: 69.31%, Test Loss: 0.9736, Test Acc: 64.33%\n",
      "Epoch [32/100], Train Loss: 0.6905, Train Acc: 69.47%, Test Loss: 0.9686, Test Acc: 64.51%\n",
      "Epoch [33/100], Train Loss: 0.6899, Train Acc: 69.79%, Test Loss: 0.9706, Test Acc: 64.68%\n",
      "Epoch [34/100], Train Loss: 0.6923, Train Acc: 69.79%, Test Loss: 0.9418, Test Acc: 63.82%\n",
      "Epoch [35/100], Train Loss: 0.6883, Train Acc: 69.47%, Test Loss: 0.9621, Test Acc: 63.99%\n",
      "Epoch [36/100], Train Loss: 0.6870, Train Acc: 69.47%, Test Loss: 0.9769, Test Acc: 64.85%\n",
      "Epoch [37/100], Train Loss: 0.6779, Train Acc: 69.63%, Test Loss: 0.9850, Test Acc: 64.85%\n",
      "Epoch [38/100], Train Loss: 0.6756, Train Acc: 69.95%, Test Loss: 0.9822, Test Acc: 63.99%\n",
      "Epoch [39/100], Train Loss: 0.6815, Train Acc: 69.95%, Test Loss: 0.9947, Test Acc: 65.19%\n",
      "Epoch [40/100], Train Loss: 0.6745, Train Acc: 69.79%, Test Loss: 0.9717, Test Acc: 63.82%\n",
      "Epoch [41/100], Train Loss: 0.6681, Train Acc: 70.11%, Test Loss: 0.9768, Test Acc: 64.33%\n",
      "Epoch [42/100], Train Loss: 0.6696, Train Acc: 70.92%, Test Loss: 0.9866, Test Acc: 63.82%\n",
      "Epoch [43/100], Train Loss: 0.6706, Train Acc: 70.44%, Test Loss: 0.9794, Test Acc: 63.99%\n",
      "Epoch [44/100], Train Loss: 0.6591, Train Acc: 70.60%, Test Loss: 0.9582, Test Acc: 64.16%\n",
      "Epoch [45/100], Train Loss: 0.6678, Train Acc: 70.60%, Test Loss: 0.9583, Test Acc: 64.51%\n",
      "Epoch [46/100], Train Loss: 0.6569, Train Acc: 70.92%, Test Loss: 0.9519, Test Acc: 65.02%\n",
      "Epoch [47/100], Train Loss: 0.6509, Train Acc: 70.92%, Test Loss: 0.9565, Test Acc: 64.85%\n",
      "Epoch [48/100], Train Loss: 0.6620, Train Acc: 71.41%, Test Loss: 0.9450, Test Acc: 65.19%\n",
      "Epoch [49/100], Train Loss: 0.6535, Train Acc: 71.57%, Test Loss: 0.9038, Test Acc: 65.19%\n",
      "Epoch [50/100], Train Loss: 0.6523, Train Acc: 71.08%, Test Loss: 0.9565, Test Acc: 64.85%\n",
      "Epoch [51/100], Train Loss: 0.6522, Train Acc: 71.73%, Test Loss: 0.9332, Test Acc: 65.36%\n",
      "Epoch [52/100], Train Loss: 0.6453, Train Acc: 71.57%, Test Loss: 0.9694, Test Acc: 64.85%\n",
      "Epoch [53/100], Train Loss: 0.6442, Train Acc: 71.41%, Test Loss: 0.9684, Test Acc: 64.85%\n",
      "Epoch [54/100], Train Loss: 0.6439, Train Acc: 71.24%, Test Loss: 0.9678, Test Acc: 64.85%\n",
      "Epoch [55/100], Train Loss: 0.6406, Train Acc: 73.51%, Test Loss: 0.9235, Test Acc: 66.04%\n",
      "Epoch [56/100], Train Loss: 0.6326, Train Acc: 73.51%, Test Loss: 0.9292, Test Acc: 65.70%\n",
      "Epoch [57/100], Train Loss: 0.6345, Train Acc: 72.37%, Test Loss: 0.9364, Test Acc: 65.02%\n",
      "Epoch [58/100], Train Loss: 0.6325, Train Acc: 72.05%, Test Loss: 0.9469, Test Acc: 65.53%\n",
      "Epoch [59/100], Train Loss: 0.6349, Train Acc: 72.86%, Test Loss: 0.9708, Test Acc: 65.70%\n",
      "Epoch [60/100], Train Loss: 0.6290, Train Acc: 72.70%, Test Loss: 0.9493, Test Acc: 65.36%\n",
      "Epoch [61/100], Train Loss: 0.6323, Train Acc: 72.86%, Test Loss: 0.9285, Test Acc: 64.85%\n",
      "Epoch [62/100], Train Loss: 0.6279, Train Acc: 73.02%, Test Loss: 0.9878, Test Acc: 65.36%\n",
      "Epoch [63/100], Train Loss: 0.6257, Train Acc: 73.02%, Test Loss: 0.9805, Test Acc: 65.53%\n",
      "Epoch [64/100], Train Loss: 0.6235, Train Acc: 74.15%, Test Loss: 0.9370, Test Acc: 65.70%\n",
      "Epoch [65/100], Train Loss: 0.6251, Train Acc: 73.18%, Test Loss: 0.9294, Test Acc: 65.70%\n",
      "Epoch [66/100], Train Loss: 0.6215, Train Acc: 72.86%, Test Loss: 0.9752, Test Acc: 66.38%\n",
      "Epoch [67/100], Train Loss: 0.6152, Train Acc: 74.47%, Test Loss: 0.9601, Test Acc: 66.04%\n",
      "Epoch [68/100], Train Loss: 0.6068, Train Acc: 74.47%, Test Loss: 0.9458, Test Acc: 65.87%\n",
      "Epoch [69/100], Train Loss: 0.6157, Train Acc: 74.47%, Test Loss: 0.9630, Test Acc: 66.04%\n",
      "Epoch [70/100], Train Loss: 0.6094, Train Acc: 74.47%, Test Loss: 0.9323, Test Acc: 65.70%\n",
      "Epoch [71/100], Train Loss: 0.6095, Train Acc: 74.47%, Test Loss: 0.9602, Test Acc: 66.04%\n",
      "Epoch [72/100], Train Loss: 0.6076, Train Acc: 74.96%, Test Loss: 0.9457, Test Acc: 66.04%\n",
      "Epoch [73/100], Train Loss: 0.6060, Train Acc: 74.31%, Test Loss: 0.9605, Test Acc: 65.87%\n",
      "Epoch [74/100], Train Loss: 0.6007, Train Acc: 74.64%, Test Loss: 0.9701, Test Acc: 65.87%\n",
      "Epoch [75/100], Train Loss: 0.6000, Train Acc: 75.28%, Test Loss: 0.9599, Test Acc: 66.21%\n",
      "Epoch [76/100], Train Loss: 0.5983, Train Acc: 75.12%, Test Loss: 0.9471, Test Acc: 66.89%\n",
      "Epoch [77/100], Train Loss: 0.5906, Train Acc: 74.15%, Test Loss: 0.9771, Test Acc: 65.70%\n",
      "Epoch [78/100], Train Loss: 0.5935, Train Acc: 76.58%, Test Loss: 0.9587, Test Acc: 66.04%\n",
      "Epoch [79/100], Train Loss: 0.5937, Train Acc: 74.80%, Test Loss: 0.9952, Test Acc: 65.70%\n",
      "Epoch [80/100], Train Loss: 0.5873, Train Acc: 74.64%, Test Loss: 0.9311, Test Acc: 65.36%\n",
      "Epoch [81/100], Train Loss: 0.5791, Train Acc: 76.74%, Test Loss: 0.9417, Test Acc: 65.87%\n",
      "Epoch [82/100], Train Loss: 0.5812, Train Acc: 75.61%, Test Loss: 0.9748, Test Acc: 66.55%\n",
      "Epoch [83/100], Train Loss: 0.5804, Train Acc: 76.25%, Test Loss: 0.9929, Test Acc: 65.70%\n",
      "Epoch [84/100], Train Loss: 0.5793, Train Acc: 76.41%, Test Loss: 0.9571, Test Acc: 66.55%\n",
      "Epoch [85/100], Train Loss: 0.5675, Train Acc: 76.90%, Test Loss: 0.9853, Test Acc: 65.02%\n",
      "Epoch [86/100], Train Loss: 0.5645, Train Acc: 76.74%, Test Loss: 0.9793, Test Acc: 66.72%\n",
      "Epoch [87/100], Train Loss: 0.5654, Train Acc: 77.71%, Test Loss: 0.9603, Test Acc: 66.04%\n",
      "Epoch [88/100], Train Loss: 0.5609, Train Acc: 76.41%, Test Loss: 0.9594, Test Acc: 66.38%\n",
      "Epoch [89/100], Train Loss: 0.5600, Train Acc: 77.54%, Test Loss: 0.9850, Test Acc: 65.53%\n",
      "Epoch [90/100], Train Loss: 0.5549, Train Acc: 77.22%, Test Loss: 1.0014, Test Acc: 66.21%\n",
      "Epoch [91/100], Train Loss: 0.5539, Train Acc: 76.25%, Test Loss: 1.0175, Test Acc: 65.19%\n",
      "Epoch [92/100], Train Loss: 0.5550, Train Acc: 78.19%, Test Loss: 0.9912, Test Acc: 65.53%\n",
      "Epoch [93/100], Train Loss: 0.5471, Train Acc: 76.90%, Test Loss: 1.0385, Test Acc: 66.04%\n",
      "Epoch [94/100], Train Loss: 0.5424, Train Acc: 77.54%, Test Loss: 1.0526, Test Acc: 65.70%\n",
      "Epoch [95/100], Train Loss: 0.5389, Train Acc: 78.35%, Test Loss: 1.0237, Test Acc: 66.38%\n",
      "Epoch [96/100], Train Loss: 0.5386, Train Acc: 78.35%, Test Loss: 1.0037, Test Acc: 66.04%\n",
      "Epoch [97/100], Train Loss: 0.5299, Train Acc: 77.71%, Test Loss: 1.0030, Test Acc: 65.70%\n",
      "Epoch [98/100], Train Loss: 0.5308, Train Acc: 78.68%, Test Loss: 1.0534, Test Acc: 66.38%\n",
      "Epoch [99/100], Train Loss: 0.5272, Train Acc: 78.68%, Test Loss: 1.0724, Test Acc: 66.55%\n",
      "Epoch [100/100], Train Loss: 0.5235, Train Acc: 77.87%, Test Loss: 1.0877, Test Acc: 65.53%\n",
      "Model saved to model.pth\n"
     ]
    }
   ],
   "source": [
    "# Training loop with validation\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluate on training and testing sets\n",
    "    train_loss, train_accuracy = evaluate_model(train_loader, model, criterion)\n",
    "    test_loss, test_accuracy = evaluate_model(test_loader, model, criterion)\n",
    "    \n",
    "    # Logging the information\n",
    "    logMsg = (f\"Epoch [{epoch+1}/{epochs}], \"\n",
    "                   f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
    "                   f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    logging.info(logMsg)\n",
    "    print(logMsg)\n",
    "    \n",
    "    scheduler.step(train_loss)  # Adjust the learning rate if needed\n",
    "# Save the model at the end of training\n",
    "save_model(model, optimizer, \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37f5a0a4-374a-40dd-9d27-81279b433394",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Plot the loss graph\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[43mlosses\u001b[49m, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss Over Epochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot the loss graph\n",
    "plt.plot(range(1, epochs + 1), losses, marker='o', linestyle='-', color='b')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79cb82-164f-4ed0-ba0b-63dcc6f6bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Activities for the Confusion matrix\n",
    "ACTIVITIES = ['sit', 'walk','upstair','downstair']\n",
    "\n",
    "# Model evaluation (confusion matrix)\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Loop through the test loader to collect predictions and true labels\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Compute the confusion matrix, explicitly specifying the labels\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions, labels=[0, 1, 2,3]) \n",
    "\n",
    "# Assuming conf_matrix and ACTIVITIES are already defined\n",
    "class_accuracies = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)  # Compute per-class accuracy\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(conf_matrix, display_labels=ACTIVITIES)\n",
    "fig, ax = plt.subplots()\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "\n",
    "# Add per-class accuracy text\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "    acc_text = f\"{class_accuracies[i] * 100:.2f}%\"\n",
    "    ax.text(\n",
    "        len(ACTIVITIES) + 0.3, i, acc_text, \n",
    "        fontsize=12, verticalalignment='center', color='black'\n",
    "    )\n",
    "\n",
    "plt.title(\"Confusion Matrix for Activities with Per-Class Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b497525-bb2d-40a0-8243-c9b6149da2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "incorrect_samples = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)  # Get raw logits\n",
    "        probabilities = F.softmax(outputs, dim=1)  # Convert to probabilities\n",
    "        predicted_labels = torch.argmax(probabilities, dim=1)  # Get predicted class\n",
    "        confidence_scores = torch.max(probabilities, dim=1).values  # Get max confidence\n",
    "\n",
    "        # Find incorrect predictions\n",
    "        incorrect_indices = (predicted_labels != batch_y)\n",
    "        for i in range(len(batch_y)):\n",
    "            if incorrect_indices[i]:\n",
    "                incorrect_samples.append({\n",
    "                    \"True Label\": batch_y[i].item(),\n",
    "                    \"Predicted Label\": predicted_labels[i].item(),\n",
    "                    \"Confidence\": confidence_scores[i].item(),\n",
    "                    \"Probabilities\": probabilities[i].tolist()\n",
    "                })\n",
    "\n",
    "# Print results\n",
    "for sample in incorrect_samples:\n",
    "    print(f\"True Label: {sample['True Label']}, Predicted: {sample['Predicted Label']}, Confidence: {sample['Confidence']:.4f}\")\n",
    "    print(f\"Full Probabilities: {sample['Probabilities']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
